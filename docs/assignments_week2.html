<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Wouter van Amsterdam" />

<meta name="date" content="2018-01-15" />

<title>Assignments Modern Methods in Data Analysis, week 2</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">epi_stats</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/jdblischak/workflowr">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Assignments Modern Methods in Data Analysis, week 2</h1>
<h4 class="author"><em>Wouter van Amsterdam</em></h4>
<h4 class="date"><em>2018-01-15</em></h4>

</div>


<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
<!-- Update knitr chunk options -->
<!-- Insert the date the file was last updated -->
<p><strong>Last updated:</strong> 2018-01-17</p>
<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
<p><strong>Code version:</strong> 4ae1ef0</p>
<!-- Add your analysis here -->
<div id="setup" class="section level2">
<h2>Setup</h2>
<div id="load-some-packages" class="section level3">
<h3>Load some packages</h3>
<pre class="r"><code>library(epistats) # contains &#39;fromParentDir&#39; and other handy functions
library(magrittr) # for &#39;piping&#39;  &#39;%&gt;%&#39;
library(dplyr)    # for data mangling, selecting columns and filtering rows
library(ggplot2)  # awesome plotting library
library(stringr)  # for working with strings
library(purrr)    # for the &#39;map&#39; function, which is an alternative for lapply, sapply, mapply, etc.</code></pre>
<p>For installing packages, type <code>install.packages(&lt;package_name&gt;)</code>, for instance: <code>install.packages(dplyr)</code></p>
<p><code>epistats</code> is only available from GitHub, and can be installed as follows:</p>
<pre class="r"><code>install.packages(devtools) # when not installed already
devtools::install_github(&quot;vanAmsterdam/epistats&quot;)</code></pre>
</div>
</div>
<div id="resampling" class="section level2">
<h2>Resampling</h2>
<blockquote>
<p>Most of the exercises below come from Moore and McCabe’s Introduction to Practical Statistics, (8th Ed) chapter 16; some have been adapted slightly. Note that for exercises 16.40 and further an R script with commands is available on Moodle. In the answers to the exercises, at the end of the current document, some extra explanation about the R commands is provided. You can also have a look at the following resource that explains most common bootstrap functions in R: <a href="http://www.statmethods.net/advstats/bootstrapping.html" class="uri">http://www.statmethods.net/advstats/bootstrapping.html</a></p>
</blockquote>
<div id="ex.-16.7-what-is-wrong" class="section level3">
<h3>Ex. 16.7 What is wrong?</h3>
<blockquote>
<p>Explain what is wrong with each of the following statements.</p>
</blockquote>
<div id="a-the-bootstrap-distribution-is-created-by-resampling-with-replacement-from-the-population." class="section level4">
<h4>(a) The bootstrap distribution is created by resampling with replacement from the population.</h4>
<p>It is created by resampling with replacement from the sample, not the population</p>
</div>
<div id="b-the-bootstrap-distribution-is-created-by-resampling-without-replacement-from-the-original-sample." class="section level4">
<h4>(b) The bootstrap distribution is created by resampling without replacement from the original sample.</h4>
<p>It is resampling with replacements</p>
</div>
<div id="c-when-generating-the-resamples-it-is-best-to-use-a-sample-size-larger-than-the-size-of-the-original-sample." class="section level4">
<h4>(c) When generating the resamples, it is best to use a sample size larger than the size of the original sample.</h4>
<p>No, this will give falsly tight confidence bounds as you are simulating that you have gathered a lot of data, while you have only simulated data.</p>
</div>
<div id="d-the-bootstrap-distribution-will-be-similar-to-the-sampling-distribution-in-shape-center-and-spread." class="section level4">
<h4>(d) The bootstrap distribution will be similar to the sampling distribution in shape, center, and spread.</h4>
<p>The bootstrap distribution is centered around the the sample mean, with sample spread. These can be different from the population and thus different from the ‘true’ sampling distribution</p>
</div>
</div>
<div id="ex.-16.40-confidence-interval-for-the-average-iq-score." class="section level3">
<h3>Ex. 16.40 Confidence interval for the average IQ score.</h3>
<blockquote>
<p>The distribution of the 60 IQ test scores in IQ.txt is roughly Normal (make a histogram and normal QQ-plot to check this), and the sample size is large enough that we expect a Normal sampling distribution. We will compare confidence intervals for the population mean IQ μ based on this sample.</p>
</blockquote>
<div id="a-use-the-formula-sn-to-find-the-standard-error-of-the-mean.-give-the-usual-95-confidence-interval-based-on-this-standard-error-and-the-t-distribution." class="section level4">
<h4>(a) Use the formula s/√n to find the standard error of the mean. Give the usual 95% confidence interval based on this standard error and the t distribution.</h4>
<pre class="r"><code>iq &lt;- read.table(fromParentDir(&quot;data/IQ.txt&quot;), header = T)
str(iq)</code></pre>
<pre><code>&#39;data.frame&#39;:   60 obs. of  1 variable:
 $ IQscore: int  86 99 95 94 72 73 95 124 97 95 ...</code></pre>
<pre class="r"><code>qqnorm(iq$IQscore)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>n    &lt;- nrow(iq)
mean &lt;- mean(iq$IQscore)
sd   &lt;- sd(iq$IQscore)
se   &lt;- sd / sqrt(n)
q_val&lt;- qt(p = 0.025, df = n-1) 

mean + c(-1, 1)*se*abs(q_val)</code></pre>
<pre><code>[1]  93.99302 101.50698</code></pre>
<p>Compare with t-test</p>
<pre class="r"><code>t.test(iq$IQscore)$conf.int</code></pre>
<pre><code>[1]  93.99302 101.50698
attr(,&quot;conf.level&quot;)
[1] 0.95</code></pre>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Install the libraries boot and simpleboot and load them (using the facilities under the tab Packages in R or RStudio, or using the commands in the R script.) Bootstrap the mean of the IQ scores, with R = 10000 resamples. Make a histogram and Normal quantile plot of the bootstrap distribution. Does the bootstrap distribution appear Normal? What is the bootstrap standard error? Give the bootstrap t 95% confidence interval.</li>
</ol>
</blockquote>
<p>Only need to do this once:</p>
<pre class="r"><code>install.packages(c(&quot;boot&quot;, &quot;simpleboot&quot;))</code></pre>
<pre class="r"><code>library(boot)
library(simpleboot)</code></pre>
<pre class="r"><code>set.seed(12345)
bootresults &lt;- one.boot(iq$IQscore, mean, R = 10000)

mean(bootresults$t); sd(bootresults$t)</code></pre>
<pre><code>[1] 97.73412</code></pre>
<pre><code>[1] 1.847751</code></pre>
<pre class="r"><code>qqnorm(bootresults$t)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>hist(bootresults$t)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-8-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>Looks nicely normally distributed.</p>
<p>Now for the 95% CI:</p>
<pre class="r"><code>mean(bootresults$t) + c(-1,1)*qt(0.025, df = nrow(iq)-1, lower.tail = F)*sd(bootresults$t)/sqrt(nrow(iq))</code></pre>
<pre><code>[1] 97.25680 98.21145</code></pre>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Give the 95% confidence percentile, BCa, and basic intervals. How well do your five confidence intervals agree? Was bootstrapping needed to find a reasonable confidence interval, or was the formula-based confidence interval in (a) good enough?</li>
</ol>
</blockquote>
<pre class="r"><code>boot.ci(bootresults)</code></pre>
<pre><code>Warning in boot.ci(bootresults): bootstrap variances needed for studentized
intervals</code></pre>
<pre><code>BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 10000 bootstrap replicates

CALL : 
boot.ci(boot.out = bootresults)

Intervals : 
Level      Normal              Basic         
95%   ( 94.14, 101.39 )   ( 94.15, 101.38 )  

Level     Percentile            BCa          
95%   ( 94.12, 101.35 )   ( 94.08, 101.30 )  
Calculations and Intervals on Original Scale</code></pre>
<p>The bootstrapping intervals agree, they are all a little narrower than the t-test based confidence interval, however the difference is small.</p>
<p>This is an (approximately) normally distributed vriable, so the t-test interval is accurate.</p>
</div>
</div>
<div id="ex.-16.65-a-small-sample-permutation-test." class="section level3">
<h3>Ex. 16.65 A small-sample permutation test.</h3>
<blockquote>
<p>To illustrate the process, let’s perform a permutation test by hand for a small random subset of the DRP data (Example 16.11). Here are the data (different from those in the lecture): Treatment group 57 53 Control group 19 37 41 42 (a) Calculate the difference in means xtreatment − xcontrol between the two groups. This is the observed value of the statistic.</p>
</blockquote>
<pre class="r"><code>df &lt;- data.frame(
  treat = c(&quot;treat&quot;, &quot;treat&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;),
  x = c(57, 53, 19, 37, 41, 42)
)

diffx &lt;- df %&gt;% 
  group_by(treat) %&gt;% 
  summarize(mean = mean(x)) %&gt;%
  pull(mean) %&gt;%
  diff()
diffx</code></pre>
<pre><code>[1] 20.25</code></pre>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Resample: randomly shuffle the 6 scores, where the two first scores will have the treatment (1), and the last 4 have control (0). What is the difference in group means for this resample?</li>
</ol>
</blockquote>
<pre class="r"><code>set.seed(2)
df %&gt;%
  mutate(treat = sample(treat)) %&gt;% # shuffle treat
  group_by(treat) %&gt;%
  summarize(mean = mean(x)) %&gt;% # calculate mean by group
  pull(mean) %&gt;%
  diff() # get difference</code></pre>
<pre><code>[1] 8.25</code></pre>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Repeat step (b) 1000 times to get 1000 resamples and 1000 values of the statistic. (Obviously you will not do this 1000 times by hand. Do it a few times by hand until you understand the procedure, then proceed with R.) Make a table of the distribution of these 1000 values. This is the permutation distribution for your resamples.</li>
</ol>
</blockquote>
<pre class="r"><code>npermutations = 1000
set.seed(2)

diffs &lt;- numeric(npermutations)

for (i in 1:npermutations) {
  diffs[i] &lt;- df %&gt;%
    mutate(treat = sample(treat)) %&gt;% # shuffle treat
    group_by(treat) %&gt;%
    summarize(mean = mean(x)) %&gt;% # calculate mean by group
    pull(mean) %&gt;%
    diff() # get difference
}</code></pre>
<p>Look at the distribution of the differences</p>
<pre class="r"><code>hist(diffs)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>What proportion of the 1000 statistic values were equal to or greater than the original value in part (a)? You have just estimated the one-sided P-value for the original 6 observations.</li>
</ol>
</blockquote>
<pre class="r"><code>mean(diffs &gt;=diffx)</code></pre>
<pre><code>[1] 0.063</code></pre>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>For this small data set, there are only 15 possible permutations of the data. As a result, we can calculate the exact P-value by counting the number of permutations with a statistic value greater than or equal to the original value and then dividing by 15. What is the exact P-value here? How close was your estimate?</li>
</ol>
</blockquote>
<p>Create all permutations by making a matrix with all the possible positions in the data where treatment can be located</p>
<pre class="r"><code>treat_positions &lt;- combn(6,2)
treat_positions</code></pre>
<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
[1,]    1    1    1    1    1    2    2    2    2     3     3     3     4
[2,]    2    3    4    5    6    3    4    5    6     4     5     6     5
     [,14] [,15]
[1,]     4     5
[2,]     6     6</code></pre>
<p>Calculate differences</p>
<pre class="r"><code>possible_differences &lt;- apply(treat_positions, 2, function(pos) {
  mean(df$x[pos]) - mean(df$x[-pos])
})
sort(possible_differences)</code></pre>
<pre><code> [1] -20.25 -17.25 -16.50  -8.25  -5.25  -3.75  -3.00   0.00   5.25   8.25
[11]   8.25   9.00  11.25  12.00  20.25</code></pre>
<pre class="r"><code>mean(possible_differences &gt;= diffx)</code></pre>
<pre><code>[1] 0.06666667</code></pre>
<p>The exact p-value is pretty close</p>
</div>
<div id="comparing-serum-retinol-levels." class="section level3">
<h3>16.76 Comparing serum retinol levels.</h3>
<blockquote>
<p>The formal medical term for vitamin A in the blood is serum retinol. Serum retinol has various beneficial effects, such as protecting against fractures. Medical researchers working with children in Papua New Guinea asked whether recent infections reduce the level of serum retinol. They classified children as recently infected or not on the basis of other blood tests, then measured serum retinol. Of the 90 children in the sample, 55 had been recently infected. Dataset retinol.txt gives the serum retinol levels for both groups, in micromoles per liter. (a) The researchers are interested in the proportional reduction in serum retinol. Verify that the mean for infected children is 0.620 and that the mean for uninfected children is 0.778.</p>
</blockquote>
<pre class="r"><code>retinol &lt;- read.table(fromParentDir(&quot;data/retinol.txt&quot;), header = T)
str(retinol)</code></pre>
<pre><code>&#39;data.frame&#39;:   90 obs. of  2 variables:
 $ retinol : num  0.59 1.08 0.88 0.62 0.46 0.39 1.44 1.04 0.67 0.86 ...
 $ infected: int  0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<pre class="r"><code>retinol %&gt;%
  group_by(infected) %&gt;%
  summarize(mean(retinol))</code></pre>
<pre><code># A tibble: 2 x 2
  infected `mean(retinol)`
     &lt;int&gt;           &lt;dbl&gt;
1        0           0.778
2        1           0.620</code></pre>
<pre class="r"><code>ratio &lt;- retinol %&gt;%
  group_by(infected) %&gt;%
  summarize(mean = mean(retinol)) %&gt;%
  pull(mean) %&gt;%
  (function(x) (x/lag(x))[2])</code></pre>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>There is no standard test for the null hypothesis that the ratio of the population means is 1. We can do a permutation test on the ratio of sample means. Carry out a one-sided test and report the P-value. Briefly describe the center and shape of the permutation distribution. Why do you expect the center to be close to 1?</li>
</ol>
</blockquote>
<p>First let’s calculate the number of possible permutations, to see if calculating all permutations is feasible</p>
<pre class="r"><code>retinol %&gt;% 
  group_by(infected) %&gt;% # group by infected
  summarize(n = n())  # count number of infected</code></pre>
<pre><code># A tibble: 2 x 2
  infected     n
     &lt;int&gt; &lt;int&gt;
1        0    35
2        1    55</code></pre>
<pre class="r"><code>choose(35+55, 55)</code></pre>
<pre><code>[1] 1.132459e+25</code></pre>
<p>Getting all permutations is clearly infeasible, so let’s do 1000</p>
<pre class="r"><code>nperm  = 1000
ratios = numeric(nperm)
set.seed(2)

for (i in 1:nperm) {
  ratios[i] &lt;-
    retinol %&gt;%
      mutate(shuff_intected = sample(infected)) %&gt;%
      group_by(shuff_intected) %&gt;%
      summarize(mean = mean(retinol)) %&gt;%
      pull(mean) %&gt;%
      (function(x) (x / lag(x))[2]) # divide each element by the previous, take second element (which is the fraction if x is 2 numbers)
}</code></pre>
<pre class="r"><code>hist(ratios)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Distributions looks pretty symmetric around 1, which is the null value for a ratio</p>
<pre class="r"><code>qqnorm(ratios)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Pretty nice normal distribution</p>
<p>Now for the P-value</p>
<pre class="r"><code>mean(ratios &lt; ratio) </code></pre>
<pre><code>[1] 0.011</code></pre>
<p>With plot for observed ratio</p>
<pre class="r"><code>hist(ratios)
abline(v = ratio, lty = 2)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="methods-of-resampling." class="section level3">
<h3>16.77 Methods of resampling.</h3>
<blockquote>
<p>In the previous exercise we did a permutation test for the hypothesis “no difference between infected and uninfected children” using the ratio of mean serum retinol levels to measure “difference.” We might also want a bootstrap confidence interval for the ratio of population means for infected and uninfected children. Describe carefully how resampling is done for the permutation test and for the bootstrap, paying attention to the difference between the two resampling methods.</p>
</blockquote>
<p>Permution tests are used to simulate a distribution for the null-hypothesis. For this unpaired case it means shuffling the outcome (infected), which are expected to depend on the determinant (retinol). Under the null-hypothesis there is no relationship between the determinant and the outcome, so each possible configuration of outcomes is equially likely to occur. A number of random configurations is simulated, and the test statistic is calculated for each permution. This gives the distribution of the null-hypothesis. Based on this distribution, we can see how probable it is to find a value for the statistic or more extreme, which is the exact definition of a p-value. So permutations give p-values</p>
<p>Bootstrapping simulates drawing random samples from the population, but uses the actual observed distribution in the sample as a proxy for the population distribution (which is usually unknown if you’re doing research). Drawing re-samples from this sample distribution and calculating the test statistic for each re-sample gives an idea of the range of possible values of the statistic for a sample of the given size. This distrubution of the sample statistic can be used to approximate the confidence interval, which can (loosely) be defined as the range of values which a sample test statistic is (e.g. 95%) likelily to fall in between, when taking many samples.</p>
</div>
<div id="calcium-intake-and-blood-pressure.-exercise-from-previous-edition" class="section level3">
<h3>Calcium intake and blood pressure. (Exercise from previous edition)</h3>
<blockquote>
<p>Does added calcium intake reduce the blood pressure of black men? In a randomized comparative double-blind trial, 10 men were given a calcium supplement for twelve weeks and 11 others received a placebo. Whether or not blood pressure dropped was recorded for each subject. Here are the data: Treatment Subjects Successes Proportion Calcium 10 6 0.60 Placebo 11 4 0.36 Total 21 10 0.48</p>
</blockquote>
<pre class="r"><code>calcium &lt;- data.frame(
  treatment = rep(c(1, 0), c(10, 11)),
  success   = rep(c(1, 0, 1, 0), c(6, 4, 4, 7))
)
calcium</code></pre>
<pre><code>   treatment success
1          1       1
2          1       1
3          1       1
4          1       1
5          1       1
6          1       1
7          1       0
8          1       0
9          1       0
10         1       0
11         0       1
12         0       1
13         0       1
14         0       1
15         0       0
16         0       0
17         0       0
18         0       0
19         0       0
20         0       0
21         0       0</code></pre>
<blockquote>
<p>We want to use these sample data to test equality of the population proportions of successes. Carry out a permutation test. Describe the permutation distribution. The permutation test does not depend on a “nice” distribution shape. Give the P-value and report your conclusion.</p>
</blockquote>
<p>The question here is to randomly divide 10 successes over 21 subjects.</p>
<p>The possible number of permutations:</p>
<pre class="r"><code>choose(21, 10)</code></pre>
<pre><code>[1] 352716</code></pre>
<p>This is doable.</p>
<p>We can get all the permutations by assigning 10 ‘rownumbers’ or positions for the successes in the 21 participants.</p>
<pre class="r"><code>positions &lt;- combn(21, 10)

positions[, 1:5]</code></pre>
<pre><code>      [,1] [,2] [,3] [,4] [,5]
 [1,]    1    1    1    1    1
 [2,]    2    2    2    2    2
 [3,]    3    3    3    3    3
 [4,]    4    4    4    4    4
 [5,]    5    5    5    5    5
 [6,]    6    6    6    6    6
 [7,]    7    7    7    7    7
 [8,]    8    8    8    8    8
 [9,]    9    9    9    9    9
[10,]   10   11   12   13   14</code></pre>
<p>Let’s make sure that all treated participants are in the first 10 rows.</p>
<p>The the number of successes in the treatment group is the number of times a position of <span class="math inline">\(\leq 10\)</span> happens in the permuted positions. Let’s call this <span class="math inline">\(n\)</span>, than the difference in ratios is <span class="math inline">\(\frac{n}{10}-\frac{10-n}{11}\)</span></p>
<pre class="r"><code>diffs &lt;- apply(positions, 2, function(position) {
  n = sum(position &lt;= 10)
  (n/10) - ((10-n)/11)
})</code></pre>
<p>Look at the distribution</p>
<pre class="r"><code>hist(diffs)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-30-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Compare with observed ratio to get a p-value</p>
<pre class="r"><code>diffx &lt;- 6/10 - 4 / 11

mean(diffx &lt;= diffs)</code></pre>
<pre><code>[1] 0.2599428</code></pre>
<p>Now check with pre-defined functions:</p>
<pre class="r"><code>perm::permTS(calcium$success ~ calcium$treatment, alternative = &quot;less&quot;, method = &quot;exact.ce&quot;)</code></pre>
<pre><code>
    Exact Permutation Test (complete enumeration)

data:  calcium$success by calcium$treatment
p-value = 0.2599
alternative hypothesis: true mean calcium$treatment=0 - mean calcium$treatment=1 is less than 0
sample estimates:
mean calcium$treatment=0 - mean calcium$treatment=1 
                                         -0.2363636 </code></pre>
<pre class="r"><code>fisher.test(calcium$success, calcium$treatment, alternative = &quot;greater&quot;)</code></pre>
<pre><code>
    Fisher&#39;s Exact Test for Count Data

data:  calcium$success and calcium$treatment
p-value = 0.2599
alternative hypothesis: true odds ratio is greater than 1
95 percent confidence interval:
 0.436804      Inf
sample estimates:
odds ratio 
  2.502462 </code></pre>
<p>Our manual calculation coincides with Fisher’s test and permTS method</p>
<p>Internally, <code>permTS</code> does something similar as we did with the permutations It creates a ‘cm’ matrix with all possible combinations, the dimensions of this matrix is 352716x21. Check the code with:</p>
<p><code>View(permute:::permTS.default)</code> which calls: <code>View(permute:::twosample.exact.ce)</code> which calls: <code>View(permute:::chooseMatrix)</code></p>
</div>
</div>
<div id="survival-exercises" class="section level2">
<h2>Survival Exercises</h2>
<div id="leukemia" class="section level3">
<h3>1. Leukemia</h3>
<blockquote>
<p>We will have a closer look at the data from the lecture, on remission times for leukemia:</p>
</blockquote>
<blockquote>
<p>Read in the dataset leukaemia.txt (saved as a flat text file), changing your working directory first or adding the path name to the file name:</p>
</blockquote>
<pre class="r"><code>leuk&lt;-read.table(fromParentDir(&quot;data/leukaemia.txt&quot;), header = TRUE)</code></pre>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Fit five models (name them fit0 – fit4) with the following predictor variables: none (intercept only), GROUP, logWBCC, GROUP and logWBCC, GROUP*logWBCC. fit0&lt;-coxph(Surv(TIME, STATUS)~1, data=leuk) etc.</li>
</ol>
</blockquote>
<pre class="r"><code>require(survival)
fit0 &lt;- coxph(Surv(TIME,  STATUS) ~ 1, data = leuk)
fit1 &lt;- coxph(Surv(TIME,  STATUS) ~ GROUP, data = leuk)
fit2 &lt;- coxph(Surv(TIME,  STATUS) ~ logWBCC, data = leuk)
fit3 &lt;- coxph(Surv(TIME,  STATUS) ~ GROUP + logWBCC, data = leuk)
fit4 &lt;- coxph(Surv(TIME,  STATUS) ~ GROUP * logWBCC, data = leuk)</code></pre>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Ask for the log-likelihood of the empty model: fit0$loglik Ask for the summaries of the 4 models fit1-fit4, and for their log-likelihoods.</li>
</ol>
</blockquote>
<pre class="r"><code>fit0$loglik</code></pre>
<pre><code>[1] -93.18427</code></pre>
<p>Let’s combine the summaries of the models by converting them to data.frames with <code>broom</code>, and bind them together with <code>rbindlist</code></p>
<pre class="r"><code>fits &lt;- list(fit1, fit2, fit3, fit4)

summaries &lt;- map(fits, broom::tidy) %&gt;%
  data.table::rbindlist(idcol = &quot;model&quot;)

summaries</code></pre>
<pre><code>   model                  term   estimate std.error  statistic
1:     1         GROUPtreatmnt -1.5721251 0.4123967 -3.8121670
2:     2               logWBCC  1.6464371 0.2979940  5.5250677
3:     3         GROUPtreatmnt -1.3860755 0.4247984 -3.2629020
4:     3               logWBCC  1.6908904 0.3358976  5.0339462
5:     4         GROUPtreatmnt -2.3749100 1.7054652 -1.3925291
6:     4               logWBCC  1.5548945 0.3986606  3.9002964
7:     4 GROUPtreatmnt:logWBCC  0.3175219 0.5257887  0.6038963
        p.value   conf.low  conf.high
1: 1.377538e-04 -2.3804079 -0.7638424
2: 3.293585e-08  1.0623795  2.2304946
3: 1.102776e-03 -2.2186651 -0.5534860
4: 4.804846e-07  1.0325432  2.3492376
5: 1.637622e-01 -5.7175604  0.9677404
6: 9.607498e-05  0.7735341  2.3362550
7: 5.459126e-01 -0.7130050  1.3480487</code></pre>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Interpret the coefficients of the models 1 through 4. What are you assuming with these models about the effect of group and logWBCC on the hazard of relapse?</li>
</ol>
</blockquote>
<pre class="r"><code>require(data.table)
summaries[order(term)]</code></pre>
<pre><code>   model                  term   estimate std.error  statistic
1:     1         GROUPtreatmnt -1.5721251 0.4123967 -3.8121670
2:     3         GROUPtreatmnt -1.3860755 0.4247984 -3.2629020
3:     4         GROUPtreatmnt -2.3749100 1.7054652 -1.3925291
4:     4 GROUPtreatmnt:logWBCC  0.3175219 0.5257887  0.6038963
5:     2               logWBCC  1.6464371 0.2979940  5.5250677
6:     3               logWBCC  1.6908904 0.3358976  5.0339462
7:     4               logWBCC  1.5548945 0.3986606  3.9002964
        p.value   conf.low  conf.high
1: 1.377538e-04 -2.3804079 -0.7638424
2: 1.102776e-03 -2.2186651 -0.5534860
3: 1.637622e-01 -5.7175604  0.9677404
4: 5.459126e-01 -0.7130050  1.3480487
5: 3.293585e-08  1.0623795  2.2304946
6: 4.804846e-07  1.0325432  2.3492376
7: 9.607498e-05  0.7735341  2.3362550</code></pre>
<p>In models 1 and 3 there is a significant group effect of GROUP (decreased hazard). In all models, logWBCC seems to have a significant effect. The interaction term of model 4 can be left out.</p>
<p>Estimates higher than 0 correspond with increased hazards. The exponent of this estimate gives the hazard ratio (leaving the rest the same)</p>
<p>Assumptions are proportional hazards, so the effects of group and logWBCC on the hazard do not depend on time (the hazard ratios are constant in time).</p>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Calculate likelihood ratio test values and AIC’s (assume that the empty model, fit0, uses no degrees of freedom)</li>
</ol>
</blockquote>
<pre class="r"><code>anova(fit0, fit1, fit2, fit3, fit4)</code></pre>
<pre><code>Analysis of Deviance Table
 Cox model: response is  Surv(TIME, STATUS)
 Model 1: ~ 1
 Model 2: ~ GROUP
 Model 3: ~ logWBCC
 Model 4: ~ GROUP + logWBCC
 Model 5: ~ GROUP * logWBCC
   loglik   Chisq Df P(&gt;|Chi|)    
1 -93.184                         
2 -85.008 16.3517  1 5.261e-05 ***
3 -75.763 18.4899  0 &lt; 2.2e-16 ***
4 -69.828 11.8708  1 0.0005702 ***
5 -69.648  0.3594  1 0.5488232    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(fit0, fit1, fit2, fit3, fit4)</code></pre>
<pre><code>Warning in is.na(coefficients(object)): is.na() applied to non-(list or
vector) of type &#39;NULL&#39;</code></pre>
<pre><code>     df      AIC
fit0  0       NA
fit1  1 172.0168
fit2  1 153.5270
fit3  2 143.6562
fit4  3 145.2968</code></pre>
<p>The lowest AIC is for fit3, both terms but no interaction. We already observed earlier on that the interaction term in model 4 was not significant, so it must be left out (AIC penalizes extra terms)</p>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Also compare the models via the likelihood ratio test, with commands like anova(fit1, fit3, test=“Chisq”) Note that not all comparisons are possible.</li>
</ol>
</blockquote>
<pre class="r"><code>anova(fit1, fit2, test = &quot;Chisq&quot;)</code></pre>
<pre><code>Analysis of Deviance Table
 Cox model: response is  Surv(TIME, STATUS)
 Model 1: ~ GROUP
 Model 2: ~ logWBCC
   loglik Chisq Df P(&gt;|Chi|)    
1 -85.008                       
2 -75.763 18.49  0 &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The anova gives a result, but fit2 is not nested within fit1, so it’s meaningless.s</p>
<blockquote>
<ol start="6" style="list-style-type: lower-alpha">
<li>Decide which model is best. Interpret the coefficients of this best model.</li>
</ol>
</blockquote>
<p>We already saw that model 3 has the lowest AIC, so we will pick that one</p>
<pre class="r"><code>summary(fit3)</code></pre>
<pre><code>Call:
coxph(formula = Surv(TIME, STATUS) ~ GROUP + logWBCC, data = leuk)

  n= 42, number of events= 30 

                 coef exp(coef) se(coef)      z Pr(&gt;|z|)    
GROUPtreatmnt -1.3861    0.2501   0.4248 -3.263   0.0011 ** 
logWBCC        1.6909    5.4243   0.3359  5.034  4.8e-07 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

              exp(coef) exp(-coef) lower .95 upper .95
GROUPtreatmnt    0.2501     3.9991    0.1088    0.5749
logWBCC          5.4243     0.1844    2.8082   10.4776

Concordance= 0.852  (se = 0.062 )
Rsquare= 0.671   (max possible= 0.988 )
Likelihood ratio test= 46.71  on 2 df,   p=7.187e-11
Wald test            = 33.6  on 2 df,   p=5.061e-08
Score (logrank) test = 46.07  on 2 df,   p=9.921e-11</code></pre>
<p>Treatment reduces the hazard, logWBCC increases the hazard</p>
</div>
<div id="nephrectomy" class="section level3">
<h3>2. Nephrectomy</h3>
<blockquote>
<p>Researchers are interested in the influence of age nephrectomy on the survival of hypernephroma patients. Read the data below into R from the file nephrect.txt:</p>
</blockquote>
<pre class="r"><code>neph&lt;-read.table(fromParentDir(&quot;data/nephrect.txt&quot;), header = TRUE)
str(neph)</code></pre>
<pre><code>&#39;data.frame&#39;:   36 obs. of  4 variables:
 $ survtime: int  9 6 21 15 8 17 12 104 9 56 ...
 $ status  : int  1 1 1 1 1 1 1 0 1 1 ...
 $ nephrect: int  0 0 0 0 0 0 0 1 1 1 ...
 $ age     : int  1 1 1 2 2 2 3 1 1 1 ...</code></pre>
<blockquote>
<p>Survival times of 36 hypernephroma patients classified according to age group and whether or not they have had a nephrectomy.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Make Kaplan-Meier plots for the three age groups, and test whether there is an effect of age on survival time (not taking nephrectomy into account) using the Log-rank test.</li>
</ol>
</blockquote>
<pre class="r"><code>plot(survfit(Surv(survtime, status)~age, data = neph))</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>survdiff(Surv(survtime, status)~age, data = neph)</code></pre>
<pre><code>Call:
survdiff(formula = Surv(survtime, status) ~ age, data = neph)

       N Observed Expected (O-E)^2/E (O-E)^2/V
age=1 19       17    19.59    0.3424    1.0150
age=2 12       10    10.77    0.0554    0.0952
age=3  5        5     1.64    6.9042    8.2183

 Chisq= 8.3  on 2 degrees of freedom, p= 0.0161 </code></pre>
<p>According to the log-rank test, there is a significant difference in survival between age groups. (see documentation: the default for <code>rho</code> is 0, which corresponds to the log-rank test)</p>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Test whether there is an effect of age on survival time (not taking nephrectomy into account) using the Likelihood Ratio test and the Wald test from a Cox Proportional Hazards regression model. (Define AGE as a categorical variable.) Compare this result to the log-rank test.</li>
</ol>
</blockquote>
<pre class="r"><code>fit1 &lt;- coxph(Surv(survtime, status) ~ factor(age), data = neph)
summary(fit1)</code></pre>
<pre><code>Call:
coxph(formula = Surv(survtime, status) ~ factor(age), data = neph)

  n= 36, number of events= 32 

               coef exp(coef) se(coef)     z Pr(&gt;|z|)   
factor(age)2 0.1010    1.1063   0.4204 0.240  0.81004   
factor(age)3 1.5144    4.5467   0.5807 2.608  0.00911 **
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

             exp(coef) exp(-coef) lower .95 upper .95
factor(age)2     1.106     0.9039    0.4853     2.522
factor(age)3     4.547     0.2199    1.4569    14.189

Concordance= 0.602  (se = 0.054 )
Rsquare= 0.148   (max possible= 0.993 )
Likelihood ratio test= 5.79  on 2 df,   p=0.05542
Wald test            = 7.09  on 2 df,   p=0.02881
Score (logrank) test = 8.4  on 2 df,   p=0.01498</code></pre>
<p>The log-rank gives the lowest p-value. Likelihood ratio test is not significant</p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>From the Cox PH output, give the hazard ratio of age class 2 vs 1 and 3 vs 1. Give the 95% confidence intervals for these hazard ratios. Interpret the results.</li>
</ol>
</blockquote>
<pre class="r"><code>broom::tidy(fit1) %&gt;%
  mutate_at(.vars = c(&quot;estimate&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;), exp)</code></pre>
<pre><code>          term estimate std.error statistic     p.value  conf.low
1 factor(age)2 1.106331 0.4203911 0.2403689 0.810044275 0.4853415
2 factor(age)3 4.546690 0.5806640 2.6080477 0.009106028 1.4569116
  conf.high
1  2.521869
2 14.189183</code></pre>
<p>Hazard ratios are the <code>estimate</code>s (exponentiated as required). For the 2 vs 1 hazard ratio, the confidence interval includes the null value of 1, which coincides with the p.value of &gt; 0.05</p>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Now take only nephrectomy as the explanatory variable. Make Kaplan-Meier plots for the two groups, test for a difference in nephrectomy status using the log-rank test.</li>
</ol>
</blockquote>
<pre class="r"><code>plot(survfit(Surv(survtime, status)~nephrect, data = neph))</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-46-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>survdiff(Surv(survtime, status)~nephrect, data = neph)</code></pre>
<pre><code>Call:
survdiff(formula = Surv(survtime, status) ~ nephrect, data = neph)

            N Observed Expected (O-E)^2/E (O-E)^2/V
nephrect=0  7        7     2.46     8.408      10.5
nephrect=1 29       25    29.54     0.699      10.5

 Chisq= 10.5  on 1 degrees of freedom, p= 0.00122 </code></pre>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Test whether there is an effect of nephrectomy using the likelihood ratio and Wald tests in a Cox PH model. Compare this result to the log-rank test. Give the hazard ratio of no nephrectomy vs nephrectomy. Calculate a 95% confidence interval for this hazard ratio.</li>
</ol>
</blockquote>
<pre class="r"><code>fit2 &lt;- coxph(Surv(survtime, status)~nephrect, data = neph)
summary(fit2)</code></pre>
<pre><code>Call:
coxph(formula = Surv(survtime, status) ~ nephrect, data = neph)

  n= 36, number of events= 32 

            coef exp(coef) se(coef)      z Pr(&gt;|z|)   
nephrect -1.4941    0.2245   0.5070 -2.947  0.00321 **
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

         exp(coef) exp(-coef) lower .95 upper .95
nephrect    0.2245      4.455    0.0831    0.6063

Concordance= 0.598  (se = 0.037 )
Rsquare= 0.19   (max possible= 0.993 )
Likelihood ratio test= 7.56  on 1 df,   p=0.005955
Wald test            = 8.69  on 1 df,   p=0.003208
Score (logrank) test = 10.31  on 1 df,   p=0.00132</code></pre>
<pre class="r"><code>exp(coef(fit2))</code></pre>
<pre><code> nephrect 
0.2244505 </code></pre>
<pre class="r"><code>exp(confint((fit2)))</code></pre>
<pre><code>              2.5 %    97.5 %
nephrect 0.08309507 0.6062696</code></pre>
<p>Nefrectomy seems to have a lower hazard of dying</p>
<blockquote>
<ol start="6" style="list-style-type: lower-alpha">
<li>Test for a difference in nephrectomy status using a Cox PH model again, but now for each of the three original age classes separately (hint: make 3 new data frames with selections of age groups). Does the effect of nephrecto¬my appear to be the same in each age class? (Optional: how would you test this, using Cox PH?)</li>
</ol>
</blockquote>
<p>Let’s do this with <code>dplyr</code> function <code>group_by</code></p>
<pre class="r"><code>fits &lt;- neph %&gt;%
  group_by(age) %&gt;%
  do(fit = coxph(Surv(survtime, status)~nephrect, data = .))

broom::tidy(fits, fit)   </code></pre>
<pre><code># A tibble: 3 x 8
# Groups: age [3]
    age term     estimate std.error statistic p.value conf.low conf.high
  &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1     1 nephrect   -2.00      0.827    -2.42   0.0156    -3.62   -0.379 
2     2 nephrect   -1.71      0.922    -1.86   0.0629    -3.52    0.0923
3     3 nephrect    0.370     1.19      0.311  0.755     -1.96    2.70  </code></pre>
<p>The estimate for nephrectomy is different in the different age groups, however, the confidence intervals all overlap.</p>
<p>We can test this by using <code>age</code> as an interaction</p>
<pre class="r"><code>fit2 &lt;- coxph(Surv(survtime, status)~nephrect*factor(age), data = neph)
summary(fit2)</code></pre>
<pre><code>Call:
coxph(formula = Surv(survtime, status) ~ nephrect * factor(age), 
    data = neph)

  n= 36, number of events= 32 

                           coef exp(coef)  se(coef)      z Pr(&gt;|z|)   
nephrect              -1.996683  0.135785  0.729653 -2.736  0.00621 **
factor(age)2          -0.032209  0.968304  0.836657 -0.038  0.96929   
factor(age)3           0.002361  1.002364  1.179871  0.002  0.99840   
nephrect:factor(age)2  0.002939  1.002944  0.971967  0.003  0.99759   
nephrect:factor(age)3  2.140991  8.507868  1.342752  1.594  0.11083   
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

                      exp(coef) exp(-coef) lower .95 upper .95
nephrect                 0.1358     7.3646   0.03249    0.5675
factor(age)2             0.9683     1.0327   0.18787    4.9909
factor(age)3             1.0024     0.9976   0.09925   10.1236
nephrect:factor(age)2    1.0029     0.9971   0.14926    6.7393
nephrect:factor(age)3    8.5079     0.1175   0.61216  118.2424

Concordance= 0.653  (se = 0.057 )
Rsquare= 0.354   (max possible= 0.993 )
Likelihood ratio test= 15.75  on 5 df,   p=0.007586
Wald test            = 14.94  on 5 df,   p=0.01061
Score (logrank) test = 19.98  on 5 df,   p=0.00126</code></pre>
<p>Interaction term is indeed not significant</p>
<blockquote>
<ol start="7" style="list-style-type: lower-alpha">
<li>Check the Cox PH assumption for AGE by building a model with NEPHRECT as explanatory variable and AGE as stratum variable. Get a log-minus-log plot and check whether the resulting log cumulative hazard functions are approximately parallel.</li>
</ol>
</blockquote>
<pre class="r"><code>fit3 &lt;- coxph(Surv(survtime, status)~nephrect + strata(factor(age)), data = neph)
summary(fit3)</code></pre>
<pre><code>Call:
coxph(formula = Surv(survtime, status) ~ nephrect + strata(factor(age)), 
    data = neph)

  n= 36, number of events= 32 

            coef exp(coef) se(coef)      z Pr(&gt;|z|)  
nephrect -1.3077    0.2705   0.5399 -2.422   0.0154 *
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

         exp(coef) exp(-coef) lower .95 upper .95
nephrect    0.2705      3.698   0.09387    0.7792

Concordance= 0.607  (se = 0.055 )
Rsquare= 0.14   (max possible= 0.962 )
Likelihood ratio test= 5.42  on 1 df,   p=0.0199
Wald test            = 5.87  on 1 df,   p=0.01543
Score (logrank) test = 6.57  on 1 df,   p=0.01035</code></pre>
<p>The fast way:</p>
<pre class="r"><code>plot(survfit(fit3), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;black&quot;), fun = &quot;cloglog&quot;)</code></pre>
<p><img src="figure/assignments_week2.Rmd/unnamed-chunk-52-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The lines cross, so this vialates our assumption of proportional hazards</p>
</div>
<div id="myeloma" class="section level3">
<h3>3. Myeloma</h3>
<blockquote>
<p>Read the data file myeloma.dat (a comma-separated file with headers at the top) into R and attach it:</p>
</blockquote>
<pre class="r"><code>myel&lt;-read.table(fromParentDir(&quot;data/myeloma.dat&quot;), header = TRUE)
str(myel)</code></pre>
<pre><code>&#39;data.frame&#39;:   48 obs. of  10 variables:
 $ patient: int  1 2 3 4 5 6 7 8 9 10 ...
 $ time   : int  13 52 6 40 10 7 66 10 10 14 ...
 $ status : int  1 0 1 1 1 0 1 0 1 1 ...
 $ age    : int  66 66 53 69 65 57 52 60 70 70 ...
 $ sex    : int  1 1 0 1 1 0 1 1 1 1 ...
 $ bun    : int  25 13 15 10 20 12 21 41 37 40 ...
 $ ca     : int  10 11 13 10 10 8 10 9 12 11 ...
 $ hb     : num  14.6 12 11.4 10.2 13.2 9.9 12.8 14 7.5 10.6 ...
 $ pc     : int  18 100 33 30 66 45 11 70 47 27 ...
 $ bj     : int  1 0 1 1 0 0 1 1 0 0 ...</code></pre>
<blockquote>
<p>The dataset contains the following variables on 48 patients diagnosed with multiple myeloma: Variable Description survtime survival time from diagnosis multiple myeloma (months) status censoring status (0=censored, 1=death from multiple myeloma) age age (years) sex sex (1=male, 2=female) bun blood urea nitrogen ca serum calcium hb haemoglobin pc percentage plasma cells in bone marrow bj Bence-Jones protein in urine</p>
</blockquote>
<blockquote>
<p>The object of the research is to determine the best fitting Cox proportional hazards model to explain death from multiple myeloma.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>First, get some descriptive statistics on the data. Then get a sense of the individual variables’ effect on survival, by fitting a Cox PH model on each of the explanatory variables separately.</li>
</ol>
</blockquote>
<pre class="r"><code>summary(myel)</code></pre>
<pre><code>    patient           time           status          age       
 Min.   : 1.00   Min.   : 1.00   Min.   :0.00   Min.   :50.00  
 1st Qu.:12.75   1st Qu.: 6.75   1st Qu.:0.75   1st Qu.:58.75  
 Median :24.50   Median :14.50   Median :1.00   Median :62.50  
 Mean   :24.50   Mean   :23.38   Mean   :0.75   Mean   :62.90  
 3rd Qu.:36.25   3rd Qu.:37.00   3rd Qu.:1.00   3rd Qu.:68.25  
 Max.   :48.00   Max.   :91.00   Max.   :1.00   Max.   :77.00  
      sex              bun               ca               hb       
 Min.   :0.0000   Min.   :  6.00   Min.   : 8.000   Min.   : 4.90  
 1st Qu.:0.0000   1st Qu.: 13.75   1st Qu.: 9.000   1st Qu.: 8.65  
 Median :1.0000   Median : 21.00   Median :10.000   Median :10.20  
 Mean   :0.6042   Mean   : 33.92   Mean   : 9.938   Mean   :10.25  
 3rd Qu.:1.0000   3rd Qu.: 39.25   3rd Qu.:10.000   3rd Qu.:12.57  
 Max.   :1.0000   Max.   :172.00   Max.   :15.000   Max.   :14.60  
       pc               bj        
 Min.   :  3.00   Min.   :0.0000  
 1st Qu.: 21.25   1st Qu.:0.0000  
 Median : 33.00   Median :0.0000  
 Mean   : 42.94   Mean   :0.3125  
 3rd Qu.: 63.00   3rd Qu.:1.0000  
 Max.   :100.00   Max.   :1.0000  </code></pre>
<p>Let’s model all variables separately (except for patient, time and status)</p>
<pre class="r"><code>vars &lt;- setdiff(colnames(myel), c(&quot;patient&quot;, &quot;time&quot;, &quot;status&quot;))

fits &lt;- map(vars, .f = function(var) {
  fit = coxph(Surv(time, status)~get(var), data = myel)
})
names(fits) &lt;- vars

fits_summary &lt;- map_dfr(fits, broom::tidy, .id = &quot;var&quot;)
fits_summary</code></pre>
<pre><code>  var     term     estimate   std.error  statistic     p.value
1 age get(var)  0.010370389 0.027974456  0.3707092 0.710854121
2 sex get(var) -0.064606144 0.354630810 -0.1821786 0.855442567
3 bun get(var)  0.020151980 0.005726557  3.5190396 0.000433112
4  ca get(var) -0.088886279 0.132472799 -0.6709776 0.502234810
5  hb get(var) -0.134673527 0.059294189 -2.2712770 0.023130216
6  pc get(var)  0.001587994 0.005680465  0.2795535 0.779820101
7  bj get(var) -0.553957353 0.387880839 -1.4281637 0.153244733
      conf.low   conf.high
1 -0.044458538  0.06519932
2 -0.759669760  0.63045747
3  0.008928135  0.03137582
4 -0.348528193  0.17075564
5 -0.250888002 -0.01845905
6 -0.009545514  0.01272150
7 -1.314189828  0.20627512</code></pre>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Now, fit an empty model (no explanatory variables), and use the forward method (use the add1() function, the option test=”Chisq” for a likelihood ratio test, and the scope option to indicate which variables may be included) to build a final model.</li>
</ol>
</blockquote>
<pre class="r"><code>fit0 &lt;- coxph(Surv(time, status)~1, data = myel)

add1(fit0, scope = vars, test = &quot;Chisq&quot;)</code></pre>
<pre><code>Warning in is.na(fit$coefficients): is.na() applied to non-(list or vector)
of type &#39;NULL&#39;</code></pre>
<pre><code>Single term additions

Model:
Surv(time, status) ~ 1
       Df    AIC    LRT Pr(&gt;Chi)   
&lt;none&gt;    214.68                   
age     1 216.54 0.1378 0.710431   
sex     1 216.65 0.0330 0.855821   
bun     1 207.32 9.3624 0.002215 **
ca      1 216.20 0.4746 0.490884   
hb      1 211.66 5.0169 0.025101 * 
pc      1 216.60 0.0773 0.780977   
bj      1 214.51 2.1680 0.140913   
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>fit1 &lt;- coxph(Surv(time, status)~bun, data = myel)
add1(fit1, scope = vars, test = &quot;Chisq&quot;)</code></pre>
<pre><code>Single term additions

Model:
Surv(time, status) ~ bun
       Df    AIC    LRT Pr(&gt;Chi)  
&lt;none&gt;    207.32                  
age     1 209.32 0.0000  0.99531  
sex     1 209.30 0.0144  0.90438  
bun     0 207.32 0.0000           
ca      1 209.28 0.0398  0.84194  
hb      1 204.70 4.6171  0.03165 *
pc      1 209.04 0.2805  0.59635  
bj      1 204.99 4.3274  0.03750 *
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>fit2 &lt;- coxph(Surv(time, status)~bun+hb, data = myel)
add1(fit2, scope = vars, test = &quot;Chisq&quot;)</code></pre>
<pre><code>Single term additions

Model:
Surv(time, status) ~ bun + hb
       Df    AIC     LRT Pr(&gt;Chi)  
&lt;none&gt;    204.70                   
age     1 206.45 0.24630   0.6197  
sex     1 206.31 0.39250   0.5310  
bun     0 204.70 0.00000           
ca      1 206.70 0.00067   0.9794  
hb      0 204.70 0.00000           
pc      1 206.47 0.22516   0.6351  
bj      1 203.90 2.79772   0.0944 .
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>No more significant improvements</p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>The re¬searchers also wish to know whether the effects of the risk factors might be modified by the age or sex of a patient, so, in the final model, test for possible interactions with these two. Note: to fit the interaction term you will also have to fit the main effect (age or sex).</li>
</ol>
</blockquote>
<pre class="r"><code>fit3 &lt;- coxph(Surv(time, status)~(bun+hb)*(age + sex), data = myel)
summary(fit3)</code></pre>
<pre><code>Call:
coxph(formula = Surv(time, status) ~ (bun + hb) * (age + sex), 
    data = myel)

  n= 48, number of events= 36 

              coef  exp(coef)   se(coef)      z Pr(&gt;|z|)
bun     -0.0005735  0.9994266  0.0402039 -0.014    0.989
hb      -0.3425636  0.7099480  0.6870670 -0.499    0.618
age     -0.0640206  0.9379857  0.1125184 -0.569    0.569
sex      0.2781463  1.3206795  1.4752258  0.189    0.850
bun:age  0.0003291  1.0003291  0.0006804  0.484    0.629
bun:sex  0.0031629  1.0031679  0.0120207  0.263    0.792
hb:age   0.0030344  1.0030391  0.0104302  0.291    0.771
hb:sex  -0.0151251  0.9849887  0.1445146 -0.105    0.917

        exp(coef) exp(-coef) lower .95 upper .95
bun        0.9994     1.0006    0.9237     1.081
hb         0.7099     1.4086    0.1847     2.729
age        0.9380     1.0661    0.7524     1.169
sex        1.3207     0.7572    0.0733    23.796
bun:age    1.0003     0.9997    0.9990     1.002
bun:sex    1.0032     0.9968    0.9798     1.027
hb:age     1.0030     0.9970    0.9827     1.024
hb:sex     0.9850     1.0152    0.7420     1.308

Concordance= 0.675  (se = 0.06 )
Rsquare= 0.275   (max possible= 0.989 )
Likelihood ratio test= 15.46  on 8 df,   p=0.05081
Wald test            = 17.62  on 8 df,   p=0.02428
Score (logrank) test = 21.28  on 8 df,   p=0.006433</code></pre>
<p>There seems to be no significant interaction</p>
<pre class="r"><code>drop1(fit3)</code></pre>
<pre><code>Single term deletions

Model:
Surv(time, status) ~ (bun + hb) * (age + sex)
        Df    AIC
&lt;none&gt;     215.22
bun:age  1 213.45
bun:sex  1 213.29
hb:age   1 213.31
hb:sex   1 213.23</code></pre>
<p>Dropping the interactions improves the model, so final model is model 2</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Interpret the coefficients from your final model (direction and size of the hazard ratios).</li>
</ol>
<pre class="r"><code>summary(fit2)</code></pre>
<pre><code>Call:
coxph(formula = Surv(time, status) ~ bun + hb, data = myel)

  n= 48, number of events= 36 

         coef exp(coef)  se(coef)      z Pr(&gt;|z|)    
bun  0.020043  1.020245  0.005816  3.446 0.000569 ***
hb  -0.134952  0.873758  0.061956 -2.178 0.029391 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

    exp(coef) exp(-coef) lower .95 upper .95
bun    1.0202     0.9802    1.0087    1.0319
hb     0.8738     1.1445    0.7738    0.9866

Concordance= 0.67  (se = 0.06 )
Rsquare= 0.253   (max possible= 0.989 )
Likelihood ratio test= 13.98  on 2 df,   p=0.0009213
Wald test            = 16.11  on 2 df,   p=0.0003173
Score (logrank) test = 19.54  on 2 df,   p=5.712e-05</code></pre>
<p><code>bun</code> increases the hazard of dying (see <code>exp(coef)</code> for hazard ratio) <code>hb</code> decreases the hazard of dying (see <code>exp(coef)</code> for hazard ratio)</p>
</div>
<div id="nephrectomy-spss" class="section level3">
<h3>4. Nephrectomy SPSS</h3>
<blockquote>
<p>This exercise is a repeat of exercise 2, but in SPSS. Read the data from exercise 2 into SPSS (File, Read Text Data) from the file NEPHRECT.TXT.</p>
</blockquote>
<p>This one was skipped</p>
</div>
<div id="myeloma-spss" class="section level3">
<h3>5. Myeloma SPSS</h3>
<blockquote>
<p>This exercise is a repeat of exercise 3, but in SPSS. Open the data file MYELOMA.POR. Use Utilities, Variables… to study the meaning of the variables in the data file.</p>
</blockquote>
<p>This one was skipped</p>
</div>
</div>
<div id="session-information" class="section level2">
<h2>Session information</h2>
<!-- Insert the session information into the document -->
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.4.3 (2017-11-30)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Sierra 10.12.6

Matrix products: default
BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] data.table_1.10.4-3 survival_2.41-3     bindrcpp_0.2       
 [4] simpleboot_1.1-3    boot_1.3-20         purrr_0.2.4        
 [7] stringr_1.2.0       ggplot2_2.2.1       dplyr_0.7.4        
[10] magrittr_1.5        epistats_0.1.0     

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.14     pillar_1.1.0     compiler_3.4.3   git2r_0.20.0    
 [5] plyr_1.8.4       bindr_0.1        tools_3.4.3      digest_0.6.13   
 [9] nlme_3.1-131     lattice_0.20-35  evaluate_0.10.1  tibble_1.4.1    
[13] gtable_0.2.0     pkgconfig_2.0.1  rlang_0.1.6      psych_1.7.8     
[17] Matrix_1.2-12    cli_1.0.0        parallel_3.4.3   yaml_2.1.16     
[21] perm_1.0-0.0     knitr_1.18       rprojroot_1.2    grid_3.4.3      
[25] glue_1.2.0       R6_2.2.2         foreign_0.8-69   rmarkdown_1.8   
[29] reshape2_1.4.3   tidyr_0.7.2      backports_1.1.2  scales_0.5.0    
[33] htmltools_0.3.6  splines_3.4.3    mnormt_1.5-5     assertthat_0.2.0
[37] colorspace_1.3-2 utf8_1.1.3       stringi_1.1.6    lazyeval_0.2.1  
[41] munsell_0.4.3    broom_0.4.3      crayon_1.3.4    </code></pre>
</div>

<hr>
<p>
    This <a href="http://rmarkdown.rstudio.com">R Markdown</a> site was created with <a href="https://github.com/jdblischak/workflowr">workflowr</a>
</p>
<hr>

<!-- To enable disqus, uncomment the section below and provide your disqus_shortname -->

<!-- disqus
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
