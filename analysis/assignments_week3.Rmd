---
title: "Assignments week 3"
author: "Wouter van Amsterdam"
date: 2017-11-06
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Day 9 Multiple regression
### Exercises with R
> *Introduction*
> Below you will find a worked example that helps you understand how to perform model building in multiple regression and analysis of covariance in R.

> We are interested in the diastolic blood pressure (y) for people on two different treatments (group). Within the two treatments different dosages were given. First type in the data (or Copy-Paste):

```{r}
y <- c(87,86.5,89,88.5,87.5,88,86.5,87,85,86,85,83)
dose <- c(5,6,7,8,9,10,5,6,7,8,9,10)
group <- c(0,0,0,0,0,0,1,1,1,1,1,1)
```

Let???s take a look at a plot of the data: 

```{r}
interaction.plot(dose, group, y , mean, ylab = "Bloodpressure")
```


> Use `help(interaction.plot)` to see how it works. R uses ":" to denote an interaction so group:dose is the R interaction term in a model. 

> It looks like there may be an interaction in the data: group 0 has higher blood pressure levels than group 1 in the higher dosages. If there is an explanatory variable in the data file that is categorical (other than 0-1), then you should tell R this by using the function factor(). So factor(group) tells R that group is not a numeric variable but that its numbers should be used as group labels. To fit an ANOVA model you can use either of the following:

```{r}
model.an <- lm(y~group) 
model.an <- glm(y~group, family = gaussian)
```

> In the second statement, the `family=gaussian` part may be left out since the gaussian (normal) is the default (???GLM??? stands for generalized linear model, of which ANOVA, linear regression, and logistic regression models are special cases). The result of the ANOVA model will be stored in model.an. In the second case, this will be an object of glm-type because you used glm to create it. To see what is in it use `names(model.an)` and if you want to see something specific use for instance `model.an$coefficients`. To get the table with the estimates: 

```{r}
summary(model.an)
```

> The function `drop1(fit, test = "F")` looks at the variables in the model ???fit???, then leaves out the terms one by one and calculates the F-test for every term if it were to be left out. Of course, in `model.an` there is only one variable so you just get one test. Note that the p-value of the F-test is exactly the same as the p-value of the t-test in the model summary.
To fit the model without the interaction: 

```{r}
model.anc <- glm(y~group + dose, family = gaussian)
summary(model.anc) 
drop1(model.anc, test = "F") 
```

> The column "Deviance" contains the residual sums of squares for different models. The first line gives the residual sums of squares if none of the terms is dropped so for the model with both group and dose in it. The second line gives the residual sum of squares for the model without group so for the model with only dose in it. The difference in these residual sums of squares gives the sum of squares for the group: 29.310-12.976 = 16.334. In the same way the sum of squares for dose can be obtained (14.583-12.976 = 1.607). For dose and group the F-values and the p-values are shown. With this information an ANOVA table could be constructed.

This somewhat elaborate method is simplified by using the Anova function in the library car:

```{r}
library(car) #Note: you might have to install this library first, using Packages, Install packages 
Anova(model.anc, test.statistic = "F")
```


> The model with an interaction can be fitted as: (or the exact same model can be given by:)

```{r}
model.int <- glm(y~group + dose + group:dose, family=gaussian)
model.int <- glm(y~group*dose, family = gaussian)
```

> Note that if you now use the drop1 function, only the interaction will be evaluated for possible dropping:

```{r}
drop1(model.int, test = "F")
Anova(model.int, test.statistic = "F")
```

> R does this because it makes little sense to drop a main effect while the interaction is still in the model; generally, one first checks whether the interaction can be removed.
The interaction term is statistically significant, so the trend in blood pressure over the dosages is different for the two treatment groups.

**this should be somewhere else, as `full` does not exist yet
> Checking Multicollinearity (used on day 10)
The variance inflation factor can be obtained using the vif() function in the car package. The argument to the vif() function is a model you have already fit.

> vif(full)
      temp  factories population       wind   rainfall   daysrain 
  3.763553  14.703175  14.340318   1.255460   3.404904   3.443932

> To get the tolerance instead, you can invert the VIF:
> 1/vif(full)

#### A note on automatic variable selection in R
> R does not have the same type of forward, backward and stepwise selection procedures as SPSS. The add1 and drop1 can be used to examine variables and decide which variable should be added/dropped next. The actual adding and dropping is not done automatically and needs to be done by the analyst, so a new model is fitted and again checked for variables that can be added/dropped. Note that the add1 and drop1 functions both give Akaike???s Information Criterion (AIC, to be treated during Modern Methods in Data Analysis) by default; an F-test can be obtained by using the option test="F" in the command. 

#### 5.
> This is a repeat of exercise #1, but now in R. Compare the results with those obtained in SPSS. 
The dataset with SO2 data from the lecture is available in the dataset so2.RData. Try to repeat the findings from the lecture notes using R. How will you do the variable selection in R?

```{r}
load(epistats::fromParentDir("data/so2.RData"))
str(so2)
```

First some histograms of the numeric variables

```{r}
# find out which variables in so2 are numeric
num_vars <- colnames(so2)[sapply(so2, is.numeric)]

# how many are there
length(num_vars)

par(mfrow = c(2, 4))
for (variable in num_vars) {
  hist(so2[[variable]], main = variable, xlab = variable)
}
  
par(mfrow = c(1, 1))
```

Then the pairwise scatterplots

```{r}
pairs(so2)
```

Correlation plot of all numeric variables (this can be done nicely with the package `corrplot`)
```{r}
cor_matrix <- cor(so2[, num_vars])
corrplot::corrplot(cor_matrix, method = "number")
```

Now for the model building. We can use the `step` function for variable selection. 
Note that it uses the Akaike Information criterion for variable selection.
NB we must remove the `city` variable, as this is merely a label of the observations, and is unique for each row.
```{r}
model_vars <- setdiff(colnames(so2), "city") # take all colnames of so2, remove 'city' from these names
fit0 <- lm(SO2~1, data = so2[, model_vars])
fit_full <- lm(SO2~., data = so2[, model_vars]) # use ~. to include all variables except for SO2

steps_forward  <- step(fit0, scope = list(lower = fit0, upper = fit_full), direction = "forward")
steps_backward <- step(fit_full, data = so2[, model_vars], direction = "backward")
steps_stepwise <- step(fit0, scope = list(upper = fit_full), data = so2[, model_vars], direction = "both")
```

The function by default prints all the steps. I do not know how to stop this behaviour

Look at the final models:

```{r}
summary(steps_forward)
summary(steps_backward)
summary(steps_stepwise)
```

#### 6. Cigarettes
> This is a repeat of exercise #2, but now in R. Compare the results with those obtained in SPSS. 
The workspace cigarette.RData contains a dataset cigarettte with data on carbon monoxide, tar and nicotine contents and weight of 25 brands of cigarettes.
We want to predict the carbon monoxide contents using the other 3 variables.
a.	Make a scatter plot matrix of the 4 variables, and formulate which variables you expect to predict (part of) carbon monoxide content.

NB the `RData` file did not seem to contain any data, so we imported the SPSS file with package `foreign`
```{r}
epistats::fromParentDir("data/cigarette.RData")
cigarette <- foreign::read.spss(epistats::fromParentDir("data/cigarette.sav"))
cigarette <- as.data.frame(cigarette)
# save file:
# save(cigarette, file = epistats::fromParentDir("data/cigarette2.RData"))
pairs(cigarette)
```

Nicotine looks highly correlated with tar and carbonmonoxide. Carbonmonoxide 
looks highly correlated with tar too.

> b.	Now make a correlation matrix of the 4 variables, and check your expectations from a.

```{r}
corrplot::corrplot(cor(cigarette), method = "number")
```

> c.	Build a regression model with all 3 predictor variables. Are all variables significant? Are regression coefficients what you would expect? Can you think of an explanation?

```{r}
fit_full <- lm(carbonmonoxide~., data = cigarette)
summary(fit_full)
```

Only tar is significant. If the explanatory variables were independent, we would 
expect that nicotine was alsa correlated with carbonmonoxide. However, due 
to coliniearity, the effect of nicotine vanishes when tar is included in the model.

> d.	Using backward selection reduce the model from c until it contains only significant variables. Which variable(s) are in the final model? Which proportion of the variation in carbon monoxide content is explained by this model?

Let's do manual backward selection

```{r}
fit_full <- lm(carbonmonoxide~., data = cigarette)
summary(fit_full)
```

Use `drop1` to determine which variable to drop first. 
Remove the coefficient with the highest p-value

```{r}
drop1(fit_full, test = "F")
fit_1 <- lm(carbonmonoxide~tar+nicotine, data = cigarette)
drop1(fit_1, test = "F")
```

Drop the next variable

```{r}
fit_2 <- lm(carbonmonoxide ~ tar, data = cigarette)
drop1(fit_2, test = "F")
```

Now we cant remove anymore predictors, because 'all' are significant.
Check assumptions of homoscedasticity and normal distribution of residuals:

```{r}
plot(fit_2, which = c(1,2))
```

Residuals look pretty OK. Homoscedasticity is a little hard to judge, but at least there is no clear funnel shape.

> e.	Based on the backward selection model, what is the predicted carbon monoxide content of a cigarette with tar = 13.0, nicotine = 1.0 and weight = 1.0?  What is its 95% prediction interval, and how do you interpret this?
	(Use the R function predict to obtain predictions for new data based on the model:
	
```{r}

new <- data.frame(tar=13.0, nicotine=1.0, weight=1.0)
predict(fit_2, newdata=new, interval="prediction", level=0.95)

```

#### 7.
> This is a repeat of exercise #4, but now in R. Compare the results with those obtained in SPSS. 
The variables in the study of 38 stream sites in New York state by Lovett et al. (2000) fell into two groups measured at different spatial sites ??? watershed variables (elevation, stream length and area) and chemical variables for a site averaged across sampling dates (averaged over 3 years). We use only the chemical variables. The data are given in the data file stream.RData

> STREAM 	name of the stream (site) from which observations were collected
MAXELEV 	maximum elevation of stream (m above sea level)
SAMPELEV 	site elevation (m above sea level)
LENGTH 	length of stream
AREA 	area of watershed
NO3 	concentration (mmol/L) of nitrogen oxide ions 
TON 	concentration (mmol/L) of total organic nitrogen 
TN 	concentration (mmol/L) of total nitrogen 
NH4 	concentration (mmol/L) of ammonia ions 
DOC 	concentration (mmol/L) of dissolved oxygen 
SO4 	concentration (mmol/L) of sulphur dioxide ions 
CL 	concentration (mmol/L) of chloride ions 
CA 	concentration (mmol/L) of calcium ions 
MG 	concentration (mmol/L) of magnesium ions 
H 	concentration (mmol/L) of hydrogen ions 

> Which of the chemical variables can predict  the maximum elevation of the stream?
Lovett et al. have used the log of the variables DOC, CL and H in their analyses. Can you imagine why they did it and is it necessary? 

```{r}
load(epistats::fromParentDir("data/stream.RData"))
str(stream)
```

All variables are numeric, except for `STREAM`, which is the name of the site.
Lets remove this variable to make our lives easier

```{r}
streams <- stream$STREAM
stream$STREAM <- NULL

car::scatterplotMatrix(stream[, c(1:7)], diagonal = "histogram")
car::scatterplotMatrix(stream[, c(8:14)], diagonal = "histogram")

```

It looks like `LENGTH` and `AREA` are tightly correlated, like `NO3` and `TN`,
 also `SO4` and `MG`. Note that many scatterplots were not included due to the limited 
 plot area.
 
We can formalize this by sorting the correlations

```{r}
# create correlation matrix
cor_matrix <- cor(stream)

# to remove the uninformative diagonal, and duplicity, retain only upper triangle
cor_matrix[lower.tri(cor_matrix, diag = T)] <- NA

# to analyze this, 'melt' the data to a conveniant format
cor_melted <- data.table::melt(cor_matrix, value.name = "correlation")

# remove the NA values
cor_melted <- cor_melted[!is.na(cor_melted$correlation),]
head(cor_melted)

# add a column with absolute correlation
cor_melted$abs_cor <- abs(cor_melted$correlation)

# sort by that column
cor_melted[order(cor_melted$abs_cor, decreasing = T), ][1:10,]
```


Zoom in on only `DOC`, `CL` and `H`
```{r}
car::scatterplotMatrix(stream[, c("MAXELEV", "DOC", "CL", "H")], diagonal = "histogram")
```

These 3 variables are right skewed, which is probably why they were log-transformed

Look at the transformed variables

```{r}
require(dplyr)
stream %>%
  mutate(DOC = log(DOC),
         CL = log(CL),
         H = log(H)) %>%
  select(c(MAXELEV, DOC, CL, H)) %>%
  car::scatterplotMatrix(diagonal = "histogram")
```

The distributions look a little nicer now. However, for linear regression,
normality of the independent variables is not assumed, only a linear relation 
between the independent variables and the dependent variable.

To answer the question which variables predict the elevation, lets use the 
`step` function with stepwise selection. And lets keep the transformed variables

```{r}
stream$DOC = log(stream$DOC)
stream$CL  = log(stream$CL)
stream$H   = log(stream$H)

fit0 = lm(MAXELEV ~ 1, data = stream)
fit_all = lm(MAXELEV ~., data = stream)

fit_step <- step(fit0, scope = list(upper = fit_all), data = stream, direction = "both")
```

Look at the final model:

```{r}
summary(fit_step)
```

Note that the `CL` variable remains included, even though we would drop this 
variable when using the p-values based on the F-statistic.

Of the variable pairs that were tightly correlated, in none of the cases 
both variables were included.

Check assumptions of the model

```{r}
plot(fit_step, which = c(1,2))
```


Residuals are a little skewed, not too much heteroscedasticity.


## Day 10 model building issues
### Exercises with R
> See the notes at the beginning of Exercises with R on day 9, especially how to check for multicollinearity.

#### 7.	
> This is a repeat of exercise #1, but now in R.
(Note that where SPSS calculates the “tolerance” for the excluded variables, R calculates it for the variables in the model.)
a.	Build a regression model with all 3 predictors. What is the tolerance for weight in this model? (Use the command 1/vif(), see previous day.)

```{r}
fit <- lm(carbonmonoxide~., data = cigarette)
1/vif(fit)
```

> b.	Now do a multiple regression with weight as dependent variable and tar and nicotine as predictors. What is the R2 of this model? How does it relate to the Tolerance?

```{r}
fit_weight <- lm(weight~tar+nicotine, data = cigarette)
summary(fit_weight)
```

The $R^2$ from this fit is the same as the tolerance from `tar` from above.

> c. 	Now what is the tolerance of nicotine in the model with only tar and nicotine? Check this, analogously to what you did in b. 

```{r}
fit2 <- lm(carbonmonoxide~tar+nicotine, data = cigarette)
1/vif(fit2)
```

The tolerance is hardly changed, so it looks like `nicotine` was pretty 
independent of `weight` in our sample.

#### 8.	
> This is a repeat of exercise #2, but now in R.
An indicator of a tree’s production capacity is the canopy. We want to examine whether there is a relation between the canopy and the production for fruit trees, and whether fertilizer has an influence on the relation. To answer this question, data on the canopy and production were collected from 14 available trees (6 fertilized and 8 unfertilized). The data are stored in crownarea.RData .
a.	Test whether the coefficient for the slope in the two groups (fertilized & unfertilized) is the same; i.e. are the lines for the two groups parallel or not?

```{r}
load(epistats::fromParentDir("data/crownarea.RData"))
str(crownarea)
```

The variable classes are pretty uncommon for R. Probably due to transferring 
data from SPSS to R. Let's polish them up.

```{r}
crownarea$FERTILIZ <- factor(crownarea$FERTILIZ)
crownarea$PRODUCTI <- as.numeric(crownarea$PRODUCTI)
crownarea$CROWNARE <- as.numeric(crownarea$CROWNARE)
```



```{r}
fit <- lm(PRODUCTI~CROWNARE*FERTILIZ, data = crownarea)
summary(fit)
```

There does not seem to be a statistically significant interaction between 
`FERTILIZ` and `CROWNARE`. Let's check in a plot:

```{r}
require(ggplot2)
ggplot(crownarea, aes(x = CROWNARE, y = PRODUCTI, col = FERTILIZ)) + 
  geom_point() + 
  geom_smooth(method = "lm")

```

The intercepts look different, but the slopes look the same (so no interaction)

> b.	Can the relation between canopy and production be described by one and the same line for both groups (fertilized & unfertilized)?

```{r}
fit2 <- lm(PRODUCTI~CROWNARE + FERTILIZ, data = crownarea)
summary(fit2)
```

Although there seems to be no significant interaction, the intercept for both 
groups is pretty different, and significant in the model fit.

> c.	Interpret the results.

Fertilization type seems to have an additive effect to canopy. For every level 
of canopy, fertilization status adds (or substracts, based on the reference category)
 the same amount of production.

> 9.	This is a repeat of exercise #3, but now in R. 
Read in the data file lowbirth.dat using the command

```{r}
lb <- read.table(epistats::fromParentDir("data/lowbirth.dat"), header = T)
str(lb)
```

> We are only going to use the following variables:
age 		age of the mother in years,
ht		history of hypertension (1 = yes, 0 = no) and 
bwt 		birth weight in grams

> a.	Fit a model with bwt as dependent and with ht and age as independent variables. Also include the interaction between age and ht in the model.

First off, lets recode `ht` as a factor variable.

```{r}
lb$ht <- factor(lb$ht)
fit <- lm(bwt ~ age*ht, data = lb)
summary(fit)
plot(fit, which = c(1,2))
```

Assumptions of homoscedasticity and normality of residuals seem ok.

Plot the data:
```{r}
require(ggplot2)
ggplot(lb, aes(x = age, y = bwt, col = ht)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```



> b.	Give an interpretation of the interaction term.

It looks like age itself is not a significant predictor, while hypertension is.
However, in the group with hypertension, age seems to be an important factor. 
For participants with hypertension, increasing age is associated with a lower 
birthweight.

> c.	Check whether you need the interaction term in the model.

```{r}
summary(fit)
```

From the summary we see that the interaction term is significant. 
Let's also compare $R_adj^2$ between this model and a model without the interaction.

```{r}
summary(lm(bwt~age+ht, data = lb))
```

The model with interaction term looks better

> d.	Can the model be reduced further? Give the final model, the one that cannot be reduced further, and interpret this model.

We cannot reduce further. The interaction needs to be included, so also the 
intercepts for hypertension groups and the $\beta$ for age.

#### 10.
> FEV (forced expiratory volume) is an index of pulmonary function that measures the volume of air expelled after one second of constant effort. The dataset fev.txt contains FEV measurements on 654 children, aged 6-22, who were seen in the Childhood Respiratory Disease Study in 1980 in East Boston, Massachusetts. The question of interest is whether a child’s smoking status has an effect on his FEV.

> a.	Read the text file into R, using fev <- read.table(choose.files()). Results of earlier analyses indicate that logfev is a better measure to use in linear regression. Make a new variable fev$logFEV <- log(fev$FEV).

```{r}
fev <- read.table(epistats::fromParentDir("data/fev.txt"), header = T)
fev$logFEV <- log(fev$FEV)
str(fev)
```


> b.	Make a box plot of logfev by smoking status. What do you notice? What might be an explanation for this unexpected result? Test the difference in mean logfev between smokers and non-smokers using a t-test.

```{r}
boxplot(logFEV~Smoker, data = fev)
```

FEV looks higher in current smokers. Maybe this is mediated by age? 
Older children tend to smoke more often.

```{r}
t.test(logFEV~Smoker, data = fev)
```

> c.	Use linear regression to model logfev as a function of smoking status (note that the variable smoker cannot be used in regression; make a new variable smoke that takes the value 1 if the child is a current smoker and 0 if not); compare the result with part b).

```{r}
fit <- lm(logFEV~Smoker, data = fev)
summary(fit)
```

> d.	We suspect that age and height are distorting the relation between smoking and FEV. Look at some graphs of logfev, age, height and smoking. Are age and height related to FEV? To smoking?

```{r}
suppressWarnings(
  car::scatterplotMatrix(fev[, c("logFEV", "Age", "Height")], diagonal = "histogram")
)

par(mfrow = c(1,3))
boxplot(logFEV~Smoker, data = fev)
boxplot(Age~Smoker, data = fev)
boxplot(Height~Smoker, data = fev)
par(mfrow = c(1,1))
```

Or in a single call to `ggpairs` from the `GGally` package:

```{r, warning = F}
GGally::ggpairs(fev[, c("logFEV", "Age", "Height", "Smoker")])
```

> e.	Fit the following models:
i	logfev  ~ smoke
ii	logfev ~ smoke + age
iii	logfev  smoke + age + height
What is the interpretation of the coefficients in the third model? What happens to the coefficient for smoke and its standard error when age and height are added to the model?

```{r}
fit0 <- lm(logFEV~Smoker, data = fev)
fit1 <- lm(logFEV~Smoker + Age, data = fev)
fit2 <- lm(logFEV~Smoker + Age + Height, data = fev)
summary(fit0)
summary(fit1)
summary(fit2)
```

The interpretation of the coefficients:

* For equal Age and Height, non-smokers have 0.05 more ml FEV
* For equal smoking status and height, FEV increases with 0.02 ml for 
every year in age
* For equal smoking status and age, FEV increase with 0.04ml for each unit 
increase in height (probably inches, since it's the USA)

> f.	Since we saw an indication of non-parallel lines for smokers and non-smokers in the relation between logfev and age, fit a fourth model, including the interaction of age and smoking:
iv	logfev = smoke, age, height, agesmoke
What happens now to the coefficient for smoke and its standard error?

```{r}
fit3 <- lm(logFEV~Smoker + Age + Height + Age*Smoker, data = fev)
summary(fit3)
```

The coefficient for smoke goes from positive to negative, but the standard error 
increases and it is no longer significant.

> g.	Now let’s try a backward regression. Make one block with all the variables in model iv, plus sex. Choose Method=Backward to allow SPSS to drop non-significant variables. What do you get? Is this a logical model?

```{r}
fit_full <- lm(logFEV~Age*Smoker + Height + Sex, data = fev)
drop1(fit_full, test = "F")
```

It looks like the interaction term can be disregarded

```{r}
fit_a <- lm(logFEV~Age + Smoker + Height + Sex, data = fev)
drop1(fit_a, test = "F")
```

Now we would keep all variables. So FEV increases with age, 'not smoking', 
height and male sex. Seems like a reasonable model. Let's check assumptions:

```{r}
plot(fit_a, which = c(1,2))
```

Nice homoscedasticity, residuals maybe a little skewed

```{r}
hist(fit_a$residuals)
```

#### 11.
> Data from a study of the effect of three different methods of instruction on reading  comprehension in children are stored in the file readingtestscores.dat. 66 participants (22 in each group) were given a reading comprehension test before and after receiving the instruction. The following variables were collected:

> Subject: Subject number 
Group: Type of instruction that student received (Basal, DRTA, or Strat) 
PRE1: Pretest score on first reading comprehension measure 
PRE2: Pretest score on second reading comprehension measure 
POST1: Posttest score on first reading comprehension measure 
POST2: Posttest score on second reading comprehension measure 
POST3: Posttest score on third reading comprehension measure

> a.	The dataset is a flat text file with fixed widths. You can read it into R by using the read.table() function. 

It looks like the values are actually separated by a tab. Which can be read by 
using `sep = "\t"`.

```{r}
comp <- read.table(epistats::fromParentDir("data/readingtestscores.dat"), header = T, sep = "\t")

str(comp)
```

> b.	Make a boxplot of the posttest score on the second reading comprehension measure for the three types of instruction. Do you think there are statistically significant differences between the groups? 

```{r}
boxplot(POST2~Group, data = comp)
```

It looks like a pretty big difference, I would expect statistical significance.

> c.	Fit a model to examine whether there are differences in the mean posttest score on the second reading comprehension measure for the three types of instruction.

```{r}
fit <- lm(POST2~Group, data = comp)
summary(fit)
```

This summary containts the F-test, equivalent to performing an ANOVA. 
It also comes up with a post-hoc test for testing difference between a baseline 
group and the other groups.

> d.	Test whether or not there are differences between the groups. Which hypothesis are you testing?

Since we have no prior hypothesis on which group should score better or worse, 
it is best to do an ANOVA with proper post-hoc tests that incorporate adjustment 
for multiple comparisons among all groups. 
$H_0$: all groups equal, $H_1$: not all groups are equal.


```{r}
TukeyHSD(aov(POST2~Group, data = comp))

```


So DRTA and Basal are equavalent. Strat is better than both of these

> e.	This study is an example of a randomized, pre-post design. There are several ways to analyze such a design (analyzing only the post measurement, as we did above; analyzing the difference between pre and post; or using analysis of covariance), but “ANCOVA analysis” (a linear regression model with post scores at the outcome, and group and pre-intervention score as explanatory variables) has a number of advantages* over other analyses. Fit a model to examine whether there are differences in the mean posttest score on the second reading comprehension measure for the three types of instruction, controlling for the posttest score on the same test. Use the drop1() function to get p-values for the two variables in the model.

Probably what is meant in the question is 'controlling for the pre-test score'

```{r}
fit2 <- lm(POST2 ~ Group + PRE2, data = comp)
summary(fit2)
```


> f.	Get the model coefficients from the model in (e) and interpret them.

```{r}
coef(fit)
```

For equal `PRE2` score, the group with Strat instruction had 2.90 higher score 
on the second post-test evaluation. 

> g.	Get a summary of the model frm (c), and compare the mean squared error (called the “Residual standard error” in the R output) to that of the model in (e). What do you notice?

```{r}
summary(fit)
summary(fit2)
```

It looks like the the second model, including pre-test scores, fits the data 
better (since the residual standard error is lower).

> *The interested reader can take a look at Vickers AJ and Altman DG. Analysing controlled trials with baseline and follow up measurements. British Medical Journal 2001;323:1123 http://www.bmj.com/content/323/7321/1123.full.pdf+html 

















## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
