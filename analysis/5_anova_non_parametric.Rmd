---
title: "ANOVA and non parametric tests"
author: "Wouter van Amsterdam"
date: 2017-10-30
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->
Teacher: Maria Schipper

* one-way ANOVA
* F-distribution (Fischer-distribution)
* Post-hoc tests
* Assumptions of ANOVA
* non-parametric statistics
* flow-chart of the statistical Tests

## ANOVA
Example data: eggs.

Set density as factor variable
```{r}
require(amstR)
load(fromParentDir("data/eggs.RData"))
summary(limpet.eggs)
limpet.eggs$density <- as.factor(limpet.eggs$density)
xtabs((eggs)~as.factor(density)+season, data = limpet.eggs)
boxplot(eggs~density, data = limpet.eggs)
```

Observe: less spread in group with lower mean. Usually: spread scales with mean.

### Assumptions
* Homoscedasticity: equal variances among different groups
* Normal distribution of residuals
* Independence of the measurements (this you cannot check in data; requires a good design)


You can do many tests
```{r}
level_combinations <- combn(1:4, m = 2)
for (i in 1:ncol(level_combinations)) {
  cat_levels = levels(limpet.eggs$density)[level_combinations[, i]]
  cat("\n", cat_levels)
  res <- t.test(eggs~density, 
         data = limpet.eggs[limpet.eggs$density %in% 
                              cat_levels,])
  print(res$p.value)
}
```


Pooled variance between group 8 and 45 is lower than in group 8 and 15. So by this, the p-value will decrease.

Take total variance, split it up into componants: variance explained by groups + extra variance.

### Types
* one-way: test for difference among three or more independent groups
* factorial ANOVA is used when the experimenter wants to study the effects of two or more treatment variables. includes interaction effects; e.g. two-way, three-way
* repeated measurement ANOVA is used when the subjects are subjected to repeated measures or matched observations; e.g. 4 different treatments, 4 time observations per subject; like paired T-test; can also be used for clustering / stratification.
* multivariate analysis of variance (MANOVA) is used when there is more than one dependent variable.

### Model

$$y_{ij} = \hat{\mu} + \hat{\tau_j} + \hat{\epsilon_{ij}}$$

$i = 1, ..., n_j$; $j = 1, ..., J$; $\sum_{j = 1}^{J}{\tau_j=0}$; $\epsilon \sim N(0, \sigma_{\epsilon}^2)$

Note: variance of residuals is not dependent on group

$$H_0 = \tau_1 = \tau_2 = ... = \tau_j = 0$$

$H_1$ is that 2 or more $\tau_j \neq 0$

The do post-hoc test (with correction of significance).
Than possibilities: test all groups against total mean.
You can also do contrasts (2 groups against each other).


Optimize
$$y_{ij} - \bar{y}$$ residuals by overall mean; variance under $H_0$.
$$y_{ij} - \bar{y}_j$$ residuals by group

$$y_{ij} - \bar{y} = \bar{y}_j - \bar{y} + y_{ij} - \bar{y}_j$$; variance under $H_1$.
$$\bar{y}_j - \bar{y}$$ is variance explained by model (group effects)

test | table
--- | ---
n1 | n2

Source | SS | df | MS | F
--- | --- | --- | --- | ---
Between | $SS_{between}$
Within | $SS_{within}$
Total | $SS_{total}$


$SS$ = Sum of squares

$SS_{between} = \sum_{j}{n_j(\bar{y}_j-\bar{y})^2}$;
$SS_{within} = \sum_i\sum_j{(y_{ij} - \bar{y}_j)^2}$;
$SS_{total} = \sum_i\sum_j{(y_{ij} - \bar{y})^2} = SS_{between} + SS_{within}$;

$SS_between$


Total: $df_{total} = \sum{(n_j)}-1$

Within groups: $df_{within} = \sum({n_j-1})$

Between groups $df_{between} = J-1$

Mean squares = sum of squares / degrees of freedom

$$F = MS_{between} / MS_{within}$$

F is F-distributed with $a = J-1$ and $b = \sum({n_j-1})$ degrees of freedom

Numerator degrees of freedom: a
Denomerator degrees of freedom: b

Null hypothesis: $MS_{between} = MS_{within}$

Always single side test: is my F statistic greater than 1?

Anova with 2 groups = T.Test with a = 1; 
$$T^2 = F$$

* Look up if T is example of ANOVA *

```{r}
summary(aov(eggs ~ density, data = limpet.eggs))
```

### Post-hoc
Do only when an overall effect is found.

choice for test is determined by:

* number of comparisons
* whether or not the multiple comparisons were planned
* other criteria (sample sizes are equal, variances are equal, multiple comparisons are orthogonal, ...)

No correction with <=3 comparisons, and each correction holds unique information (= orthogonal)

### Fisher's least significant difference (LSD) test
* regular unpaired t-test, but with $MS_{residual}$ instead of $\sigma_{p}$, $n-J$ degrees of freedom (=df associated within-groups SS); so for each comparison, all observations are used for estimating sigma (thus the high degrees of freedom, more power)
* P-value is *not* corrected for multiple comparisons
* sometimes used for a small number of planned (*a priori*) comparisons

### Fisher's LSD with Bonferroni correction
* Take p-value from Fisher's LSD test multiplied by the number of tests
* Better for a small number of *a priori* defined comparison than LSD
* Too conservative when testing *all possible* pairwise comparisons

### Tukey's HSD test (or Turkey-Kramer test)
* Like T-test, but with Studentized range distribution; larger critical value
* Used for testing all possible pairwise comparisons (preferred)

### Dunnett's test
* Compare experimental groups to a control group (so less pair-wise comparisons)

### Contrasts (linear combinations of means) in general
* t-tests based on weighted sum of the group means

### Scheffe test
* used for controlling type I error when testing all possible linear combinations of means


```{r}
TukeyHSD(aov(eggs ~ density, data = limpet.eggs))
```

With T-tests:
$$ T = \frac{\bar{X_1}-\bar{X_2}}{s_{p}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$$

Now use:
$$s_{p} = \sqrt(MS_{residual})$$

### Evaluating assumptions
#### Normality of residuals
Check normal distribution of residuals. This is easiest by using a linear model.
```{r}
fit <- lm(eggs~density, data = limpet.eggs)
summary(fit)
plot(fit, which = c(1,2))
```

Formal tests: 
* Kolmogorov-Smirnov test
* Shapiro-Wilk test

```{r}
ks.test(resid(fit), y = "pnorm")
shapiro.test(resid(fit))
```

Caveats for formal testing for normality:
* many data points means high power for testing, also for testing for non-normalitys


NB: central limit theorem weakens the reliance of ANOVA on this assumption, given 
homoscedascicity

NB: ANOVA robus against to non-normality when sample sizes are equal

#### Check homogeneity of variance
* plot residuals vs fitted (which is residuals per gropu)

##### Formal testing
use Levene's test or Brown-Forsyth test
* Tests the null hypothesis that the population variances are equal
* In practice the test is equal to a one way ANOVA analysis on a transformation (|y_ijâˆ’y_j |) of the original data
* One advantage of Levene's test is that it does not require normality of the underlying data. 
* Another good test is the Brown-Forsyth test in which the mean is replaced by the median in the transformation. 


NB ANOVA not robust to clear differences between variances, especially when the sample sizes differ considerably.

NB Transformations can make distributions more normal and variances more homogeneous.

#### Check independence of observations
Very imporant, but it can't be checked.

### When assumptions are not met
* Weighted ANOVA or weighted regression -> groups with lower variance are weighted more
* non-arametric statistics; generally non-normality is OK, but homoscedasticity is usually needed
* transform data: when non-normal distribution, and when there is also no homscedasticity
* bootstrapping can always be used

## Non-parametric tests
In general: replace values with ranks, do a test on the ranks
No need for assumption on distribution, but the distribution should be the same for each group. 
Only location can be different, spread should be comparable.

parametric | non-parametric
--- | ---
1-sample t-test or paired sample t-test | Wilcoxon signed-rank test or Sign Test (difference pos vs negative, and use binomial distribution)
2-sample t-test | Wilcoxon rank sum test or Mann-Whitney test
one-way ANOVA | Kruskal-Wallice test
two-way ANOVA | **no non-parametric variant**
randomized block design | Friedman test
Pearson correlation | Spearman rank correlation

#### Wilcoxon rank-sum and Kruskal-Wallis test assumptions
* iid-random samples
* measurements scale is at least ordinal
* population distribution functions are identical for groups except for a possible difference in location (not spread)


### Kruskal-Wallis
Let's compare Kruskal-Wallice and ANOVA

```{r, message=F, warning = F}
require(dplyr)
set.seed(2)
ngroups <- 4
ntotal  <- 80
group_size <- ntotal/ngroups
groups  <- rep(letters[1:ngroups], each = group_size)
means   <- runif(min = 0, max = 2, n = ngroups)
group_means <- rep(means, each = group_size)
y <- as.vector(sapply(means, function(x) rnorm(mean = x, sd = 1, n = group_size)))
```

View data, perform ANOVA and Kruskal-Wallice
```{r}
boxplot(y~groups)
summary(aov(y~groups))
kruskal.test(y~factor(groups))
```

Do manual Kruskal-Wallice (ANOVA on ranks)
```{r}
ranks <- rank(y)
summary(aov(ranks~factor(groups)))
```

Not exactly the same

#### Wilcoxon signed rank test
* Differences are mutually independent (random sample)
* measurement scale of the differences is at least interval
* the distribution of the differences is symmetric





## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
