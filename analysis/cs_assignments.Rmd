---
title: "Assignments Computational Statistics"
author: "Wouter van Amsterdam"
date: 2018-02-12
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

# Setup

```{r}
library(dplyr)
library(data.table)
library(magrittr)
library(purrr)
library(here) # for tracking working directory
```


# Day 1 advanced-R

## Excercise 1, A Coinflip function

### A custom dice roll function

```{r}
coinflip <- function(num){
  sample(x = c(1,2), size = num, replace = TRUE)
  }
```

Make dice roll function

```{r}
diceroll <- function(num) {
  sample(x = 1:6, size = num, replace = TRUE)
}

diceroll(4)
```


### B Dice roll with arbitrary number of sides

```{r}
diceroll2 <- function(num, sides) {
  sample(x = 1:sides, size = num, replace = TRUE)
}

diceroll2(num = 4, sides = 4)
```


### C Averages of dice roll

Take dice with 6 sides, which was `diceroll`

Let's roll the dice 1, 10, 100, 1000 and 10000 times, calculate the mean,
and do this 1000 times

We will use the package `purrr` to map functions to the list of number of 
throws

```{r}
require(magrittr)
require(purrr)
nsim = 1000

nthrow = 10^(0:4)

nthrows <- rep(nthrow, times = nsim)

nthrow %>%
  map(diceroll) %>%
  map_dbl(mean)

means <- list(nthrows) %>%
  pmap(diceroll) %>% # apply diceroll to each row of nthrows, returns a list
  map_dbl(mean)      # apply mean to each element of the list, return a double vector

df <- data.frame(nthrows, means)

```

Visualize results

```{r}
require(ggplot2)

df %>%
  ggplot(aes(x = nthrows, y = means)) +
  geom_point(alpha = .1) + 
  scale_x_log10() + theme_minimal() + 
  ggtitle("Means of 1000 simulations of n-dice throws") + 
  labs(x = "Number of throws")
```


Now with uniform sampling on logarithmic scale


```{r}
nthrows <- round(10^runif(min = 0, max = 4, n = nsim))
means <- list(nthrows) %>%
  pmap(diceroll) %>% # apply diceroll to each row of nthrows, returns a list
  map_dbl(mean)      # apply mean to each element of the list, return a double vector

df <- data.frame(nthrows, means)

df %>%
  ggplot(aes(x = nthrows, y = means)) +
  geom_point(alpha = .1) + 
  scale_x_log10() + theme_minimal() + 
  ggtitle("Means of 1000 simulations of n-dice throws") + 
  labs(x = "Number of throws")
```

Or with base-R plot

```{r}
plot(means~nthrows, data = df, log = "x")
```


## 2 tables

Get data

```{r}
smokedat <- data.frame(
  id = 1:20,
  smoking.status = c(NA, "never smoked", "never smoked", "never smoked", "never smoked", "never smoked", "never smoked", "has smoked", "has smoked", "has smoked", "has smoked", "has smoked", "has smoked", "currently smoking", "currently smoking", "currently smoking", "currently smoking", "currently smoking", "currently smoking", "currently smoking"),
  outcome = c("no hart infarction", NA, "no hart infarction", "no hart infarction", "no hart infarction", "no hart infarction", "hart infarction", "no hart infarction", "no hart infarction", "no hart infarction", "no hart infarction", "hart infarction", "hart infarction", "hart infarction", "no hart infarction", "no hart infarction", "no hart infarction", "hart infarction", "hart infarction", "hart infarction")
    )
```

View

```{r}
smokedat
```


### A Get help

```{r}
?table
```

### B cross table

```{r}
table(smokedat$smoking.status, smokedat$outcome)
```

### C with missings

> Modify the previous command, to also display the numbers of missing.

```{r}
table(smokedat$smoking.status, smokedat$outcome, useNA = "always")
```

> These tables are rather simple, we would like to add row and column totals as well as proportions.

### D Add margins

> Assign the table created in C) to an object called smoketab. Use the book, R forum, Google, etc. to find the commands to calculate row and column sums. In the same way, find the commands to bind rows and columns. Apply the above commands to create smoketab2, with the row totals added to smoketab as a new column. In addition, create smoketab3 with the column totals added to smoketab2 as a new row.

```{r}
smoketab <- table(smokedat$smoking.status, smokedat$outcome, useNA = "always")

smoketab2 <- cbind(smoketab, total = rowSums(smoketab))
smoketab3 <- rbind(smoketab2, total = colSums(smoketab2))
smoketab3
```

### E: Proportions

> Calculate row proportions for smoketab3 (hint: You can divide smoketab3 by the row totals). Store the new table in an object called  smoketab3.RP.


Each row get's multiplied with 1 divided by the sum of the columns

We can create a matrix with 1 over the column totals, and do element-wise 
product

```{r}
totals <- tail(smoketab3, 1)
total_matrix <- matrix(rep(1/totals, nrow(smoketab3)), 
                       nrow = nrow(smoketab3), byrow = T)
total_matrix

smoketab3.RP <- smoketab3 * total_matrix
smoketab3.RP
```

### F column proportions

> Calculate columns proportions as well, in the same manner (note: you will need to transpose the table, and then transpose back afterwards). Store the new table in an object called smoketab3.CP.

```{r}
col_totals <- smoketab3[, "total"]
col_total_matrix <- matrix(rep(1/col_totals, each = ncol(smoketab3)),
                           nrow = nrow(smoketab3), byrow = T)
col_total_matrix

smoketab3.CP <- smoketab3 * col_total_matrix
smoketab3.CP

```


### G list of matrices

>using the list() command, produce a list containing smoketab3, smoketab3.RP and smoketab3.CP. Name these three components “tab”, “rowprop” and “colprop”, respectively.

```{r}
tab_list <- list(tab = smoketab3, rowprop = smoketab3.RP, colprop = smoketab3.CP)
```


### H improved tablefunction

> Now make an improved table function yourself. Note: build a function that uses the arguments variable1, variable2 and dat, to produce a list such as you made in A) - G). variable1 and variable2 should be character values (containing text between quotes). Note that selection of columns or rows can also be done e.g. using smokedat[,"smoking.status"] (try it).

```{r}

table2 <- function(data, variable1, variable2) {
  x <- data[[variable1]]
  y <- data[[variable2]]
  
  # use dnn to preserve variable names
  tab <- table(x, y, dnn = c(variable1, variable2), useNA = "always")
  
  row_totals <- rowSums(tab)
  tab2 <- cbind(tab, total = row_totals)
  
  col_totals <- colSums(tab2)
  tab3 <- rbind(tab2, total = col_totals)
  
  # update col and row totals
  row_totals <- rowSums(tab3) / 2
  col_totals <- colSums(tab3) / 2

  row_total_matrix <- matrix(rep(row_totals, each = ncol(tab3)), 
                       nrow = nrow(tab3), byrow = T)
  col_total_matrix <- matrix(rep(col_totals, each = nrow(tab3)), 
                       nrow = nrow(tab3), byrow = F)
  
  tab3.RP <- tab3 / row_total_matrix
  tab3.CP <- tab3 / col_total_matrix
  
  tabs <- list(tab = tab3, rowprop = tab3.RP, colprop = tab3.CP)

  return(tabs)
  
}

table2(smokedat, "smoking.status", "outcome")

```

### I find updated table function

> In the book, R forum, internet or elsewhere, can you find a crosstable function that provides similar functionality (row and column totals and proportions)?

I googled for 'r package table with proportions and margins'

Found: gmodels on the website Quick-R

Works like:

```{r}
gmodels::CrossTable(smokedat$smoking.status, smokedat$outcome)
```


### J add rounding

> If you still have time, make an improved version of the function that you made in H) that includes rounding of the decimals in rowprop and colprop.

```{r}


table3 <- function(data, variable1, variable2, ndigits = NULL) {
  x <- data[[variable1]]
  y <- data[[variable2]]
  
  # use dnn to preserve variable names
  tab <- table(x, y, dnn = c(variable1, variable2), useNA = "always")
  
  row_totals <- rowSums(tab)
  tab2 <- cbind(tab, total = row_totals)
  
  col_totals <- colSums(tab2)
  tab3 <- rbind(tab2, total = col_totals)
  
  # update col and row totals
  row_totals <- rowSums(tab3) / 2
  col_totals <- colSums(tab3) / 2

  row_total_matrix <- matrix(rep(row_totals, each = ncol(tab3)), 
                       nrow = nrow(tab3), byrow = T)
  col_total_matrix <- matrix(rep(col_totals, each = nrow(tab3)), 
                       nrow = nrow(tab3), byrow = F)
  
  tab3.RP <- tab3 / row_total_matrix
  tab3.CP <- tab3 / col_total_matrix
  
  tabs <- list(tab = tab3, rowprop = tab3.RP, colprop = tab3.CP)
  
  # no rounding
  if (is.null(ndigits)) return(tabs)

  # rounding
  tabs_rounded <- tabs %>%
    purrr::map(round, ndigits)
  return(tabs_rounded)
  
  
}

table3(smokedat, "smoking.status", "outcome")
table3(smokedat, "smoking.status", "outcome", ndigits = 2)


```

## 3 split and unsplit

### A load data

> Load and examine the haartdat dataset. Note: This can be done using the function load().

```{r}
load(here("data", "haartdat.rda"))
str(haartdat)
```

### B Split

>Try out the split() command in the following manner: * Split haartdat into a list of individual datasets, based on the patient ID * Name this list datlist * Look at some components in this list, e.g. the first, second or 20th component

```{r}
uniqueN(haartdat$patient)
datlist <- base::split(haartdat, haartdat$patient)
datlist[[1]]
datlist[[10]]
length(datlist)
```

### C unsplit

> Reverse the splitting operation by unsplit(), name the resulting object haartdat2. Are haartdat and haartdat2 identical? (Hint: take a summary of haartdat - haartdat2).

```{r}
haartdat2 <- unsplit(datlist, haartdat$patient)
summary(haartdat2 - haartdat)
```

All zeros, so equal, check dimensions

```{r}
dim(haartdat)
dim(haartdat2)
```

### D Minimum by patient

> Suppose we want to determine the minimum of the CD4 counts for each patient. Make an individual dataset inddat, containing one row for each patient, with only the patient ID (HINT: use unique()). Split the CD4 measurements in haartdat per individual, name this object  cd4.split. Using sapply(), take the minimum of CD4 count for each individual, and add the resulting vector to inddat.

```{r}
inddat <- unique(haartdat$patient)

cd4.split <- split(haartdat$cd4.sqrt, haartdat$patient)

cd4s <- sapply(cd4.split, min)

inddat <- data.frame(patient = inddat, min_cd4 = cd4s)
head(inddat)
```

We can do this we fewer lines of code using `dplyr`

```{r}
haartdat %>%
  group_by(patient) %>%
  summarize(min_cd4 = min(cd4.sqrt))
```

Or using `data.table`

```{r}
setDT(haartdat) # make data.table (only need to do this once)
haartdat[, list(min_cd4 = min(cd4.sqrt)), by = "patient"]
```


### E max per patient

> Similarly to D), calculate for each patient the maximum of haartind, indicating if somewhere during the follow-up HAART was given. Add the resulting vector to inddat.

Let's use `data.table`

```{r}
inddat <- haartdat[, list(
  min_cd4 = min(cd4.sqrt), 
  haart = max(haartind)
), by = "patient"]

head(inddat)

table(inddat$haart)
```

### F cumulative HAART

> We would also like to compute for each patient the cumulative amount of HAART treatment at each time point. Hint: to transform a list object (e.g. listname) to a single numeric vector, you can use the command do.call(c, listname) (of course, this only works when the list only contains numeric values).

We can get this by sorting on timepoint, and then calculating a cumulative sum 
of haart for each patient

```{r}
haartdat[order(tstart), cum_haart:=cumsum(haartind), by = "patient"]
haartdat[order(patient, tstart)][patient %in% unique(patient)[1:2]]
```

## 4 Simulating longitudinal patient data

> The goal of this exercise is to simulate longitudinal patient data (n = 100). As an example, we use the development of CD4 count in HIV positive individuals. We consider development since the moment of HIV seroconversion (= t0). Typically, CD4 count decreases over time after seroconversion. For now, we assume that the square root of CD4 count decrease linearly over time. We further assume each patient has a different intercept and slope for sqrt(CD4).

### A

> Build a data frame called basdat, containing patient IDs 1, 2, …, 100.

```{r}
basdat <- data.frame(id = 1:100)
```

### B

> From a bivariate normal distribution with means (26, -2), variances (5, 0.25) and covariance -0.5, draw 100 samples. These represent the patient-level intercepts and slopes. Paste the values to basdat. Make sure the column names in basdat are informative. Also make sure that the class of basdat remains “data.frame”.

The covariance of $<0$ means that we model that patients with higher intercepts 
('baseline CD4 count'), the slope is more negative, so the CD4 count goes 
down faster.

```{r}
require(mvtnorm)
means <- c(26, -2)
variances <- c(5, 0.25)
covariance <- -0.5

cov_mat <- matrix(c(variances[1], covariance, covariance, variances[2]), 
                  nrow = 2, byrow = T)

set.seed(2)
samp1 <-rmvnorm(n = nrow(basdat), mean = means, sigma = cov_mat)

str(samp1)

```

Check means, variances and covariance

```{r}
mean(samp1[, 1]); var(samp1[, 1])
mean(samp1[, 2]); var(samp1[, 2])
cov(samp1[, 1], samp1[, 2])
```

Plot

```{r}
plot(samp1[,1], samp1[, 2])
```


Add to data.frame

```{r}
basdat %<>% 
  mutate(intercept = samp1[, 1], slope = samp1[, 2])
head(basdat)
```


### C add follow-up

> We (simply) assume these patients are all followed for 10 years. However, the number of CD4 measurements differs between patients. From a Poisson distribution, draw a vector of numbers of measurements, with a mean of 5 (= lambda). Make sure these draws will be a new column of basdat. Call the column nmeas.

```{r}
basdat %<>% 
  mutate(nmeas = rpois(nrow(basdat), lambda = 5))
```

### D Validity check

> Check for zeros in nmeas, convert these all to 1 for simplicity. Note that in reality, allowing for the zeros will be more realistic.

Use data.table

```{r}
setDT(basdat)
basdat[nmeas == 0, .N]
```

Change to 1

```{r}
basdat[nmeas == 0, nmeas:=1]
```


### E simulate single patient

> Now we will simulate and plot longitudinal data for the first patient (we will repeat the process for the patients 2-100 later). Make a dataframe “longdat”, which will contain the simulated CD4 measurements. At first put in this dataframe the patient ID, repeated according to the value of  nmeas for patient 1. Hint: you can use rep(). Also put in the individual slope and intercept for patient 1.

```{r}
longdat <- basdat[id == 1, {
  .SD %>%             # take the columns as specified by .SDcols
    map(rep, nmeas)   # repeat each one nmeas times
}, .SDcols = c("id", "intercept", "slope")]
```

### F simulate measure times

> In longdat, simulate the measurement times, drawn from the uniform distribution on (0, 10).

For readibility, sort on time

```{r}
longdat[, times:=runif(n = .N, min = 0, max = 10)]
setorder(longdat, times)
print(longdat)
```

### G Simulate measurements

>In longdat, compute the “true” square root of CD4, based on the intercept, slope and time. To this “true” value, add normally distributed noise with mean = 0 and SD = 1. This will give the simulated CD4 measurements (on the square root scale). Now compute measured CD4 count itself by squaring.

With dplyr, we can nicely mutate variables after each other

```{r}
longdat %<>%
  mutate(true_CD4 = intercept + slope * times,
         CD4_sqrt = true_CD4 + rnorm(nrow(longdat), mean = 0, sd = 1),
         CD4      = CD4_sqrt^2) %>% data.table()
head(longdat)
```

### H Plot

> Try to make a pretty plot of measured CD4 count against follow-up time for patient 1. Suggestion: use xlim and ylim, to at least display 0 for both axes.

```{r}
longdat %>%
  ggplot(aes(x = times, y = CD4)) + 
  geom_line() + 
  lims(x = c(0, max(longdat$times)), y = c(0, max(longdat$CD4))) + 
  theme_minimal()
```


### I for all patients

> Some more fun: put together all the code used to simulate and plot the data for patient 1. Now replace the number 1, indicating patient 1, by the letter i. Try: with first defining i <- 1, the same plot as before will be produced. In the main title of the plot, make sure that the patient ID (based on i) is plotted. (HINT: find out how to use paste()). Try it also with i <- 2. Now put your syntax within a “loop” with i ranging from 1 to 100. (Hint: place for (i in 1:100){ before and } after your code). Final step, make sure the output of the loop is send to a PDF plot. Look at the plot, scroll through. Be proud.

By writing the code with data.table, it is now easy to do this for each patient

```{r}
longdat2 <- basdat[, {
  .SD %>%             # take the columns as specified by .SDcols
    map(rep, nmeas)   # repeat each one nmeas times
}, .SDcols = c("intercept", "slope"), by = "id"]
print(longdat2)
```

Simulate times. Note that it does not matter whether we do this for each 
patient separately or all at the same time.

```{r}
longdat2[, times:=runif(n = .N, min = 0, max = 10)]
setorder(longdat2, times)
```

Add simulated CD4 counts, also not needed to do this by patient,
since for each line we have a time, intercept and slope

```{r}
longdat2 %<>%
  mutate(true_CD4 = intercept + slope * times,
         CD4_sqrt = true_CD4 + rnorm(n(), mean = 0, sd = 1), # n() counts the number of rows
         CD4      = CD4_sqrt^2) %>% 
  data.table()
```

Create a plot of all patients at the same time

```{r, cache = T}

longdat2 %>%
  mutate(id = factor(id)) %>%
  ggplot(aes(x = times, y = CD4, col = id)) + 
  geom_line() + 
  lims(x = c(0, max(longdat2$times)), y = c(0, max(longdat2$CD4))) + 
  theme_minimal()

```

Create a plot for each patient separately (only 4 are shown here)

```{r}

longdat2[id %in% unique(id)[1:4]] %>%
  mutate(id = factor(id)) %>%
  ggplot(aes(x = times, y = CD4)) + 
  geom_line() + 
  lims(x = c(0, max(longdat2$times)), y = c(0, max(longdat2$CD4))) + 
  theme_minimal() + 
  facet_wrap(~id, scales = "fixed")

```

To create pdfs separately for each patient, we will use looping.

Let's only use 3 patients here

```{r, eval = F}
for (i in 1:3) {
  longdat2[id == i] %>%
  ggplot(aes(x = times, y = CD4)) + 
    geom_line() + 
    lims(x = c(0, max(longdat2$times)), y = c(0, max(longdat2$CD4))) + 
    theme_minimal() + 
    ggtitle("CD4 number over time", paste0("patient ", i))
  ggsave(filename = here("figs", paste0("patient_", i, ".pdf")), device = "pdf")
}
```

Or to put them all in one file

```{r, eval = F}
plots <- vector("list", nrow(basdat))
for (i in 1:nrow(basdat)) {
  plots[[i]] <- 
    longdat2[id == i] %>%
    ggplot(aes(x = times, y = CD4)) + 
      geom_line() + 
      lims(x = c(0, max(longdat2$times)), y = c(0, max(longdat2$CD4))) + 
      theme_minimal() + 
      ggtitle("CD4 number over time", paste0("patient ", i))
}
```

To save them in a single pdf with one page per patient (this takes some time)

```{r, cache = T, eval = F}
require(gridExtra)

pdf(here("figs", "cd4_vs_time.pdf"), onefile = T)
for (i in seq(length(plots))) {
  grid.arrange(plots[[i]])  
}
dev.off()


```


Now be proud ;)

# Day 2 simulations

## Excercise 1 measures of spread

Setup as asked

```{r}
## Install and load robustbase
install.packages('robustbase')
library('robustbase')

## Simulation of 25 samples from normal population
simdat <- rnorm(n = 25, mean = 0, sd = 1)   

## Estimate the population sd by the sample sd, MAD and Qn
est1 <- sd(simdat)
est2 <- mad(simdat)
est3 <- Qn(simdat)
```

Do this many times

```{r}
## Specify the number of simulations
numsim <- 10000

## Create empty lists of size numsim
simdat <- vector(mode = "list", length = numsim)
est1 <- vector(mode = "list", length = numsim)
est2 <- vector(mode = "list", length = numsim)
est3 <- vector(mode = "list", length = numsim)

## Start for() loop
for(i in 1:numsim){

  ## Simulation of 25 samples from normal population
  simdat[[i]] <- rnorm(n = 25, mean = 0, sd = 1)    

  ## Estimate the population sd by the sample sd, MAD and Qn
  est1[[i]] <- sd(simdat[[i]])
  est2[[i]] <- mad(simdat[[i]])
  est3[[i]] <- Qn(simdat[[i]])

  ## End for() loop
  }
```

Transform workflow in to a function

```{r}
## Start function
simfun1 <- function(

  ## Function parameters
  numsim,
  n = 25,
  pop.mean = 0,
  pop.sd = 1
  ){

    ## Create empty lists of size numsim
    simdat <- vector(mode = "list", length = numsim)
    est1 <- vector(mode = "list", length = numsim)
    est2 <- vector(mode = "list", length = numsim)
    est3 <- vector(mode = "list", length = numsim)

    ## Start for() loop
    for(i in 1:numsim){

      ## Simulation of 25 samples from normal population
      simdat[[i]] <- rnorm(n = n, mean = pop.mean, sd = pop.sd) 

      ## Estimate the population sd by the sample sd, MAD and Qn
      est1[[i]] <- sd(simdat[[i]])
      est2[[i]] <- mad(simdat[[i]])
      est3[[i]] <- Qn(simdat[[i]])

      ## End for() loop
      }

    ## Save parameter specifications
    pars.spec <- data.frame(numsim, n, pop.mean, pop.sd)

    ## Return the lists
    list(pars.spec = pars.spec, simdat = simdat, est1 = est1, est2 = est2, est3 = est3)

    ## End function
    }

```


To run

```{r}
## Set random seed and run the function
set.seed(234878)
res1 <- simfun1(numsim = 10000)
```

Transform output lists to vectors

```{r}
## Transform results from lists to vectors
est1.v <- unlist(res1$est1)
est2.v <- unlist(res1$est2)
est3.v <- unlist(res1$est3)
```


Visualize results 

```{r}
## Kernel-Density plots
plot(density(est1.v), xlim = c(0,2), ylim = c(0,3), main = 'Results simfun1')
lines(density(est2.v), col = 'blue')
lines(density(est3.v), col = 'green')

## Add means
abline(v = mean(est1.v))    
abline(v = mean(est2.v), col = 'blue')  
abline(v = mean(est3.v), col = 'green') 

## Add true value
abline(v = res1$pars.spec$pop.sd, col = 'red')  

## Add legend
legend('topright', c('Sample SD', 'MAD', 'Qn', 'True value'), 
  col = c('black', 'blue', 'green', 'red'), lty = 1)
```

### 1.1 Best estimator

Both SD and Qn seem to be centered at the true value, sample SD is more peaked
 so this is the most efficient of the three.
 
In numbers:

```{r}
## Bias (= mean(estimates) - the true population value)
mean(est1.v) - res1$pars.spec$pop.sd
mean(est2.v) - res1$pars.spec$pop.sd
mean(est3.v) - res1$pars.spec$pop.sd

## Standard error (= standard deviation of estimates)
sd(est1.v)  
sd(est2.v)      
sd(est3.v)  

## Mean squared error (= bias^2 + standard error^2)
(mean(est1.v) - res1$pars.spec$pop.sd)^2 + sd(est1.v)^2
(mean(est2.v) - res1$pars.spec$pop.sd)^2 + sd(est2.v)^2
(mean(est3.v) - res1$pars.spec$pop.sd)^2 + sd(est3.v)^2

```

Looks like eye-balling was not perfect. 
Qn is closest to the true value (lowest biast), SD is second.
SD has the lowest variance, as we saw. 
Mean squared error (including bias and variance) is best for SD

### 1.2 Best MSE

Mean squared error (including bias and variance) is best for SD

### 1.3 Number of simulations

No, they do not chance much.

### 1.4 Robustness

```{r}
## Start function
simfun1_outlier <- function(

  ## Function parameters
  numsim,
  n = 25,
  pop.mean = 0,
  pop.sd = 1
  ){

    ## Create empty lists of size numsim
    simdat <- vector(mode = "list", length = numsim)
    est1 <- vector(mode = "list", length = numsim)
    est2 <- vector(mode = "list", length = numsim)
    est3 <- vector(mode = "list", length = numsim)

    ## Start for() loop
    for(i in 1:numsim){

      ## Simulation of 25 samples from normal population
      simdat[[i]] <- rnorm(n = n, mean = pop.mean, sd = pop.sd) 
      
      ## generate an outlier that is 10 times as big as expected
      simdat[[i]][1] <- 10*simdat[[i]][1]

      ## Estimate the population sd by the sample sd, MAD and Qn
      est1[[i]] <- sd(simdat[[i]])
      est2[[i]] <- mad(simdat[[i]])
      est3[[i]] <- Qn(simdat[[i]])

      ## End for() loop
      }

    ## Save parameter specifications
    pars.spec <- data.frame(numsim, n, pop.mean, pop.sd)

    ## Return the lists
    list(pars.spec = pars.spec, simdat = simdat, est1 = est1, est2 = est2, est3 = est3)

    ## End function
    }
```


Run simulations

```{r}
## Set random seed and run the function
set.seed(23487)
res1 <- simfun1_outlier(numsim = 10000)

## Transform results from lists to vectors
est1.v <- unlist(res1$est1)
est2.v <- unlist(res1$est2)
est3.v <- unlist(res1$est3)
```

Evaluate estimators

```{r}
## Bias (= mean(estimates) - the true population value)
mean(est1.v) - res1$pars.spec$pop.sd
mean(est2.v) - res1$pars.spec$pop.sd
mean(est3.v) - res1$pars.spec$pop.sd

## Standard error (= standard deviation of estimates)
sd(est1.v)  
sd(est2.v)      
sd(est3.v)  

## Mean squared error (= bias^2 + standard error^2)
(mean(est1.v) - res1$pars.spec$pop.sd)^2 + sd(est1.v)^2
(mean(est2.v) - res1$pars.spec$pop.sd)^2 + sd(est2.v)^2
(mean(est3.v) - res1$pars.spec$pop.sd)^2 + sd(est3.v)^2

```

Now both Qn and MAD are clearly preferable to SD. 

Qn seems best in terms of bias and variance

## Excercise 2: T-test vs Wilcoxon-Mann-Whitney test

> The Student’s t-test is used to compare the locations of two samples. One of the assumptions of this test is that the samples come from normal distributions. If this assumption is thought to be violated, the Wilcoxon-Mann-Whitney (WMW) test is often used as an alternative, since this test does not assume a specific distribution. In this simulation exercise, we will assess the performance (in terms of the power) of both tests when used for normal and non-normal data.

### Question 2.1

> Start by writing a function that draws a sample of size n.s1 from a normal population distribution with mean equal to mean.s1 and standard deviation equal to sd.s1. Then, draw a second sample of size n.s2 from a normal population distribution with mean equal to mean.s2 and standard deviation equal to sd.s2. Compare the two samples using t.test(x = s1, y = s2, var.equal = TRUE). Specify that the function repeats these steps numsim times, each time storing the data and the t-test results in a list. Let the function return these lists. If you want, you can use the same general function structure as was used in simfun1().

```{r}
simfun_2.1 <- function(
  n.s1, n.s2, mean.s1, mean.s2, sd.s1, sd.s2,
  nsim = 10000
) {
  
  dat <- vector(mode = "list", length = nsim)
  t_results <- vector(mode = "list", length = nsim)
  
  for (i in seq(nsim)) {
    s1 <- rnorm(n = n.s1, mean = mean.s1, sd = sd.s1)
    s2 <- rnorm(n = n.s2, mean = mean.s2, sd = sd.s2)
    
    dat[[i]] <- data.frame(s1, s2)
    t_results[[i]] <- t.test(s1, s2, var.equal = T)

  }
  
  params <- data.frame(n.s1, n.s2, mean.s1, mean.s2, sd.s1, sd.s2, nsim)
  
  list(parameters = params, simdat = dat, t.test = t_results)
  
}
```


### Question 2.2

> Specify the function’s parameters as n.s1 = 10, n.s2 = 10, mean.s1 = 0, mean.s2 = 0.5, sd.s1 = 1, sd.s2 = 1 and  numsim = 10000. Run the function. From the results (i.e. the list of t-test objects), extract the p-values (see the hint below), and calculate the power of the test (using α=0.05). Note that the power of a test is the probability that the test will reject the null hypothesis when the null hypothesis is false. Here, the null hypothesis is false (since the population means of s1 and s2 differ). The power is then calculated as the proportion of results that were significant.

> Hint: One way to extract the p-values from the list of t-test objects is by using the sapply() function: for example, for a list named listname,  sapply(1:length(listname), FUN = function(i) listname[[i]]$p.value) will return a vector of p values.

Run simulation

```{r}
set.seed(12345)
simres <- simfun_2.1(n.s1 = 10, n.s2 = 10, mean.s1 = 0, mean.s2 = 0.5, 
                     sd.s1 = 1, sd.s2 = 1, nsim = 10000)
```

Grab p-values. Note that there is a handy package called `broom` that 
helps grabbing important coefficients from a model fit and puts them in a 
data.frame. Use the `map` function from `purrr` to apply `broom::tidy` to 
each element of a list. Use `map_df` to give back a data.frame

```{r}
require(broom)
require(purrr)

simres_df <- simres$t.test %>%
  map_df(tidy)
head(simres_df)
dim(simres_df)
```

Now see how many times the p-value is below 0.05

```{r}
table(simres_df$p.value < 0.05)
```

So the t-test found a significant group difference in 1850 out of 10000 simulations,
this means a power of 18.5%

### Question 2.3

> Include the WMW-test (see ?wilcox.test) in your simulation function. Would you perform the two tests on the same data in each run or would you draw new data before each test? Using the function, perform a simulation study investigating the power of both tests for n = 10, 20, 40 and 80 in each group. Use numsim = 10000. Do not adjust the other parameters, and make the simulation replicable. From the output, create a table like the one below. Furthermore, generate a plot of the results, with the sample size on the x-axis and the power on the y-axis. Is numsim sufficiently large?

Yes you would evaluate both tests on each simulated datasets, to reduce 
variance

Write function

```{r}
simfun_2.3 <- function(
  n.s1, n.s2, mean.s1, mean.s2, sd.s1, sd.s2,
  nsim = 10000
) {
  
  dat <- vector(mode = "list", length = nsim)
  t_results <- vector(mode = "list", length = nsim)
  w_results <- vector(mode = "list", length = nsim)
  
  for (i in seq(nsim)) {
    s1 <- rnorm(n = n.s1, mean = mean.s1, sd = sd.s1)
    s2 <- rnorm(n = n.s2, mean = mean.s2, sd = sd.s2)
    
    dat[[i]] <- data.frame(s1, s2)
    t_results[[i]] <- t.test(s1, s2, var.equal = T)
    w_results[[i]] <- wilcox.test(x = s1, y = s2)

  }
  
  params <- data.frame(n.s1, n.s2, mean.s1, mean.s2, sd.s1, sd.s2, nsim)
  
  list(parameters = params, simdat = dat, t.test = t_results, w.test = w_results)
  
}
```


Use function on a range of values

```{r, cache = T}
set.seed(123456)
sim_10 <- simfun_2.3(n.s1 = 10, n.s2 = 10, mean.s1 = 0, mean.s2 = 0.5, 
                     sd.s1 = 1, sd.s2 = 1, nsim = 10000)
sim_20 <- simfun_2.3(n.s1 = 20, n.s2 = 20, mean.s1 = 0, mean.s2 = 0.5, 
                     sd.s1 = 1, sd.s2 = 1, nsim = 10000)
sim_40 <- simfun_2.3(n.s1 = 40, n.s2 = 40, mean.s1 = 0, mean.s2 = 0.5, 
                     sd.s1 = 1, sd.s2 = 1, nsim = 10000)
sim_80 <- simfun_2.3(n.s1 = 80, n.s2 = 80, mean.s1 = 0, mean.s2 = 0.5, 
                     sd.s1 = 1, sd.s2 = 1, nsim = 10000)
```

Get p-values

Let's only grab the p-values now, we can also do this with map.

Use `map_dbl` to return a double vector 
(which is computer language for 'numeric with double precision', where 
double stands for the number of digits that are recorded)

```{r, cache = T}
df_10_t <- sim_10$t.test %>% map_dbl("p.value")
df_10_w <- sim_10$w.test %>% map_dbl("p.value")
df_20_t <- sim_20$t.test %>% map_dbl("p.value")
df_20_w <- sim_20$w.test %>% map_dbl("p.value")
df_40_t <- sim_40$t.test %>% map_dbl("p.value")
df_40_w <- sim_40$w.test %>% map_dbl("p.value")
df_80_t <- sim_80$t.test %>% map_dbl("p.value")
df_80_w <- sim_80$w.test %>% map_dbl("p.value")

```

Calculate power

```{r}
df <- data.frame(
  test = rep(c("t", "w"), 4),
  sample_size = rep(c(10, 20, 40, 80), each = 2),
  power = map_dbl(list(df_10_t, df_10_w, df_20_t, df_20_w, df_40_t, df_40_w, 
                   df_80_t, df_80_w), function(x) mean(x < 0.05))
)

df
```

Put in a table

```{r}
xtabs(power~sample_size+test, data = df)
```

The result for sample size 10 for the t-test is consistent with our previous
simulation, so it seems that nsim is large enough

Plot it

```{r}
require(ggplot2)
df %>%
  ggplot(aes(x = sample_size, y = power, col = test)) + 
  geom_line()
```

The t-test seems to have a consistently higher power for these normal distributed 
data.

### Question 2.4

> Perform the same simulations on non-normal data using rlnorm(). Use meanlog = 0 and sdlog = 1 for s1 and meanlog = 0.5 and  sdlog = 1 for s2.

This is a generic function for simulating data from any distribution built 
in to R that takes a location and spread parameter, 
and returning alongside the data, the results of a t-test and 
a wilcoxon-mann-whitney test.

This only works because many of the functions `r..` where `..` is the distribution work with the 
same argument order (n, mean, sd). (like `rnorm` and `rlnorm`)


```{r}
two_group_location_sim <- function(
  n.s1, n.s2, mean.s1, mean.s2, sd.s1, sd.s2,
  nsim = 10000,
  distribution_function = "rnorm"
) {
  
  dat <- vector(mode = "list", length = nsim)
  t_results <- vector(mode = "list", length = nsim)
  w_results <- vector(mode = "list", length = nsim)
  
  for (i in seq(nsim)) {
    s1 <- do.call(distribution_function, list(n.s1, mean.s1, sd.s1))
    s2 <- do.call(distribution_function, list(n.s2, mean.s2, sd.s2))

    dat[[i]] <- data.frame(s1, s2)
    t_results[[i]] <- t.test(s1, s2, var.equal = T)
    w_results[[i]] <- wilcox.test(x = s1, y = s2)

  }
  
  params <- data.frame(n.s1, n.s2, mean.s1, mean.s2, sd.s1, sd.s2, 
                       nsim, distribution_function)
  
  list(parameters = params, simdat = dat, t.test = t_results, w.test = w_results)
  
}

```


Now let's try to evaluate this function a little more systematically

```{r, cache = T}
sample_sizes <- list(10, 20, 40, 80)

set.seed(12345678)
sims <- map(sample_sizes, function(n) {
  two_group_location_sim(n.s1 = n, n.s2 = n, mean.s1 = 0, mean.s2 = 0.5, 
                     sd.s1 = 1, sd.s2 = 1, nsim = 10000,
                     distribution_function = "rlnorm")
})
```

Grab t-tests and w-tests for each sample size setting

This gets a little complicated since we're mapping on different levels of the list
(remember this is now a list of 4 sample sizes, each consisting of 4 lists 
("parameters", "simdat", "t.test", "w.test")), of which the last two are lists
of length 10000, containing the test results

```{r, cache = T}
pvals_t <- sims %>%
  map("t.test") %>% 
  map(~map_dbl(.x, "p.value"))
pvals_w <- sims %>%
  map("w.test") %>% 
  map(~map_dbl(.x, "p.value"))
```

Calculate powers

```{r}
df <- data.frame(
  test = rep(c("t.test", "wilcox.test"), each = 4),
  sample_size = rep(unlist(sample_sizes), 2),
  power = c(map_dbl(pvals_t, function(x) mean(x<0.05)),
            map_dbl(pvals_w, function(x) mean(x<0.05)))
)
```

Create table

```{r}
xtabs(power~sample_size+test, data = df)
```

Plot

```{r}
require(ggplot2)
df %>%
  ggplot(aes(x = sample_size, y = power, col = test)) + 
  geom_line()
```

Now it looks like the wilcox.test is the clear winner.

### Question 2.5

> Briefly discuss your findings.


So for the log-normal distribution, the wilcoxon-mann-whitney test 
seems to have better power than the t-test, whereas for normally distributed 
samples, the t-test has more power.

## Excercise 3: Handling missing data

This will be skipped, since it is the graded quiz question

## Excercise 4: Sample size and cluster size in clust-randomized trial

> In cluster randomized trials, randomization is performed on clusters of patients (e.g. hospitals or GP’s), instead of on individual patients. There are multiple possible reasons for choosing such a design, but important ones are (1) logistic efficiency and (2) avoiding treatment group contamination.

> Suppose that we aim to perform a randomized trial to study the effect of a certain (dichotomous) intervention X on a continuous outcome Y, and to avoid treatment group contamination we will randomize hospitals, not individual patients. Further suppose that two strategies are considered:

> including 10 hospitals, with 10 patients each
including 50 hospitals, with 2 patients each
Perform a simulation study in which you compare these strategies. More specifically, focus on the bias, the standard error, and the MSE of the estimate of the effect of X. In order to deal with the clustering in the data, fit a random intercept model using lmer() (from the lme4 package). Let the true model equal

$$E(Y_{ij})=2+η_i−3X_{ij}+ϵ_{ij}$$

> where $η_i∼N(mean=0,sd=0.5)$ and $ϵ_{ij}∼N(mean=0,sd=1)$ for patient $j$ in hospital $i$.

> Note that, due to the complexity of the model, convergence may not be reached in every simulation run. A convenient function to use in such cases is tryCatch.W.E() from the package simsalapar. This function, which can be ‘wrapped’ around a model specification (e.g.  fit1 <- tryCatch.W.E(lm(Y~X))), produces a list with objects value and warning. fit1$value contains the fitted model, if no error occurred. Warnings or errors, if they occurred, are stored in fit1$warning. This is convenient in a for loop, since it enables us to retrospectively see where exactly something went wrong (as opposed to only seeing warning messages after running the loop, or errors causing the loop to stop).

> Make sure that the data and fitted models are stored, and that the results are replicable. Use system.time() to estimate how many simulations can be performed given the time you have, but make sure you performed enough runs so that replicating the simulations does not affect your conclusions.

Create simulation function

```{r}
sim_clust_rand <- function(
  nhospital = 10,
  npatients = 100 / nhospital,
  nsim = 10000,
  true_intercept = 2,
  true_effect = -3,
  random_intercept_sd = 0.5,
  residual_sd = 1
) {
  # grab parameters
  params = c(as.list(environment())) # grabs all function parameters
  
  # check validity
  if (nhospital %% 2 > 0) stop("please provide an even number of participating hospitals")
  
  # initialize lists
  simdat = vector(mode = "list", length = nsim)
  fits   = vector(mode = "list", length = nsim)
  
  for (i in seq(nsim)) {
    hospital = rep(1:nhospital, each = npatients)
    x = rep(c(1,0), each = (nhospital / 2) * npatients)
    random_intercept = rep(rnorm(n = nhospital, 0, random_intercept_sd), 
                           each = npatients)
    y = true_intercept + random_intercept + true_effect * x + 
      rnorm(nhospital * npatients, 0, residual_sd)
    
    simdat[[i]] <- data.frame(hospital, x, random_intercept, y)
    
    fits[[i]] <- simsalapar::tryCatch.W.E(
      lme4::lmer(y~x + (1|hospital))
      )
  }
  
  list(parameters = as.data.frame(params), 
       simdat = simdat,
       fits = fits)
}
```

Generate 100 simulations to estimate time per simulation

```{r, cache = T}
system.time({
sims_1 <- sim_clust_rand(nhospital = 10, nsim = 100)
})
```

So about 1.8 second per 100 simulations. 10000 should take 3 around minutes.

```{r, cache = T}
nsim = 10000
nhospital = 10
npatients = 100 / nhospital
true_intercept = 2
true_effect = -3
random_intercept_sd = 0.5
residual_sd = 1


set.seed(345678)

system.time({
sims_1 <- sim_clust_rand(nhospital = 10, nsim = nsim)
sims_2 <- sim_clust_rand(nhospital = 50, nsim = nsim)
})
```

(actually it took 6-7 minutes for 10000)

We want to grab the estimate of the effect of $X$. 

Let's see what the result of a single fit looks like

```{r}
fit1 <- sims_1$fits[[1]]
fit1
```

Since we used the function `simsalapar::tryCatch.W.E()`,
the actual fit is put in an element called value

See if we can get effects easily

```{r}
coef(sims_1$fits[[1]]$value)
```

No, this gives us the random effects

What if we try `broom`

```{r}
broom::tidy(fit1$value)
```

Yes! Someone made sure there is a method for the function `lmer` for `broom::tidy`
Now all we have to do is create a vectorized way of grabbing the coefficients
Since we want to know the effect of x, we will focus on that.

```{r}
broom::tidy(fit1$value) %>% .[.$term == "x", "estimate"]
```

or

```{r, cache = T}
require(broom)
x_hats_1 <- sims_1$fits %>%
  map("value") %>%
  map(tidy) %>%
  map("estimate") %>%
  map_dbl(2)

x_hats_2 <- sims_2$fits %>%
  map("value") %>%
  map(tidy) %>%
  map("estimate") %>%
  map_dbl(2)
```


See if we can get confidence bounds too

```{r}
confint(fit1)
```

Awesome. Now to vectorize. First get the profile confidence bounds 
This takes some time to compute.

In fact, it takes so long (plus it throws warnings/errors) that we will skip it.


```{r, eval = F}
confints_1 <- sims_1$fits %>%
  map(confint)
confints_2 <- sims_2$fits %>%
  map(confint)
```

We can create a data.frame to store the estimated effects

```{r}
require(dplyr)

df <- data.frame(
  x_estimate = c(x_hats_1, x_hats_2),
  nhospitals = rep(c(10, 50), each = nsim)
)

df %>%
  group_by(nhospitals) %>%  
  # calculate bias, standard error and coverage for both situations
  summarize(
    bias = mean(x_estimate) - true_effect,
    se   = sd(x_estimate)
  ) %>%
  ungroup() %>%
  # from these, calculate z-score and MSE
  mutate(
    se_bias = se / sqrt(nsim),
    z_score_bias = bias / se_bias,
    mse = bias^2 + se^2
    )
```

We observe that both methods have low bias.
Bias is lowest for 50 hospitals, and the variance too
MSE is best for 50 hospitals.

50 hospitals seems preferable

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
