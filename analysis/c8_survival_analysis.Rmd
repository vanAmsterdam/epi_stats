---
title: "Survival analysis"
author: "Wouter van Amsterdam"
date: 2017-11-03
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->
## Intro
Tutor: Rebecca Stellato

## Approach to survival analysis
```{r}
library(survival)
data("veteran")
str(veteran)
```

ANOVA on survival time (ignores censoring).
```{r}
fit <- lm(time~celltype, data = veteran)
summary(fit)
anova(fit)
summary(aov(time~celltype, data = veteran))
```

Chi-squared for alive at a certain time point
```{r}
hist(veteran[veteran$status == "1",]$time)
plot(ecdf(veteran[veteran$status == "1",]$time))
plot(ecdf(veteran[veteran$status == "0",]$time))
```


Both methods will make you lose information.


Survival analysis: in general means time to event (death / disease / relapse / whatever).

General problems

* Survival time is generally not normally distributed (but right skewed)
* Survival times may often be censored

Calculate
* Cumulative incedence
* Incidence ratio (assuming the risk of having the event is independent of time,
all participants have the same survival distribution (ignoring groups))

Assumptions
All methods used here assume **uninformative censoring**
* censoring unrelated to probability of an event
* censoring unrelated to treatment


### Survival functions
Calculate cumulative distribution of survival times $F(t)$

Always plotted: survival time $S(t) = 1 - F(t)$.

```{r}
curve(1-ecdf(veteran$time)(x), from = min(veteran$time), to = max(veteran$time))
```

Hazard function:
$$p(Event | t = t)$$

Chance of getting an event at time t, given survival up to time t.
Usually, cumulative hazard is plotted.

Cumulative hazard is theoretically unbounded.

Hazard function:

$$h(t) = -S'(t) / S(t)$$

Parametric approach:
Distribution of survival function
* Exponential
* Weibull

Non-parametric: Kaplan-Meier

Cox-proportional hazards: semi-parametric

Estimate survival function

```{r}
veteran[order(veteran$time), c("time", "status")]
```


Divide survival time in intervals j

Number at risk at beginning of interval $n_j$
Number of deaths within the interval $d_j$
Proportion surviving interval j $p_j = \frac{n_j-d_j}{n_j}$

$$S(t_j) = \prod_j{p_j} = \prod_j{\frac{n_j-d_j}{n_j}}$$

$$S(t_j) = P(T>t_j) = P(Event | t = t_{1-j})*P(T = t_{1-j}) = (1-P(t_{j-1} < t < t_j | T > t_{j-1}))*P(T>t_{j-1})$$

A sensored participant is included up until and including the last preceding 
event.


With data
Status = 1 means an event has taken place (death), status = 0 means censoring.
```{r}
require(survival)
data("veteran")

times  <- veteran$time
status <- veteran$status
n_total<- length(times)

# order by survival time
surv_order <- order(times)
times  <- times[surv_order]
status <- status[surv_order]

intervals <- unique(c(0, times))
n_intervals <- length(intervals)


# vectors for interval survival probability, cumulative survival and hazard function
interval_survival_probability <- numeric(n_total)
cumulative_survival <- numeric(1+n_total)
cumulative_survival[1] <- 1
se_cumulative_survival <- numeric(1+n_total)
se_cumulative_survival[1] <- 0
se_factor <- numeric(1+n_intervals)
se_factor[1] <- 0

interval_hazard   <- numeric(n_intervals)
cumulative_hazard <- numeric(1+n_total)
cumulative_hazard[1] <- 0

# create a counter for the n-th observation
n_at_risk <- n_total

for (i in 1:(n_intervals-1)) {
# for (i in 1:5) {
  t_start  <- intervals[i]
  t_end    <- intervals[i+1]
  
  # select the observations that match the time interval
  k_obs   <- times > t_start & times <= t_end
  k_min   <- min(which(k_obs))
  k_max   <- max(which(k_obs))
  n_obs   <- sum(k_obs)
  
  n_events <- sum(status[k_obs])
  n_censor <- n_obs - n_events
  
  # proportion surviving this interval
  prop_interval_survival <- (n_at_risk - n_events)/n_at_risk
  interval_survival_probability[k_min:k_max] <- prop_interval_survival
  
  # update cumulative survival function
  previous_survival <- cumulative_survival[k_min]
  cumulative_survival[1+k_min:k_max] <- previous_survival*prop_interval_survival
  
  
  ## 
  ## 
  ## These calculations need updating
  ## 
  ## 
  se_interval <- n_events / (n_at_risk*(n_at_risk - n_events))
  se_factor[i+1] <- sum(se_interval)
  
  se_cum_surv <- previous_survival*prop_interval_survival*sqrt(se_factor[i+1])
  se_cumulative_survival[1+k_min:k_max]<-se_cum_surv
  
  # update hazard function
  d_cumulative_survival   <- previous_survival*(1-prop_interval_survival)
  ddt_cumulative_survival <- d_cumulative_survival / (t_end-t_start)
  interval_hazard[i] <- ddt_cumulative_survival / previous_survival
  
  n_at_risk <- n_at_risk - (n_events + n_censor)
}

surv_data <- data.frame(status, times, 
                        interval_survival_probability, 
                        survival_function = tail(cumulative_survival, -1))

# kaplan-meier based on own calculation
plot(c(0, times), cumulative_survival, type = "s")
points(x = times[status == 0], y = cumulative_survival[c(F,status == 0)], pch = 3)

# plot own survival function over kaplan-meier based on survival package
surv_fit <- survival::survfit(Surv(time = times, event = status)~1)
plot(surv_fit,conf.int = F)
lines(c(0, times), cumulative_survival, col = "red", type = "s", lty = 2)

# plot cumulative hazard function
plot(intervals, cumsum(interval_hazard), type = "s")
```



### Kaplan-Meier
Average survival time can be done in several way:
1. Take mean (or median) of the survival times of all subjects, disregarding censoring
2. Take the mean (or median) of those who died
3. Use the Kaplan-Meier curve to estimate the median survival time a which 
the curve drops below 0.5 
$$\min(t_j|S(t_j) \leq 0.5)$$

Since survival time is usually right-skewed, median is usually best.

NB it only works when the survival cure dips beplow 0.5

```{r}
mean(veteran$time)
median(veteran$time)

km_fit <- survfit(Surv(time, status) ~ 1, data=veteran)
summary(km_fit, times = c(1,30,60,90*(1:10)))
max(km_fit$time[km_fit$surv >= 0.5])
# ggplot2::autoplot(km_fit)
plot(km_fit)
```

Confidence interval:

$$SE[S(t)] = S(t)\sqrt{\sum_{j=1}^k{\frac{d_j}{n_j(n_j-d_j)}}}$$
$t_{k-1} < t < t_k$

confidence interval:

$S(t) \pm z_{\alpha/2}*SE(S(t))$

Best way: take log(S(t)) and calculate SE on log scale, then translate back.

$log(S(t)) \pm z_{\alpha/2}*log(SE[S(t)])$

Which is default way in R.

Kaplan Meier is non-parametric
Assumptions
* Prognosis of patient is independent of the moment of inclusion in the study in 
the timepath of their disease / treatment -> clear inclusion start point
* Censoring was independent of the prognosis of the patient
* The times at which events occur are known exactly (otherwise it gets tricky)
* Patients are monitored throughout the whole study period

### Comparison of survival curves: Log-Rank test

* Order al survival times, $t_1 \leq t_2 \leq ... \leq t_j \leq t_n$
* Let $n_{ij}$ be number of paitents in group i = 1, 2, just begore time $t_j$
* Let $d_{ij}$ be the deaths.

Null hypothesis: $e_{ij} = n_{ij}*(d_j/n_j)$. 
So the number of deaths are equally distributed among groups, in proportion 
to their number of patients at that time-point.

$$T = \sum_i\sum_j{\frac{(e_{ij}-d_{ij})^2}{n_{ij}}}$$

T is chi-squared distributed with i-1 degrees of freedom.

Test with chi-square

Options
* Log Rank (Mantel-Cox)
* Breslow test (Gehan-Wilcoxon) more weight to short survival times. Unreliable when pattern of censoring is different across groups
* Tarone-Ware is less sensitive to censoring patterns than Breslow test.

### Sample size for Log-Rank tests
#### Step 1: effect
Treatment effect:

$$\delta_0 = \frac{ln(p_1)}{ln(p_0)} = HR$$

With $p_1$ and $p_0$ the estimated survival probability after a certain follow-up time t.

So the power is dependent on the number of events.

Hazard Ratio is the instatanious risk of dying for group 1, compared to the 
risk of dying for group 2, at some time-point t.

#### Step 2: number of events
Estimate number of events
$$d = (\frac{1+\delta_0}{1-\delta_0})^2(Z_{\alpha} + Z_{\beta})^2$$

#### Step 3: estimate total number of patients
$$n \geq \frac{2d}{2-p_0-p_1}$$

Assuming
* constant hazard ratio
* inclusion time
* follow-up time

### Recurrent events
There are several ways, all in framework of Cox models with some specific assumptions.


## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
