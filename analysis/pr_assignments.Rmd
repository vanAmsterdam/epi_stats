---
title: "Assignments for prognostic research"
author: "Wouter van Amsterdam"
date: 2018-03-13
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

# Setup R


```{r, message = F}
library(dplyr)
library(data.table)
library(magrittr)
library(purrr)
library(here) # for tracking working directory
library(ggplot2)
library(epistats)
library(broom)
```

# Day 2. model development 1

This excercise was intented for SPSS

> In this practical exercise we will study the prognosis of patients with traumatic brain injury.
We will assess the individual prognostic strength in univariable and multivariable analyses.
The aim of the multivariable analysis is to adjust for correlation between prognostic factors,
either to adjust for confounding (1) or to predict the prognosis with multiple predictors (2).
The data are in SPSS format: "TBI.sav". See below for a description of the dataset.

> Data set traumatic brain injury (TBI.sav, n=2159)
Patients are from the International and US Tirilazad trials; distributed here for didactic
purposes only. The primary outcome was 6 months Glasgow Outcome Scale (range 1 to 5).

> Name	Description (coding: no/yes is coded as 0/1)	Development (n=2159)
Trial	Study identification:
74 = Tirilazad international (n=1118)
75 = US (n=1041)	
52%
48%
d.gos	Glasgow Outcome Scale at 6 months:
1 = dead
2 = vegetative
3 = severe disability
4 = moderate disability
5 = good recovery	
23%
4%
12%
16%
44%
d.mort	Mortality at 6 months (0/1)	23%
d.unfav	Unfavorable outcome at 6 months (0/1)	39%
cause	Cause of injury
3 = Road traffic accident
4 = Motorbike
5 = Assault
6 = Domestic/fall
9 = Other	
39%
20%
6%
17%
18%
age	Age, in years (median [interquartile range])	29 [21 - 42]
d.motor	Admission motor score, range: 1 - 6 (median)	4
d.pupil	Pupillary reactivity
1=both reactive
2=one reactive
3= no reactive pupils 	
70%
14%
16%
pupil.i	Single imputed pupillary reactivity, 1;2;3 	70%/14%/16%
hypoxia	Hypoxia before / at admission, 1=yes 	22%
hypotens	Hypotension before / at admission, 1=yes	19%
ctclass	Marshall CT classification, 1 - 6 (median)	2
tsah	tSAH at CT, 1=yes	46%
edh	EDH at CT, 1=yes 	13%
cisterns	Compressed cisterns at CT, 0=no;1=slightly;2=fully	57%/26%/10%
shift	Midline shift > 5 mm at CT, 1=yes	18%
glucose	Glucose at admission, mmol/l (median [interquartile range])	8.2 [6.7 - 10.4]
glucoset
ph	Truncated glucose values (median [interquartile range])
pH (median [interquartile range])	8.2 [6.7 - 10.4]
7.4 [7.3 - 7.5]
sodium	Sodium, mmol/l (median [interquartile range])	140 [137 - 142]
sodiumt	Truncated sodium (median [interquartile range])	140 [137 - 142]
hb	Hb, g/dl (median [interquartile range])	12.8 [10.9 - 14.3]
hbt	Truncated hb (median [interquartile range])	12.8 [10.9 - 14.3]
* d. variables denoted 'derived'.

> Exercises

Load in data

```{r}
require(haven)
tbi <- read_spss(here("data", "TBI.sav"))
str(tbi)
```

Coerce `labelled` variables into factors, as R works with factors and 
`labelled` variables are foreign to R.

Use the package `haven` with the function `as_factor` to get this done while 
preserving factor labels.

```{r}
tbi %<>%
  mutate_if(is.labelled, as_factor)
str(tbi)
```


## 1) Cause of injury

### a) 

> Give the frequencies of the outcome (d.gos).
What is the most commonly observed outcome?

```{r}
tabl(tbi$d.gos)
```

### b) 

> Check the categorization in favorable vs unfavorable outcome (d.unfav variable).
What is the overall risk of an unfavorable outcome? How was d.gos dichotomized?

```{r}
tabl(tbi$d.gos, tbi$d.unfav)
```

Overall risk of unfavorable outcome:

```{r}
mean(tbi$d.unfav)
```

### c) 

> Give the frequencies of cause of injury.
What is the most common cause of injury?

```{r}
tabl(tbi$cause)
```


### d) 

> Study the univariable effect of the prognostic factor 'cause of injury' on 'unfavorable outcome'
(with crosstabs). What is the risk of an unfavorable outcome for each cause of injury? (use option <cells>
<percentages>)

```{r}
gmodels::CrossTable(tbi$cause, tbi$d.unfav, prop.chisq = F, prop.t = F, prop.c = F)
```


### e) 

>Quantify the effect with a logistic regression model. <Analyze> <Regression> <Binary logistic>
Specify cause of injury as a categorical covariate.

Let's make sure that 'other' is the reference category

```{r}
tbi %<>% mutate(cause = relevel(cause, ref = "other"))
```


```{r}
fit <- glm(d.unfav ~ cause, data = tbi, family = binomial("logit"))
summary(fit)
```

> By default SPSS will use the last category ('Other') as the reference category.
Which causes give the highest risk of unfavorable outcome, and which the lowest risk,
according to the regression result?

Motorbike is lowest, domestic/fall is highest. 
These match with the cross-table. 
Let's look at the predicted probabilities for each category

```{r}
cbind(levels(tbi$cause), predict(fit, newdata = data.frame(cause = levels(tbi$cause)),
        type = "response"))
```

### f) 

> Verify that the intercept estimate in e) corresponds to the risk for the "Other" cause category
as noted in d). Is the exp(intercept) an "odds ratio" or simply an "odds"?

With our fit the reference category is other

```{r}
o_int = exp(coef(fit)[1])
o_int
```

To probability

```{r}
o_int / (1 + o_int)
```

This matches the observed probability for other.

This is an actual 'odds'

> Verify also that the risk for the category "Domestic/fall" is only slightly higher than that of the
"Other" cause category, both according to the crosstable (d)) and the regression result in e).

### g) 

> Is the pattern of risk in d) and e) what you would expect? Can you think of confounders?
Hint: What is the mean age for each cause of injury?

You would expect the risk for traffic accidents to be higher. Let's include
 age in the summary
 
```{r}
tbi %>%
  group_by(cause) %>%
  summarize(mean_age = mean(age), prop_unfavouroble = mean(d.unfav))
```

Motorbike and traffic have low age and low unfavourable outcomes

### h) 

> Now fit a multivariable model to adjust the effect of cause of injury for age.

```{r}
fit2 <- glm(d.unfav ~ cause + age, data = tbi, family = binomial("logit"))
summary(fit2)
```

This model did not fit, the residual deviance is higher than the degrees of 
freedom

#### 1. 

> Is the effect of cause still statistically significant? Hint: focus on the overall p-value, based
on a 4 df test.

We should take 6 degrees of freedom, as there are 6 parameters estimated

```{r}
anova(fit2, test = "Chisq")
```

Dropping cause will significantly decrease the goodness of fit, so yes.

#### 2.

> How do the effects of the different causes change?

```{r}
require(tidyr)
fits <- list(without_age = fit, with_age = fit2)
fits %>%
  map_df(tidy, .id = "model") %>%
  select(estimate, model, term) %>%
  spread(key = c("model"), value = "estimate")
```

Assault and domestic become lower risk, motorbike and road traffic higher risk

### i) 

> What is your conclusion on the effect of "cause of injury"? Do you think "cause of injury"
should be used for predictive purposes?

Based on these data, yes. However, when adding more covariates, this may change.

## 2) Prediction model: Risk of unfavourable outcome

> We will now develop a simple prediction model with three predictors: motor score, pupillary
reactivity and age.

### a) 

> Give some descriptive statistics of motor score, pupillary reactivity and age.

```{r}
tbi %>%
  select(d.motor, d.pupil, age) %>%
  summary()
```

> b) Assess the univariable effects of motor score, pupillary reactivity and age on the outcome
(d.unfav) with a logistic regression model.

```{r}
terms <- c("d.motor", "d.pupil", "age")

fits_uni <- terms %>%
  map(function(term) glm(reformulate(term, "d.unfav"), 
                         data = tbi, family = "binomial"))
names(fits_uni) <- terms

fits_uni %>%
  map_df(tidy)
```

> What is the univariable effect of age on the risk of unfavorable outcome?

```{r}
exp(coef(fits_uni[["age"]]))
```

> What would be a good way to express the effect, using a linear scale?
Hint: think of recoding age by decade.

```{r}
tbi %<>%
  mutate(age_cat = cut(age, breaks = seq(from = 10*floor(min(age / 10)),
                                         to = 10*ceiling(max(age / 10)), by = 10)))
tbi %>%
  glm(formula = d.unfav ~ age_cat, family = "binomial") %>%
  summary()
```

If the effect of age were linear (on the log-odds scale), there would be a constant difference between 
each consecutive category

Alternatively, we could plot the log-odds per age category

```{r}
logit <- function(x) log(x / (1-x))
tbi %>%
  group_by(age_cat) %>%
  mutate(p_unfav = mean(d.unfav),
         p_unfav_lo = binom.confint_logical(d.unfav)$lower,
         p_unfav_hi = binom.confint_logical(d.unfav)$upper,
         logit_unfav = logit(p_unfav),
         logit_unfav_lo = logit(p_unfav_lo),
         logit_unfav_hi = logit(p_unfav_hi)
         ) %>%
  ggplot(aes(x = age_cat, ymin = logit_unfav_lo, y = logit_unfav, ymax = logit_unfav_hi)) + 
  geom_errorbar()
```

Linearity does not look too bad on logit scale.

I'm not sure whether calculating a confidence interval for the proportion and 
then transforming with logit is the best way to go.

### c) 

> Now fit a multivariable model. Include in your model: motor score, pupillary reactivity and age (continuous).
Note that there are missing values in the variable 'd.pupil', which have been filled in with a
statistical imputation procedure in 'pupil.i'. Perform the analyses twice: once with 'pupillary reactivity' including missing values (d.pupil) and once with missing values imputed (pupil.i).
What are the numbers of patients in each analysis? Are there differences in prognostic
effects? 

```{r}
fit_mis <- glm(d.unfav ~ d.motor + d.pupil + age, data = tbi, family = "binomial")
fit_imp <- glm(d.unfav ~ d.motor + pupil.i + age, data = tbi, family = "binomial")

fits <- list(with_missings = fit_mis, imputed = fit_imp)
fits %>%
  map_df(tidy, .id = "model") %>%
  select(model, term, estimate) %>%
  spread(model, estimate)
```

The estimate for age stays the same, for motor is a little different.

Overall the estimates are pretty much the same

```{r}
fits %>% map_df("df.null") + 1
```

### d) 

> Can we interpret the change in age coefficient from univariable analysis to multivariable
analysis if the number of subjects between the two analyses differ? Therefore: which variable for 'pupillary reactivity' do you prefer for modeling? Use this as the final multivariable model.

The number of missings is relatively low compare to the total number of observations 
(around 5%). If the missings are random and / or not associated with age or the 
outcome, the coefficients would not change. Precision decreases a little bit.
Best would be to use the imputed variable, but difference will be small.

```{r}
fits <- list(univariate = fits_uni[["age"]], 
             with_missings = fit_mis, imputed = fit_imp)
fits %>%
  map_df(tidy, .id = "model") %>%
  select(term, model, estimate) %>%
  spread(model, estimate)
```

And for the p-value (precision):


```{r}
fits %>%
  map_df(tidy, .id = "model") %>%
  select(term, model, p.value) %>%
  spread(model, p.value)
```

### e)

> How many missing values are imputed in the variable 'pupil.i'? How many more cases can be
analyzed by using 'pupil.i' rather than 'd.pupil'? 

See above

### f) 

> The regression coefficients of the logistic model can be used to calculate the individual
predicted risk of unfavorable outcome. Fit the model (as in d) again and use the option
<save> <predicted values> <probabilities>.
Note that your dataset (not the output screen) shows an extra column.

Predicted probabilities are stored in the R object

```{r}
fit_imp$fitted.values[1:10]
```

### g) 

> Look at the descriptives of the predicted risks. Is the range very narrow / reasonably wide?

```{r}
summary(fit_imp$fitted.values)
```

Looks like it covers a whide range of the 0-1 interval

### h)

> If the model is well calibrated, groups of patients with low predicted risks will include only few
patients with unfavorable outcomes; groups of patients with high predicted risks many.
To check this, group the patients by predicted risk (use "recode into different variable"):
1: 0.00 - 0.15
2: 0.15 - 0.30
3: 0.30 - 0.40
4: 0.40 - 0.60
5: 0.60 - 1.00
Give the observed proportions of patients with unfavourable outcome for each group (use
crosstabs with option cells, percentage).

Add predicted to data.frame

```{r}
tbi %<>% mutate(
  pred_unfav = fit_imp$fitted.values,
  pred_unfav_cat = cut(pred_unfav, breaks = c(0, .15, .3, .4, .6, 1)))
```

View results

```{r}
tbi %>%
  group_by(pred_unfav_cat) %>%
  summarize(observed_prob = mean(d.unfav))
```

These line up OK

Let's plot calibration

```{r}
tbi %>%
  mutate(pred_unfav_deciles = quant(pred_unfav, n.tiles = 10)) %>%
  group_by(pred_unfav_deciles) %>%
  summarize(observed_prob = mean(d.unfav),
         observed_prob_lo = binom.confint_logical(d.unfav)$lower,
         observed_prob_hi = binom.confint_logical(d.unfav)$upper) %>%
  ggplot(aes(x = seq(0.05, .95, length.out = 10), 
             ymin = observed_prob_lo, ymax = observed_prob_hi,
             y = observed_prob)) +
  geom_errorbar() + geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0), lty = 2) +
  lims(y = c(0,1))
```



### i) 

> Use the Hosmer-Lemeshow test to assess the calibration of the model as fitted in step d). By default, this test groups patients by deciles of risk. Does the test give a statistically significant result? Is that to be expected when a model is fitted and tested for fit in the same data?

```{r}
ResourceSelection::hoslem.test(x = tbi$d.unfav, y = tbi$pred_unfav, g = 10)
```

No rejection of null-hypothesis. Seems to fit OK.

Howerever the fit was based on the data, so this may be overfitted.

> What do you think would happen with calibration at external validation, i.e. predictions are made for another data set?

Probably, calibration will be worse.

However, we have 851 cases, and fitted 5 degrees of freedom, so overfitting 
should be limited

### j) 

> Study the discriminative ability of the model with the ROC curve.
Use <analyze> <ROC curve>.
For comparison, make also a ROC curve for age alone as a single predictor.

```{r}
logit_roc <- function(fit, add = F, ...) {
  if (!("glm" %in% class(fit)) & fit$family$family == "binomial" & fit$family$link == "logit") {
    stop("only works for glm fits with family = binomial(link = 'logit')")
  }
  
  formula0  = formula(fit)
  all_vars  = all.vars(formula0)
  response  = all_vars[1]
  all_terms = all_vars[-1]

  roc <- pROC::roc(fit$data[[response]], fit$fitted.values, ci = T)
  pROC:::plot.roc.roc(roc, ci = T, add = add, ...)
}
logit_roc(fit_imp, main = "multivariate model")
```


```{r}
logit_roc(fit_imp, main = "comparison with univariate model of only age")
logit_roc(fits_uni[["age"]], add = T)
```


### k) 

> What would you expect for the area under the ROC-curve) if the model were applied in a new data set (external validation)?

A little worse.

> What would you expect if the prognostic model is developed in a selection of patients with very narrow
inclusion criteria with respect to important predictors such as age and motor score?

It will do a worse, since contrasts are smaller.

# Day 3

> The aim of this practical is to introduce you to some of the more commonly used techniques to perform shrinkage and validation. In this practical you will use R (SPSS doesn’t have sufficient functionality).

## TBI revisited

> Start R
This practical can be done in R-studio. Once R-studio is started make sure you open a new R-script (File -> New File -> R script). Start your script by including the following code to load the appropriate R-packages (copy, paste and run (Control+R)):

### dependencies #


```{r}
require(glmnet)
require(rms)
require(ggplot2)
require(logistf)

```

> If these packages haven’t been installed already, you may install them by using:
install.packages("package name")
Data
The easiest way to work with data in R is to first assign a so-called working directory. For instance, if you want to use the folder: “H/prognostic_research/practical3”, you can use the following code:

> setwd("H:/prognostic_research/practical3")
We are going to use a slightly modified version of the Traumatic Brain Injury (TBI) data (TBIday3.RDS). If you haven’t done so already, download this data set (epidemiology-education.nl) and store it in your working directory. Then use these code lines to load these data in R and split it up into a development and a validation data set:

### load development and validation data #


```{r}
tbi2 <- readRDS(here("data", "TBIday3.RDS"))
devdata <- tbi2[tbi2$trial=="Tirilazad US",-1]
valdata <- tbi2[tbi2$trial=="Tirilazad International",-1]

```

### Descriptive statistics
> We are going to model mortality at 6 months (d.mort) as a function of ten prognostic factors: 1. age 2. hypoxia 3. hypotens 4. glucose 5. hb 6. d.sysbpt 7. edh 8. tsah 9. shift 10. pupil.dich (dichotomized to “both pupils reactive” (0) and “not both pupils reactive”" (1)). The definitions of these prognostic factor are found in the documents of the practical of day 2.

> Q1: What are the numbers of events in both data sets? Are the numbers of events sufficient to warrant development and validation?
Code:

```{r}
table(devdata$d.mort)
table(valdata$d.mort)

```

> Answer: The number of events in the development data is 225 and in the validation data is 278. There seems enough data for a validation study (>200 events). Given that we have 10 candidate predictors (and we assume no interaction or non-linear relationships with the outcome), EPV = 22.5 for development, which is higher than the often suggested minimum (EPV=10) but lower than EPV=50. One may therefore expect that these models may suffer from some level of overfitting, especially when variable selection strategies are employed.

### Q2 

> Study the the distribution of the continuous predictor variables in the development data: age, glucose, hb and d.sysbpt. Are these variable normally distributed? If not: should we make adjustments?
Code:

```{r}
# ggplot(aes(age, colour = as.factor(d.mort)),data=devdata)+geom_density()
# ggplot(aes(glucose, colour = as.factor(d.mort)),data=devdata)+geom_density()
# ggplot(aes(hb, colour = as.factor(d.mort)),data=devdata)+geom_density()
# ggplot(aes(d.sysbpt, colour = as.factor(d.mort)),data=devdata)+geom_density()

```

Or we can do:

```{r}
pred_vars <- c("age", "hypoxia", "hypotens", "glucose", "hb", "d.sysbpt", "edh",
               "tsah", "shift", "pupil.dich")
num_vars  <- c("age", "glucose", "hb", "d.sysbpt")
resp_var  <- c("d.mort")

tbi2 %>%
  select(num_vars, resp_var) %>% 
  mutate(d.mort = factor(d.mort)) %>%
  gather(-d.mort, key = "variable", value = "value") %>%
  ggplot(aes(x = value, col = d.mort)) + 
  geom_density() + 
  facet_wrap(~variable, scales = "free")
```


> Answer: not all continuous predictor variables are normally distributed. There are, however, no direct assumptions made about the distribution of predictor variables in a logistic regression. No adjustments have to be made at this point.

### Q3: 

> Look at the univariable associations between the d.mort and binary predictor variables in the development data. Make cross-tables.
Code:

```{r}
table(devdata$hypoxia,devdata$d.mort)
table(devdata$hypotens,devdata$d.mort)
table(devdata$edh,devdata$d.mort)
table(devdata$tsah,devdata$d.mort)
table(devdata$shift,devdata$d.mort)
table(devdata$pupil.dich,devdata$d.mort)

```

> Develop prediction models by maximum likelihood

### Q4

> Develop a prediction model by maximum likelihood with all 10 predictors incorporated (no selection or shrinkage). What is the apparent area under the ROC curve (C statistic) for this model?
Code:

```{r}
# logistic regression model (Full model)
m1 <- d.mort~age+hypoxia+hypotens+glucose+hb+d.sysbpt+edh+tsah+shift+pupil.dich
full_model <- lrm(as.formula(m1),data=devdata,x=T,y=T)
full_model
```

> Answer: the apparent C statistics is 0.774.


### Q5

> Now use step-wise backward selection with alpha = .05. What is the apparent area under the ROC curve (C statistic) of this model? Which variables are dropped from the model? How does it compare to the full model?
Code:

```{r}
# logistic regression model (backward selection)
selection <- fastbw(full_model,rule="p",sls=.05)
bw_model <- lrm(as.formula(paste("d.mort~",paste(selection$names.kept,collapse="+"))),data=devdata)
bw_model
```

> Answer: hypotens, edh and hb were deleted from the final model. The apparent C statistic of the final model is 0.771, very close to the apparent C statistic of the full model.

>  Perform internal validation using bootstrap
Background
Optimism (due to overfitting) can be investigated using the bootstrap. One bootstrap sample is a random sample with replacement of the original data. A bootstrap sample has the same dimensions, i.e. sample size, as the orginal data set. To study optimism: multiple bootstrap samples are generated (say, 1000 bootstrap samples). The prediction model is fitted on each of those samples. If variable selection is applied: this procedure is executed on each bootstrap sample. The predictive performance of these (final) bootstrap models are evaluated on the original data sample. The average of the bootrap performances provides an estimate of performance of the model in the original data sample that is corrected for optimism.

> The above described bootstrap procedure is implemented in the validate function (rms library).

### Q6

> Perform an internal validation of the full model (Q4). For computational time reasons: take 200 bootstrap samples.

```{r}
# internal validation full model
internalfull_model <- validate(full_model,B=200)
internalfull_model
```

### Q7

> Calculate the optimism corrected C statistic for the full model. Make use of the fact that: C = (Dxy/2)+0.5.

```{r}
internalfull_model[1,] / 2 + 0.5
```

> Answer: the bootstrap corrected estimate for the C statistic is about .762. This may vary slightly between executions because bootstrap sampling is a random process.

### Q8

> Perform an internal validation of the backward selection model. For computational time reasons: take 100 bootstrap samples. Hint: the selection must be executed on each bootstrap sample.
Code:

```{r}
# internal validation backward selection model
internvalbw_model <- validate(full_model,bw=T,rule="p",sls=.05,B=200)
internvalbw_model
```

### Q9 

> Calculate the optimism corrected C statistic for the backward selection model.

```{r}
internvalbw_model[1, ] / 2 + .5
```


> Answer: the bootstrap corrected estimate for the C statistic is about .756.

### Q10

> Does this internal validation exercise provide evidence of over- or underfitting? If so, which of these models are affected?
Answer: the corrected calibration slopes (“Slope” in the output of the validation function) are around .942 (full model) and .928 (after backward selection). This indicates that the models suffer from some overfitting (Slope = 1 indicates no overfitting, Slope > 1 indicates underfitting).

## Shrinkage using likelihood penalization

### Background

>maximum likelihood with or without stepwise selection is still the most commonly used approach for developing prediction models. However, it is long known that maximum likelihood estimation is not optimal for prediction purposes. For this, the maximum likelihood estimates need shrinkage.

> Maximum likelihood estimation of the logistic model proceeds by maximizing the log-likelihood function:
logL=∑iyilogπi+(1−yi)log(1−πi),
where i stands for individual i, yi is the observed outcome for individual i and πi is the predicted outcome for individual i. There are several methods available to perform shrinkage. We here discuss three methods that are also known as penalized likelihood models. Each of these methods have the form:
logL−p(⋅),
where p(⋅) stands for a penalty function. Firth’s correction gives penalty: −1/2log|I(θ)|, where log|I(θ)| denotes the Fisher information matrix; Ridge gives penalty: p(⋅)=λ∑j=1β2j, where λ denotes a so-called tuning parameter and βj denotes regression coefficient j; and Lasso gives penalty p(⋅)=λ∑dfj=1|βj|. Estimating the tuning parameters (λ) for Lasso and Ridge is often done using a cross-validation approach. Details appear in a book called “The Elements of Statistical Learning” by Trevor Hastie et al.

### Q11

> Develop a model using Firth’s correction (all ten variables included). Compare the estimated regression coefficients to the full model (Q4).
Code:

```{r}
require(logistf)
# logistic regression model with Firth's correction (Full model)
firth_model <- logistf(as.formula(m1),data=devdata,firth=T)
firth_model
```

> Answer: the coefficients are slightly ‘shrunken’ towards to zero effect as compared to the orignal full model estimated with maximum likelihood.

```{r}
cbind(max_likelihood = coef(full_model), firth = coef(firth_model))
```

### Q13

> Develop a model using Ridge (all ten variables included). Compare the estimated regression coefficients to the full model (Q4). Note: estimating this model may take some time.
Code:

```{r}
# logistic Ridge regression model using leave-one-out cross-validation
ridge_tuning_parameter <-cv.glmnet(
  x=as.matrix(devdata[,-1]),
  y=as.matrix(devdata[,1]),
  family="binomial",type.measure="mse",
  alpha=0,nfolds=nrow(devdata))$lambda.min

ridge_model <-glmnet(
  x=as.matrix(devdata[,-1]),
  y=as.matrix(devdata[,1]),
  family="binomial",
  lambda=ridge_tuning_parameter,alpha=0)
```

> Answer: the coeffients can be seen using coef(ridge_model). When compared to the full model estimated with maximum likelihood, these coefficients are shrunken.

```{r}
cbind(max_likelihood = coef(full_model), 
      firth = coef(firth_model), 
      ridge = coef(ridge_model))

```


### Q14

> Develop a model using Lasso (all ten variables included). Note: estimating this model may take some time.
Code:

```{r}
# logistic Ridge regression model using leave-one-out cross-validation
lasso_tuning_parameter <- cv.glmnet(
  x=as.matrix(devdata[,-1]),
  y=as.matrix(devdata[,1]),
  family="binomial",type.measure="mse",
  alpha=1,nfolds=nrow(devdata))$lambda.min

lasso_model <-glmnet(
  x=as.matrix(devdata[,-1]),
  y=as.matrix(devdata[,1]),
  family="binomial",
  lambda=lasso_tuning_parameter,
  alpha=1)

```



```{r}
# fits <- list(
#   max_likelihood = (full_model), 
#       firth = (firth_model), 
#       ridge = (ridge_model),
#       lasso = (lasso_model))

coef(lasso_model)

```


> Answer: the coefficients can be seen using coef(lasso_model). Notice that, unlike ridge, Lasso may perform automated selection by shrinking some variables to zero.

## External validation

### Q15 

> study possible case-mix differences. Are there differences between the development and validation data?
Example code

```{r}
# compare case-mix between development and validation data
by(tbi2,tbi2$trial,function(x)colMeans(x[,-1]))
by(tbi2,tbi2$trial,function(x)table(x$hypoxia))
ggplot(tbi2,aes(hb,colour=trial))+geom_density()

```

> Answer: yes, there are differences in case-mix. For instance, hypoxia is more common in the development data than in the validation data.

Or, using data.table

```{r}
setDT(tbi2)

tbi2[, id := .I]
tbi2 %>%
  data.table::melt(id.vars = c("trial", "id")) %>%
  .[, list(mean = mean(value), sd = sd(value)), by = c("trial", "variable")] %>%
  data.table::melt(id.vars = c("trial", "variable"), 
                   variable.name = "measure", value.name = "value") %>%
  data.table::dcast(variable~trial+measure)

```



### Q16

> evaluate the predictive performance at external validiation
Code

```{r}
# external performance full model
predfull_model <- predict(full_model, newdata=valdata, type = "fitted")

val.prob(predfull_model,valdata$d.mort)
```

```{r}
# external performance backward selection model
predbw_model <- 1/(1+exp(-predict(bw_model, newdata=valdata)))
val.prob(predbw_model,valdata$d.mort)

```

```{r}
# external validation Firth's model
X <- model.matrix(firth_model, data=valdata)
predfirth_model <- 1/(1+exp(-X %*% coef(firth_model)))
val.prob(predfirth_model,valdata$d.mort)

```

```{r}
# external validation Ridge model
predridge_model <- 1/(1+exp(-predict(ridge_model, newx=as.matrix(valdata[,-1]))))
val.prob(predridge_model,valdata$d.mort)

```

```{r}
# external validation Lasso model
predlasso_model <- 1/(1+exp(-predict(lasso_model, newx=as.matrix(valdata[,-1]))))
val.prob(predlasso_model,valdata$d.mort)

```


## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
