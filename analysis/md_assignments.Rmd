---
title: "Assignments Missing Data"
author: "Wouter van Amsterdam"
date: 2018-05-23
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Setup


### Load some packages

```{r, message = F}
library(epistats) # contains 'fromParentDir' and other handy functions
library(magrittr) # for 'piping'  '%>%'
library(dplyr)    # for data mangling, selecting columns and filtering rows
library(data.table)# for advanced subsetting and rbindlist
library(ggplot2)  # awesome plotting library
library(stringr)  # for working with strings
library(purrr)    # for the 'map' function, which is an alternative for lapply, sapply, mapply, etc.
library(here)     # for managing working directory (in rstudio project)
```

# Day 2

> Introduction
You are an epidemiology consultant. You are asked to participate in a cohort study on annual influenza vaccine effectiveness. The aim of the study is to assess whether annual influenza vaccination reduces the risk of hospitalisation. The data consist of observations on eligible subjects who did, or did not, receive the annual influenza vaccine. The endpoint in this study is hospitalisation during the influenza epidemic period. Note that the vaccine was not randomly allocated; rather vaccination status may depend on characteristics of the participants in the study.

> Variable	Description	Values	Meaning
vacc	Influenza vaccination	0	Unvaccinated
1	Vaccinated
age	Age	Continuous	Years
sex	Sex	0	Male
1	Female
cvd	Cardiovascular disease	0	Absent
1	Present
pulm	Pulmonary disease	0	Absent
1	Present
DM	Diabetus mellitus	0	Absent
1	Present
contact	Number of GP contacts	Discrete	Count
hosp	Hospitalisation status	0	No hospitalisation
1	Hospitalisation
Preparation
The research question you will address here is whether influenza vaccination affects hospitalisation. As vaccination was not randomized, confounder adjustment is necessary. To address this effect, we here will fit the following regression model:

```{r}
Formula <- formula(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex)
```

> Let’s first install and load some relevant R packages

```{r}
# install.packages(c("VIM", "mnormt"))
# library("VIM")
library("mnormt")

```

> We can load the data from the cohort study on annual influenza vaccine effectiveness as follows:

> load("miss.data.uni.RData") # if the data is in your working directory
load(file.choose()) # if it is not

```{r}
load(here("data", "miss.data.uni.RData"))
```


> Before undertaking any analysis, we initialize a random seed so you can compare results with peers.

```{r}
set.seed(111111)
```

> Question 1. How many observations do we have? What percentage observations is missing, on each variable?

```{r}
summary(miss.data.uni)
str(miss.data.uni)

```

```{r}
# pctMissing <- function(x) round(colMeans(is.na(x)) * 100, 2) 
# pct.missing <- pctMissing(miss.data.uni)
# pct.missing
nna(miss.data.uni)
nna(miss.data.uni, prop = T)
```

```{r}
# miss.cvd <- is.na(miss.data.uni$cvd) # Indicators of missing information on cvd.

miss.data.uni %<>% mutate(miss.cvd = is.na(cvd))

```


> We can inspect the presence of missing values more thoroughly using aggr():

```{r}
# aggr(miss.data.uni, numbers = TRUE)
```


## Simple methods

> We will now pursue several strategies for handling the missing values for cvd in our dataset. We will start the analysis with three simple methods:

> Complete case analysis (CCA),
Dropping predictors with missing values,
Mean imputation.
For all imputation methods, we will store the estimated regression coefficient for influenza vaccination and the corresponding standard error.

```{r}
results.vacc <- data.frame("b" = numeric(), "se" = numeric())

```

## A. Complete case analysis (CCA)

> In this approach, we simply omit participants with one or more missing values from the statistical analysis. This is also the default approach in glm() (and many other statistical software packages):


```{r}
CCAmodel <- glm(Formula, family = binomial(), data = miss.data.uni)
CCAmodel
```

### Question 2. 

> How many participants were used for estimating the parameters of CCAmodel?

30207 (residual degrees of freedom + 1)

```{r}
results.vacc["CCA",] <- c(coef(CCAmodel)["vacc"], coef(summary(CCAmodel))["vacc", "Std. Error"])

```

### Question 3. 

> How does CCA affect the distribution of the variable cvd? Inspect the mean, the standard deviation and the correlation with vacc in the figure below.

Figure missing by copying.

In the complete case analysis, cvd has a lower prevalence, 
a more or less equal variance

### Question 4. 

> What is the adjusted odds ratio for vacc, and the corresponding 95% confidence interval? Is annual influenza vaccination effective in reducing the risk of hospitalisation?

```{r}
extract_RR(CCAmodel)
```

CI for adjusted OR of vacc includes 1, so no.

### Question 5. 

> Does the CCA approach yield unbiased estimates for the regression coefficients and corresponding standard errors?

Not unbiased, and standard errors are higher than needed

## B. Drop covariates

> Rather than omitting participants with missing values, it is possible to omit covariates with one or more missing values. Since we only have missing data for cvd, we can omit this variable from the statistical analysis and use all 40000 participants for estimating the adjusted odds ratio of influenza vaccination.

```{r}
dropmodel <- glm(hosp ~ vacc + DM + pulm + I(log(contact)) + age + sex, data = miss.data.uni, family = binomial()) 
dropmodel

```

```{r}
results.vacc["Drop", ] <- c(coef(dropmodel)["vacc"], coef(summary(dropmodel))["vacc", "Std. Error"])

```

### Question 6. 

> What is a key problem of aformentioned approach?

We can no longer use a full (and maybe correct) model.
If many variables have some missings, we end up with an empty model

## C. Mean imputation

> Another common approach is to replace each missing value by the mean of their observed values. This approach results in an imputed dataset, here denoted as mean.imputed.data:

```{r}
mean.imputed.data <- miss.data.uni 
mean.imputed.data$cvd[miss.data.uni$miss.cvd] <- mean(miss.data.uni$cvd, na.rm = TRUE)
summary(mean.imputed.data)

```

### Question 7. 

> How does mean imputation affect the mean of the imputed variable cvd?

It doesn't

```{r}
mean(miss.data.uni$cvd, na.rm = TRUE) # Results from CCA
mean(mean.imputed.data$cvd) # Results from mean imputation

```

### Question 8. 

> How does mean imputation affect the standard deviation of the imputed variable cvd?

Goes down

```{r}
sd(miss.data.uni$cvd, na.rm = TRUE) # Results from CCA
sd(mean.imputed.data$cvd) # Results from mean imputation

```

### Question 9. 

> How does mean imputation affect the correlation between the imputed variable cvd and the treatment vacc?

```{r}
cor(miss.data.uni, use = "complete.obs")["cvd", "vacc"]
cor(mean.imputed.data)["cvd", "vacc"]

```

Correlation goes down

A summary is given below:

```{r}
dsets <- rbindlist(list(
  original_data = miss.data.uni,
  complete_cases = miss.data.uni[complete.cases(miss.data.uni),],
  mean_imputed = mean.imputed.data
), idcol = "dataset")
dsets[, list(N = .N,
             cor_cvd_vacc = cor(cvd, vacc, use = "complete.obs"),
             mean_cvd = mean(cvd, na.rm = T),
             sd_cvd = sd(cvd, na.rm = T)), by = "dataset"]
```


### Question 10. 

> Estimate the adjusted odds ratio for influenza vaccination in the imputed data. Do you expect
an unbiased estimate of the effect of vaccination?
an unbiased estimate of the error of the effect of vaccination?

```{r}
meanmodel <- glm(Formula, data = mean.imputed.data, family = binomial())
meanmodel

```

If missing were completely at random (which it isn't), the results would 
be unbiased, but it's not.

Errors are too low (artificial precision created by imputation)

```{r}
results.vacc["Mean imputation", ] <- c(coef(meanmodel)["vacc"], coef(summary(meanmodel))["vacc", "Std. Error"])

```

## Regression

> As demonstrated in the previous exercises, all of the aforementioned approaches are problematic in clinical practice. Complete case analysis is only valid when data are MCAR, but even then may be ineffective because information from many participants is ignored. Alternatively, when covariates with missing values are dropped from the analysis model, adjustment for corresponding confounders is no longer possible (which may again lead to bias). Finally, when replacing missing values by their observed mean, we ignore (and therefore distort) their potential relation with other variables. One approach to account for this correlation is to generate subject-specific imputations, rather than imputing the same value for all subjects. This approach requires the development of a so-called prediction model, which can be developed using the complete data at hand. As cvd represents a binary variable, we can use logistic regression analysis to generate predictions for the missing values. The dependent variable is then cvd, and the independent variables are all remaining variables including the outcome hosp.

> In the following four methods, we will add increasing layers of complexity, to model the data more adequately. Note that the mean imputation we performed earlier is equivalent to an intercept-only regression model.

## D. Predict

> As a first attempt, we can simply impute missing values with their predicted value, conditional on the observed values. The model to produce these predicted values conditional on the other variables is:

```{r}
imp.outcome <- "cvd"
imp.predictors <- "hosp + vacc + DM + pulm + I(log(contact)) + age + sex"
imp.formula <- formula(paste(imp.outcome, "~", imp.predictors) )
impmodel1   <- glm(imp.formula, data = miss.data.uni, family = binomial())

```

> For binary variables, this predicted value represents a probability (type = "response"). Imputed values can then be generated as follows:

```{r}
regression1.data <- miss.data.uni
regression1.data$cvd[miss.data.uni$miss.cvd] <- prob.cvd2 <- predict(impmodel1, newdata = miss.data.uni[miss.data.uni$miss.cvd, ], type = "response")

```

### Question 11. 

> How does imputation affect the mean, the standard deviation and the correlation of the imputed variable cvd?

```{r}
mean(regression1.data$cvd) 
sd(regression1.data$cvd) 
cor(regression1.data)["cvd", "vacc"]

```

Mean goes up, SD goes down, correlation with treatment goes up

> An overview of the distribution for cvd is given below:



> We can now estimate the effect of influenza vaccination:

```{r}
regression1model <- glm(Formula, data = regression1.data, family = binomial())
regression1model 
results.vacc["Regression 1", ] <- c(coef(regression1model)["vacc"], coef(summary(regression1model))["vacc", "Std. Error"])

```

### Question 12. 

> Do you think this approach yields valid estimates for the effect of influenza vaccination?

If the missing values can be completely explained by the imputation model, then it is 
unbiased, however, precision is artificially higher

> It may be clear that the prediction that is used for imputation may still differ from the actual (unknown) value. Predicted values, however, do not portray this uncertainty, and may therefore distort subsequent analyses.

## E. Predict + noise

> We can improve upon the prediction method by adding an appropriate amount of random noise to the predicted value (van Buuren 2012). For binary outcomes, we can add model-based noise by generating the imputed value from a binomial distribution of one ‘trial’ (size = 1), conditional on covariates:

```{r}
N <- sum(miss.data.uni$miss.cvd) # Number of missing cvd
regression2.data <- miss.data.uni
regression2.data$cvd[miss.data.uni$miss.cvd] <- rbinom(n = N, size = 1, prob = prob.cvd2)

```

### Question 13. 

> How does imputation affect the mean, the standard deviation and the correlation of the imputed variable cvd?

Mean remains the same, SD goes up, correlation goes down

```{r}
mean(regression1.data$cvd) 
sd(regression1.data$cvd) 
cor(regression1.data)["cvd", "vacc"]
mean(regression2.data$cvd) 
sd(regression2.data$cvd) 
cor(regression2.data)["cvd", "vacc"]

```


> Again, we can estimate the effect of influenza vaccination:

```{r}
regression2model <- glm(Formula, data = regression2.data, family = binomial())
regression2model 
results.vacc["Regression 2", ] <- c(coef(regression2model)["vacc"], coef(summary(regression2model))["vacc", "Std. Error"])

```


> It may be clear that regression method 2 already works quite well. In practice, however, the predict + noise method may become problematic when sample sizes are relatively small and for this reason more advanced imputation methods are preferred. We therefore consider 2 additional extensions below.

### F. Predict + noise + parameter uncertainty

> Adding noise is a major step forward, but not quite right. The method in the previous section requires that the intercept and slope of the imputation model impmodel1 are known. However, the values of these parameters are estimated from the data at hand, and are particularly uncertain when sample sizes are relatively small. Hence, we can further improve our imputations as follows. Rather that directly using the estimated regression coefficients for generating predicted values for cvd, we can add variability in these coefficients by relating to their standard error.

> For each patient, generate a random draw of regression coefficients from the multivariate Student-t distribution (rmt() from the  mnormt package). Ideally, we only need to do this for patients with missing values (denoted by miss.cvd). As we want to incorporate the uncertainty of the coefficients into the predictions, we cannot simply use predict(imp.model1) anymore. We will have to code it ourselves, as follows.

> We start with drawing coefficients from a multivariate Student-t distribution:

```{r}
beta.i3 <- rmt(N, mean=impmodel1$coef, S=vcov(impmodel1), df=impmodel1$df.residual)
head(beta.i3)
str(beta.i3)
```

> Now we have to structure our data with missing cvd observations, so that we can apply our regression coefficients afterwards to calculate the individual predictions. The predict() function does this behind the scenes, but can no longer be used here as we are changing the regression coefficients. Hence, we will use the model.matrix() function to prepare the data:

```{r}
r3.pred.data <- model.matrix(formula(paste("~", imp.predictors)),
                             data=miss.data.uni[miss.data.uni$miss.cvd,])
head(r3.pred.data)
str(r3.pred.data)

```

Note that this is just the data with added intercept and log transformation,
in the form of a numeric matrix

> Now that we have our data, we can calulate the linear predictors and use the inverse logit to produce the probabilities of cvd.

```{r}
prob.cvd3 <- rep(NA, N)

system.time({
for (i in 1:N) {
  prob.cvd3[i] <- 1/(1+exp(-r3.pred.data[i,] %*% beta.i3[i,]))
}
})
```

We can also do this without a for-loop, using element-wise matrix multiplication
and rowsummation

```{r}
dim(beta.i3)
dim(r3.pred.data)
system.time({
prob.cvd3.1 <- 1 / (1 + exp(rowSums(-r3.pred.data * beta.i3)))
})
max(abs(prob.cvd3 - prob.cvd3.1))
```

Which is a lot faster since it exploits R's vectorization optimizations

> And finally, we again draw cvd from a binomial distribution to account for sampling variation:

```{r}
regression3.data <- miss.data.uni
regression3.data$cvd[miss.data.uni$miss.cvd] <- rbinom(n = N, size = 1, prob = prob.cvd3)

```

> The resulting distribution for cvd is given below:

> Again, we use the imputed data to estimate the effect for influenza vaccination:

```{r}
regression3model <- glm(Formula, data = regression3.data, family = binomial())
regression3model 
results.vacc["Regression 3", ] <- c(coef(regression3model)["vacc"], coef(summary(regression3model))["vacc", "Std. Error"])

```

### G. Multiple Imputation

> So far, we have analyzed imputed datasets as if all their data were actually observed. It may be clear that this approach is problematic, as it ignores any uncertainty arising from imputation. Moreover, because the imputation methods based on regression imputed values through random sampling, the validity of imputations becomes highly dependent on chance. In order to preserve all uncertainty arising from imputation, we need to repeat the sampling procedures many times, and generate many imputed datasets. Then, values that can reliably be imputed will not vary much across imputed datasets, whereas other values that are difficult to impute will substantially vary across imputed datasets. This variation in imputations for a certain missing value can lead to differences in the analysis of imputed datasets, thereby reflecting to what extent our results (i.e. odds ratio of influenza vaccination) are affected by the presence of missing data.

> Briefly, we can repeat the imputation procedure as follows:

```{r}
n.imp <- 50 # Number of predictions per patient.
results.vacc4 <- as.data.frame(matrix(NA, ncol = 5, nrow = n.imp)) # Save distribution of vacc for each imputed dataset
colnames(results.vacc4) <- c("mean", "sd", "cor", "beta.vacc", "se.vacc")

# Initialize full dataset
regression4.data <- miss.data.uni

# Initiatize data for which imputations are needed
r4.pred.data <- model.matrix(formula(paste("~", imp.predictors)),
                             data=miss.data.uni[miss.data.uni$miss.cvd,])

# Multiple Imputation
for (j in 1:n.imp) { 
  beta.i4 <- rmt(N, mean=impmodel1$coef, S=vcov(impmodel1), df=impmodel1$df.residual)
  prob.cvd4 <- rep(NA, N)
  for (i in 1:N) {
    prob.cvd4[i] <- 1/(1+exp(-r4.pred.data[i,] %*% beta.i4[i,]))
  }

  regression4.data$cvd[miss.data.uni$miss.cvd] <- rbinom(n = N, size = 1, prob = prob.cvd4)
  regression4model   <- glm(Formula, data = regression4.data, family = binomial())

  # Save the results
  results.vacc4$mean[j] <- mean(regression4.data$cvd)
  results.vacc4$sd[j] <- sd(regression4.data$cvd)
  results.vacc4$cor[j] <- cor(regression4.data)["cvd", "vacc"]
  results.vacc4$beta.vacc[j] <- coef(regression4model)["vacc"]
  results.vacc4$se.vacc[j]    <- coef(summary(regression4model))["vacc", "Std. Error"]
}

```

> We then obtain 50 imputed datasets, for which each one we can assess the distribution of vacc and its log odds ratio:

```{r}
print(results.vacc4)

```


> Although it is helpful to present results separately for each imputed dataset, this is often impractical. For this reason, Rubin has proposed some rules to combine the results across datasets. For simple statistics such as the mean or standard deviation, we can simply take the average:

```{r}
apply(results.vacc4[,c("mean", "sd", "cor", "beta.vacc")], 2, mean)

```

> More technically, suppose that Q̂ l is the estimate of the lth repeated imputation, then the combined estimate is equal to

$$\bar{Q} = \frac{1}{m}\sum_{l = 1}^m{\hat{Q_l}}$$

> To calculate the standard error of the pooled estimates, we need to account for variation within and between the imputed datasets. In other words, it is not sufficient to take the average of results.vacc4$se.vacc to obtain the standard error of the pooled regression coefficient for vacc. Instead, the total error variance of Q¯ is given as:

$$T = \bar{U} + (1 + \frac{1}{m})B$$

> where m is the amount of generated imputations (i.e. n.imp). Further, we have:

$$\bar{U} = \frac{1}{m}\sum_{l = 1}^m{\bar{U_l}}$$

> where the square root of U¯l denotes the complete-data standard errors results.vacc4$se.vacc. Finally, we have:

$$B = \frac{1}{m-1}\sum_{l = 1}^m{(\hat{Q_l} - \bar{Q})^2}$$

> The standard error for the pooled regression coefficient for vacc is thus given as:

```{r}
Qbar <- mean(results.vacc4$beta.vacc)
U <- sum(results.vacc4$se.vacc**2)/n.imp
B <- sum((results.vacc4$beta.vacc - Qbar)**2)/(n.imp-1)
se.beta.vacc <- sqrt(U + (1+1/n.imp)*B)
se.beta.vacc

```


```{r}
# Store results
results.vacc["Regression 4", ] <- c(Qbar, se.beta.vacc)

```

### H. Multiple Imputation in mice()

> We can directly apply all of the aforementioned steps via the mice() package:

```{r}
data.mice <- model.frame(formula(paste("~ 0 + ", imp.predictors, "+", imp.outcome)), 
                         data = miss.data.uni, na.action = 'na.pass')
data.mice$cvd <- as.factor(data.mice$cvd)
colnames(data.mice)[5] <- "logContact"
head(data.mice)

```

```{r}
# Initialize Imputation Model
require(mice)
setup.imp <- mice(data.mice, maxit=0)

# Lets ensure logistic regression is used for imputation
setup.imp$method["cvd"] <- "logreg"

# Start the imputation. No Gibbs sampler is needed (hence maxit=1)
system.time({
dat.imp <- mice(data.mice, method = setup.imp$method, m = n.imp, maxit = 1, printFlag = F)
})

# Start the analyses
regression5model <- with(data=dat.imp, 
                         exp=glm(hosp ~ vacc + DM + cvd + pulm + logContact + age + sex,
                                 family = binomial()))

# Use Rubin's rules
pooledEst <- pool(regression5model)

# Store results
results.vacc["Regression 5", ] <- c(pooledEst$qbar["vacc"], sqrt(pooledEst$t["vacc", "vacc"]))

```

NB: note that the mice package exports a modified method for multiple imputed
datasets (see mice::glm.mids).

### Comparison

> For comparison, we will now analyze the original data where no missing values were presen. We can then compare the coefficients and standard errors of our models to what we would have obtained if we had had the full data. First we load and inspect the full data:
And apply the logistic model:

```{r}
load(here("data", "full.data.RData"))
fullmodel <- glm(Formula, data = FULL.data, family = binomial())
results.vacc["Original data", ] <- c(coef(fullmodel)["vacc"], coef(summary(fullmodel))["vacc", "Std. Error"])

```

### Question 14. 

> For which method are the obtained parameter estimates closest to estimates of the full data model?

```{r}
results.vacc
```


In terms of bias:

For regression 2 (predict + noise), 1 (predict), 
4 (manual multiple imputation) and 5 (mice, equivalent to 4)

Only regression 5 has also has increased SE (which is appropriate, since we have fewer dataa0)

> Or use the following function to compute the percentage difference between the estimates obtained with the different strategies, and the estimates obtained on the full data:

```{r}
PCT.diff <- function(x, ref = ncol(x)) 
{
  x <- t(as.matrix(x))
  y <- round(t(((x[ , -ref] - x[ , ref]) / x[, ref])[, -ref] * 100), 2)
  y[ , ] <- paste(y, "%", sep = "")
  noquote(y)
}

PCT.diff(results.vacc)

```

> References
van Buuren, Stef. 2012. Flexible Imputation of Missing Data. Chapman & Hall/Crc Interdisciplinary Statistics Series. Boca Raton, Fla.: CRC Press.

# Day 3

> Missing Data practical III
Valentijn de Jong and Thomas Debray
May 2018
This practical was developed using R version 3.4.3.

> Introduction
You are an epidemiology consultant. You are asked to participate in a cohort study on annual influenza vaccine effectiveness. The aim of the study is to assess whether annual influenza vaccination reduces the risk of hospitalisation. The data consist of observations on eligible subjects who did, or did not, receive the annual influenza vaccine. The endpoint in this study is hospitalisation during the influenza epidemic period. Note that the vaccine was not randomly allocated; rather vaccination status may depend on characteristics of the participants in the study.

> Variable	Description	Values	Meaning
vacc	Influenza vaccination	0	Unvaccinated
1	Vaccinated
age	Age	Continuous	Years
sex	Sex	0	Male
1	Female
cvd	Cardiovascular disease	0	Absent
1	Present
pulm	Pulmonary disease	0	Absent
1	Present
DM	Diabetus mellitus	0	Absent
1	Present
contact	Number of GP contacts	Discrete	Count
hosp	Hospitalisation status	0	No hospitalisation
1	Hospitalisation
Preparation
The research question you will address here is whether influenza vaccination affects hospitalisation. As vaccination was not randomized, confounder adjustment is necessary? To address this effect, we here will fit the following regression model:

```{r}
Formula <- formula(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex)

```

## Multivariate missingness

> Yesterday we attempted to handle univariate missingness in a variety of ways. For regression-based imputation methods, imputation of Yj is based on the remaining columns Y−j as predictors. The rationale is that conditioning on Y−j preserves the relations among the Yj in the imputed data. Today we will consider multivariate missingness. As indicated by van Buuren et al. (2006), this has major implications for most imputation methods:
the predictors Y−j themselves contain missing values;
“circular” dependence occurs, where ymisj depends on ymish and ymish depends on ymisj (h≠j), because in general Yj and Yh are correlated, even given other variables;
especially with large k (number of covariates with missing values) and small n, collinearity or empty cells occur;
rows or columns can be ordered, e.g., as with longitudinal data;
variables are of different types (e.g., binary, unordered, ordered, continuous), thereby making the application of theoretically convenient models, such as the multivariate normal, theoretically inappropriate;
the relation between Yj and predictors Y−j is complex, e.g., nonlinear, or subject to censoring processes;
imputation can create impossible combinations such as pregnant fathers.
In this practical, we will adopt fully conditional specification (FCS) for imputing multivariate data. Briefly, this approach involves specifying a separate conditional density P(Yj|Yâj,θj) for each variable with missing values (Yâj). This density is used to impute ymisj given yâj, for example, by linear or logistic regression applied to the cases in yobsj. Imputation under FCS is done by iterating over all conditionally specified imputation models, each iteration consisting of one cycle through all Yj.

> Inspecting the data
First, load the data with missing observations.

```{r}
load(here("data", "miss.data.multi.RData")) # if the data is in your working directory
```

> load(file.choose()) # if it is not

## Question 1. 

> How many observations do we have? What percentage observations is missing, on each variable?

```{r}
summary(miss.data.multi)
str(miss.data.multi)
pct.missing <- round(colMeans(is.na(miss.data.multi)) * 100, 2) 
pct.missing
miss.cvd <- is.na(miss.data.multi$cvd) # Indicators of missing information on cvd.

```

## Question 2. 

> Do you think the missingness was caused by a MAR or MNAR mechanism? Why?

```{r}
miss.data.multi %>%
  group_by(is.na(cvd)) %>%
  summarize_all(nna, prop = T)
```

There are clear differences between missingness within strata of missing value for cvd.
So clearly not MCAR.

We cannot tell from the data whether they are MAR or MNAR

> First, we will explore how inefficient Complete Case Analysis becomes in the case of multivariate missingnes. Subsequently, we will use Multiple imputation to analyse the data more adequately. For all approaches, we will store the estimated regression coefficient for influenza vaccination and the corresponding standard error.

```{r}
results.vacc <- data.frame("b" = numeric(), "se" = numeric())

```

## Analysis of the original data

> For comparison, we will include results from the original data, before missing values were introduced. We can then assess the validity of estimated regression coefficients and standard errors.

```{r}
load(here("data", "full.data2.RData"))
fullmodel <- glm(Formula, data = FULL.data, family = binomial())
results.vacc["Original data", ] <- c(coef(fullmodel)["vacc"], coef(summary(fullmodel))["vacc", "Std. Error"])
```

> The estimated log hazard ratio for influenza vaccination is plotted below, together with its 95% confidence interval: 

```{r}
coefplot::coefplot(fullmodel)
```

## Ignoring missingness

## A. Complete case analysis (CCA)

> Fit the prespecified model. Complete case analysis is the default option in glm() (as is common in statistical software), so we can easily fit it as follows:

```{r}
CCAmodel <- glm(Formula, family = binomial(), data = miss.data.multi)
CCAmodel
results.vacc["CCA", ] <- c(coef(CCAmodel)["vacc"], coef(summary(CCAmodel))["vacc", "Std. Error"])

```

## Question 3. 

> Does this method use all information in the data set? Hint: check the degrees of freedom and consider the number of estimated parameters in the regression model.

no, the degrees of freedom are much lower than the total number of patients


## Multiple imputation

> We are now going to perform multiple imputation, using the mice package.

```{r}
require(mice)
```


> MICE requires that categorical (including binary) variables are entered as factors. Then MICE can automatically select a plausible distribution for each variable.

> MICE.data <- with(miss.data.multi, data.frame(sex = factor(sex), 
                                              age = age, 
                                              vacc = factor(vacc), 
                                              cvd = factor(cvd), 
                                              pulm = factor(pulm), 
                                              DM = factor(DM), 
                                              contact = contact, 
                                              hosp = factor(hosp)))
                                              
With dplyr:

```{r}
factor_vars <- c("sex", "vacc", "cvd", "pulm", "DM", "hosp")
MICE.data <- mutate_at(miss.data.multi, vars(factor_vars), funs(as.factor))
```

> We begin by performing a dry run for the multiple imputation procedure.

```{r}
set.seed(25042017)

```

```{r}
# Configuration of the MICE model
MICE.0 <- mice(MICE.data, maxit = 0)
MICE.0

```

## Question 4. 

> Interpret the output of MICE.0 (use ?mice ) + What does “Imputation methods” tell us? Why is this important? + For which variables do you want to impute? + Which variables do you want to use as predictors in the imputation model? + And which are used by default?

```{r}
str(MICE.data)
```

methods for binary variables are all logistic regression
method for contact is pmm (predictive mean matching). it is an integer, poisson would be good too.

note that it did not use a random seed (we could give it as an argument in mice)

## B. MICE - default

> By default, mice() will generate 5 imputed datasets and perform 5 iterations in the Gibbs sampler. Please be patient, as this takes a few minutes.

```{r}
MICE.imp.data.default <- mice(MICE.data)

```

> We can inspect the imputation for each variable as follows. Here, we will focus on the contact variable. Since we are generating 5 imputed datasets through a Gibbs sampler with maxit iterations, we have a total of m * maxit (i.e. 5×5) imputed datasets. In practice, we only use the imputed datasets from the last iteration of the Gibbs sampler. However, we can use imputed datasets from earlier iterations of the Gibbs sampler to assess whether imputed values lead to consistent characterics of the corresponding variables. Hereto, we can construct so-called trace line plots, which portray the value of an estimate against the iteration number. Below, we plot the mean and standard deviation of the imputed (not observed) values against the iteration number for each of the m replications. On convergence, the streams should intermingle and be free of any trend.

```{r}
plot(MICE.imp.data.default, y = "contact", layout = c(2,1))

```


## Question 5. 

> Do you think that the Gibbs sampler has properly converged? Try to increase the number of iterations and evaluate whether convergence has improved (and how many interations were needed to reach convergence)

Seems like ok mixing without trend (althoug 5 iterations is not too much)

```{r}
MICE.imp.data.20 <- mice(MICE.data, maxit = 20)
plot(MICE.imp.data.20, y = "contact", layout = c(2,1))
```

After 5 iterations, not much is changed

> We can now analyze the imputed datasets and store the results. We fit a logistic regression model in each imputed data set, and combine the results using Rubin’s rules. Note that when applying the with() statement to MICE.imp.data.default, the  glm() command recognizes this is a dataset with multiple imputations.

```{r}
# Start the analyses
MIversion1 <- with(data = MICE.imp.data.default, exp = glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex, family = binomial()))

# Use Rubin's rules
pooledEst <- pool(MIversion1)

# Store results
results.vacc["MI - default", ] <- c(pooledEst$qbar[2], sqrt(pooledEst$t[2,2]))

```

## C. MICE - regression

> So far, we have configured MICE to decide by itself how to impute each variable with missing values. As indicated, all binary variables have been imputed using logistic regression whereas missing values for contact have been imputed using predictive mean matching. We will now specify our own imputation method for this variable, and adopt linear regression for imputing  log(contact):

```{r}
MICE.data <- with(miss.data.multi, data.frame(sex = factor(sex), 
                                              age = age, 
                                              vacc = factor(vacc), 
                                              cvd = factor(cvd), 
                                              pulm = factor(pulm), 
                                              DM = factor(DM), 
                                              contact = contact,
                                              log.contact = log(contact), 
                                              hosp = factor(hosp)))

```

> Note that we now have 2 variables contact and log.contact for the number of GP contacts. In order to ensure that mice() does not impute these variables twice, it needs to be informed about their relation. This can be achieved by means of passive imputation. Briefly, passive imputation can be used to ensure that a data transform always depends on the most recently generated imputations. Here, we will use passive imputation to ensure that imputed values for contact are obtained via  log.contact (which is imputed via linear regression).

> Recall that performing a dry run in mice() generates an empty multiply imputed data set with information on the imputation procedure.

```{r}
# Configuration of the MICE model
MICE.0 <- mice(MICE.data, maxit = 0)
MICE.0

```

log.contact is still pmm

> We will now modify the imputation procedure to impute log-transformed values of contact using linear regression. First, we need to modify the predictor matrix to indicate that we will impute via the variable log.contact rather than via contact.

> We have now specified that the variable contact should no longer be used for imputation. The next step is to specify that we want to impute log.contact via linear regression:


```{r}
pred <- MICE.0$predictorMatrix
pred[, "contact"] <- 0
pred
```

> Each variable in our data set MICE.data has a row and a column in the predictor matrix. A value of 1 indicates that the column variable was used to impute missing values for the row variable. For example, the 1 at entry [vacc, sex] indicates that variable sex was used to impute the incomplete variable vacc. Note that the diagonal is zero because a variable is not allowed to impute itself. The row of sex contains all zeros because there were no missing values in sex. We can modify the predictor matrix to specify which variables should be used for imputation:

```{r}
method <- MICE.0$method
method["log.contact"] <- "norm"
```

> Finally, we need to specify the relation between contact and log.contact, so that we can directly obtain imputed values for  contact via log.contact:

```{r}
method["contact"] <- "~I(exp(log.contact))"
```

> We can now start the imputation procedure:

```{r}
# Start the imputation
MICE.imp.data.regress <- mice(MICE.data, method = method, pred = pred, print = F)
plot(MICE.imp.data.regress, y="log.contact", layout=c(2,1))

```

## Question 6. 

> Do you think that the Gibbs sampler has properly converged? Did you need fewer iterations to reach convergence as compared to MICE.imp.data.default?

again it looks pretty ok after 3 or 4 iterations

> Again, we can estimate the adjusted odds ratio for influenza vaccination in the imputed datasets:

```{r}

# Start the analyses
MIversion2 <- with(data = MICE.imp.data.regress, exp = glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex, family = binomial()))

# Use Rubin's rules
pooledEst <- pool(MIversion2)

# Store results
results.vacc["MI - regression 1", ] <- c(pooledEst$qbar[2], sqrt(pooledEst$t[2,2]))

```

## Question 7. 

>Interpret MICE.imp.data.default and MICE.imp.data.regress. Which method would you prefer for imputing binary and continuous variables?

for binary, both are equal (using logistic regression)

for continous, probably log-normal is better (the regress variant). At least it makes more sense

> A complete overview of the imputation methods in mice can be found by

```{r}
methods(mice)
```

## Congeniality

> Recall that we have missing values for the outcome variable hosp.

```{r}
summary(miss.data.multi$hosp)
```

> Although we can impute missing outcome values, this is not necessary if our analysis model includes the same variables as the imputation model. In that case, imputation does not add any new information, and missing values for hosp can be considered to be ignorable:

```{r}
# Identify for which subjects we have data for 'hosp'
hosp.observed <- which(!is.na(miss.data.multi$hosp))

# Start the analyses in patients with complete data for 'hosp'
MIversion3 <- with(data = MICE.imp.data.regress, exp = glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex, family = binomial(), subset=hosp.observed))

# Use Rubin's rules
pooledEst <- pool(MIversion3)

# Store results
results.vacc["MI - regression 2", ] <- c(pooledEst$qbar[2], sqrt(pooledEst$t[2,2]))

```

```{r}
results.vacc %>%
  tibble::rownames_to_column(var = "model") %>%
  mutate(upper = b + 1.96 * se,
         lower = b - 1.96 * se) %>%
  ggplot(aes(x = model, y = b)) + 
  geom_point() + geom_linerange(aes(ymin = lower, ymax = upper), lty = 2)
```


> More technically, patients with imputed Y contain no information about the regression of Y on X. However, this does not mean that patients with missing outcomes are useless (von Hippel 2007). As Little writes, “cases with Y missing can provide a minor amount of information for the regression of interest, by improving prediction of missing X’s for cases with Y present”.

> Let’s now investigate imputation issues that arise when modeling interaction terms. To this purpose, we will investigate whether the effect of influenza vaccination is affected by pulmonary disease (pulm). In the original data, we can do this as follows:

```{r}
# Placeholder for results
results.interaction <- data.frame(b=numeric(), se=numeric())

# Fit the model
model.int.FULL <- glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex + vacc*pulm, data = FULL.data, family = binomial())

# The estimate of interest is the interaction term
results.interaction["Original data", ] <- c(coef(model.int.FULL)["vacc:pulm"], coef(summary(model.int.FULL))["vacc:pulm", "Std. Error"])
```

We can also estimate the interaction in the complete cases:

```{r}
# Fit the model
model.int.CCA <- glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex + vacc*pulm, data = miss.data.multi, family = binomial())
# The estimate of interest is the interaction term
results.interaction["CCA", ] <- c(coef(model.int.CCA)["vacc:pulm"], coef(summary(model.int.CCA))["vacc:pulm", "Std. Error"])
results.interaction
```


> The estimated log hazard ratio for the interaction between influenza vaccination and pulmonary disease is plotted below, together with its 95% confidence interval: 

> Evidently, as we have missing values for vacc, pulm and most of the confounders, aforementioned approach is not feasible in the “actual” dataset. For this reason, we can adopt multiple imputation. Previously, we imputed missing values using regression analysis (MICE.imp.data.regress) and we can use the resulting datasets to analyze the interaction:

```{r}
# Start the analyses
MIversion5 <- with(data = MICE.imp.data.regress, exp = glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex + vacc*pulm, family = binomial()))

# Use Rubin's rules
pooledEst <- pool(MIversion5)

# Store results
results.interaction["MI - uncongenial", ] <- c(pooledEst$qbar[9], sqrt(pooledEst$t[9,9]))
results.interaction
```


> This yields:

> Note that we obtain attenuated results for the interaction term. This is because the standard MICE algorithm only models main effects, such that our analysis model including an interaction term has become uncongenial/incompatible with the imputation model (which ignores any interaction terms). To avoid this problem we need to add interactions of interest to the imputation model.

```{r}
MICE2.data <- with(miss.data.multi, data.frame(sex = factor(sex), 
                                              age = age, 
                                              vacc = factor(vacc), 
                                              cvd = factor(cvd), 
                                              pulm = factor(pulm), 
                                              DM = factor(DM), 
                                              contact = contact,
                                              log.contact = log(contact), 
                                              hosp = factor(hosp),
                                              i_vacc_pulm = factor(vacc*pulm)))
head(MICE2.data)

```

> One approach is to account for the interaction i_vacc_pulm is adopt passive imputation (von Hippel 2009, Seaman, Bartlett, and White (2012)). Unfortunately, this approach generally leads to bias as the missing values are imputed from a model which assumes additive effects. A solution to this problem is to impute the interaction variable i_vacc_pulm directly, as if it were ‘just another variable’ (JAV). The consequence of this is that in the imputed datasets, the imputed values of the interaction variable  i_vacc_pulm will not necessarily be equal to the product of vacc and pulm. Although it has been demonstrated that JAV provides consistent estimation when X is MCAR von Hippel (2009), it generally leads to biased results when data are MAR (Seaman, Bartlett, and White 2012). As an example, we apply the JAV approach to our data:

```{r}
# Configuration of the MICE model
MICE2.0 <- mice(MICE2.data, maxit = 0)

# Passive imputation for 'contact'
pred <- MICE2.0$predictorMatrix
pred[,"contact"] <- 0
method <- MICE2.0$method
method["log.contact"] <- "norm"
method["contact"] <- "~I(exp(log.contact))"

# Start the imputation
MICE.imp.data.interaction <- mice(MICE2.data, method = method, pred = pred, print = F)
plot(MICE.imp.data.interaction, y="i_vacc_pulm", layout = c(2,1))

```


## Question 8. Did the Gibbs sampler converge?

No, lines are not yet mixing nicely

```{r}
# Start the analyses
MIversion6 <- with(data = MICE.imp.data.interaction, exp = glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex + i_vacc_pulm, family = binomial()))

# Use Rubin's rules
pooledEst <- pool(MIversion6)

# Store results
results.interaction["MI - JAV linear", ] <- c(pooledEst$qbar[9], sqrt(pooledEst$t[9,9]))

```


> Note that the JAV approach as implemented here suffers from convergence problems and is not able to preserve the treatment-covariate interaction. This problem arises because we are trying to impute vacc * pulm using a logistic regression model where the linear predictor is given by vacc + pulm + hosp + DM + cvd + log.contact + age + sex. Evidently predicting an interaction via a linear sum does not work. For this reason, other imputation methods such as classification and regression trees (CART) have been proposed (Doove, Van Buuren, and Dusseldorp 2014). The CART approach searches for a split that is most predictive of the response variable by searching through all predictor variables. Within the subgroups created from one predictor variable, a new split is searched within the same variable or other predictor variables. Since splits are conditional on previous splits, the variables used may indicate interaction effects (Doove, Van Buuren, and Dusseldorp 2014). Note that maxit is set to a very high number, as achieving convergence remains difficult.


```{r}
method["i_vacc_pulm"] <- "cart"

# Start the imputation
MICE.imp.data.interaction2 <- mice(MICE2.data, method = method, pred = pred, print = F, maxit = 200)
plot(MICE.imp.data.interaction2, y="i_vacc_pulm", layout=c(2,1))


# Start the analyses
MIversion7 <- with(data = MICE.imp.data.interaction2, exp = glm(hosp ~ vacc + DM + cvd + pulm + I(log(contact)) + age + sex + i_vacc_pulm, family = binomial()))

# Use Rubin's rules
pooledEst <- pool(MIversion7)

# Store results
results.interaction["MI - JAV nonlinear", ] <- c(pooledEst$qbar[9], sqrt(pooledEst$t[9,9]))

```

> This yields:

```{r}
results.interaction %>%
  tibble::rownames_to_column(var = "model") %>%
  mutate(upper = b + 1.96 * se,
         lower = b - 1.96 * se) %>% 
  ggplot(aes(x = model, y = b)) + 
  geom_point() + geom_linerange(aes(ymin = lower, ymax = upper), lty = 2)
```


> To conclude, there is currently no perfect approach to impute transformed variables and interaction terms. Previously, Seaman, Bartlett, and White (2012) formally compared predictive mean matching (PMM), passive imputation and JAV. They found that none of the three MI methods worked well in MAR scenarios. In general, JAV performed better than passive imputation or PMM for linear regression with an interaction effect. However, for logistic regression (as is the case here), the best performing imputation method was PMM.

##Final questions

## Question 9 (“Bonus”). 

> Assess the influence of adjusting the number of multiple imputations and/or the number of iterations. How many imputation and iterations do you think we need?

## References

> Doove, L.L., S. Van Buuren, and E. Dusseldorp. 2014. “Recursive Partitioning for Missing Data Imputation in the Presence of Interaction Effects.” Computational Statistics & Data Analysis 72 (April): 92–104. doi:10.1016/j.csda.2013.10.025.
Seaman, Shaun R., Jonathan W. Bartlett, and Ian R. White. 2012. “Multiple Imputation of Missing Covariates with Non-Linear Effects and Interactions: An Evaluation of Statistical Methods.” BMC Med Res Methodol 12 (April): 46. doi:10.1186/1471-2288-12-46.
van Buuren, Stef, J. P.L. Brand, C. G.M. Groothuis-Oudshoorn, and D. B. Rubin. 2006. “Fully Conditional Specification in Multivariate Imputation.” Journal of Statistical Computation and Simulation 76 (12): 1049–64. doi:10.1080/10629360600810434.
von Hippel, Paul T. 2007. “Regression with Missing Ys: An Improved Strategy for Analyzing Multiply Imputed Data.” Sociological Methodology 37 (1): 83–117. doi:10.1111/j.1467-9531.2007.00180.x.
———. 2009. “How to Impute Interactions, Squares, and Other Transformed Variables.” Sociological Methodology 39 (1): 265–91. doi:10.1111/j.1467-9531.2009.01215.x.




## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
