---
title: "Maximum likelihood estimation and logistic regression"
author: "Wouter van Amsterdam"
date: 2018-01-09
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Introduction

Tutor: Jan van den Broek

## Logistic regression

Binary outcome.

### Binary outcome, binary predictor

Contingency table. $\chi^2$ table.

### Notation

Population description

* Prevalence $\pi_0, \pi_1$
* Odds: $\frac{\pi_i}{1-\pi_i}$
* Odds ratio: $w$

Population description

* Prevalence in exposure group $i$: $p_i$

### Logarithm math

$$^{b}\log(x) = y\iff b^y = x$$
$$^e\log(x) = \ln(x)$$

$$\log(x*y) = \log(x) + \log(y)$$
$$\log(\frac{x}{y}) = \log(x) - \log(y)$$

$$\log(x^a) = a*\log(x)$$

$$\ln(\frac{\pi}{1-\pi})=a \iff \pi = \frac{e^a}{1+e^a}$$

### Logistic regression model

Log odds

$$\ln(\frac{\pi}{1-\pi})=w$$

Where 

$$w = \beta_0 + \sum_{i}{\beta_i*x_i}$$

Then $\beta_i = \ln(OddsRatio_{x_i})$

#### Centering

For interpretation it can be better to center predictors.

For instance: $age - mean(age)$

This gives a better interpretation of the intercept.

#### Simulate data
```{r, message = F}
require(purrr)
library(tidyverse)

set.seed(2)
n = 1000

x1 = rnorm(n)
or1 = 1/10
b1 = log(or1)

x2 = runif(n)
or2 = 15
b2 = log(or2)

base_risk = 0.3
base_odds = base_risk / (1-base_risk)
b0 = log(base_odds)

y <- x1 %>% 
  map_dbl(function(x) {
    odds = exp(x * b1) * base_odds
    risk = (odds) / (1+odds)
    sample(c(0,1), size = 1, prob = c(1-risk, risk))
  })

y <- 
  map2_dbl(x1, x2, function(z1, z2) {
    odds = exp(b0 + z1 * b1 + z2 * b2)
    risk = (odds) / (1+odds)
    sample(c(0,1), size = 1, prob = c(1-risk, risk))
  })

fit <- glm(y ~ x1 + x2, family = binomial)

print(b0)
print(b1)
print(b2)

summary(fit)

ggplot(data.frame(x1, x2, y = factor(y)), 
       aes(x = x1, y = x2, col = y, shape = y)) + 
  geom_point() + 
  theme_minimal() + theme(panel.grid = element_blank()) + 
  geom_abline(intercept = -fit$coefficients[1]/fit$coefficients[2], 
              slope =  -fit$coefficients[3]/fit$coefficients[2])
```



#### Create a general function for simulating logistic regression data


## Maximam likelihood estimation

### Notation

* Random variable with values $0$ and $1$, with attached probabilities: $Y_i$; Values + probability attached
* Realization of $Y_i$ written as $y_i$
* For the binary case, with $P(Y_i = 1) = \pi$, than we can write: 

$$P(Y_i = y_i) = \pi^{y_i}(1-\pi)^{1-y_i}$$

This can be written for every $n$ observations $y_i$ in a sample.

The **likelihood** of oberving all $y_i$

$$L(\pi) = P(Y_1 = y_1 \& Y_2 = y_2 \& ... \& Y_n = y_n) = P(Y_1=y_1)*P(Y_2=y_2)*...*P(Y_n = y_n)$$
$$ L(\pi) =\prod_i^n{P(Y_i=y_i)}=\pi^{\sum_i{y_i}}*(1-\pi)^{\sum_i{(1-y_i)}}$$

Likelihood is a function of $\pi$, 
it gives the likelihood of $\pi$, 
given the observed data.

When $\pi$ maximizes the likelihood is the maximum 
likelihood estimator



Log likelihood

$$l(\pi) = ln[L(\pi)] = \sum_i^n[y_iln(\pi)+(1-y_i)ln(1-\pi)]$$

We can get the max by differentiating with respect to $\pi$.

$$l'(\pi) = \sum_i^n[\frac{y_i}{\pi}-\frac{1-y_i}{1-\pi}]$$



Function of log-likelihood can be very peaked or 
very flat. The exact estimators can be the same, 
but confidence bounds different.

To check in which situation you are, calculate second 
derivative

Definition of **information**: $-l''(p)$

$$l''(\pi) = \sum_i[-\frac{y_i}{\pi^2}-\frac{1-y_i}{(1-\pi)^2}]$$

##### Create functions

```{r}
likelihood <- function(p, observations) {
  n_positive = sum(observations)
  (p^sum(n_positive))*((1-p)^(length(observations)-n_positive))
}

log_likelihood <- function(p, observations) {
  n_positive = sum(observations)
  n_negative = length(observations) - n_positive
  n_positive*log(p)+n_negative*log(1-p)
}

log_likelihood_first_derivative <- function(p, observations) {
  n_positive = sum(observations)
  n_negative = length(observations) - n_positive
  n_positive / p - (n_negative)/(1-p)
}


log_likelihood_second_derivative <- function(p, observations) {
  n_positive = sum(observations)
  n_negative = length(observations) - n_positive
  -n_positive / p^2 - (n_negative)/(1-p)^2
}
```

#### Simulate data

Compare shape of likelihood function for 2 different sample sizes
```{r}
set.seed(2)
n    <- 10
pi   <- 0.2
x    <- sample(c(0,1), size = n, replace = T, prob = c(1-pi, pi))

p_sec = seq(0,1, length.out = 100)
log_likelihoods <- map_dbl(p_sec, function(p) log_likelihood(p, x))

plot(p_sec, log_likelihoods, ylim = c(-800,0))

n2 <- 100
x2 <- sample(c(0,1), size = n2, replace = T, prob = c(1-pi, pi))
points(p_sec, map_dbl(p_sec, function(p) log_likelihood(p, x2)))
```

Look at derivatives
```{r}
set.seed(2)
n_sec = 10^(1:6)
pi   <- 0.2

second_derivatives <- map_dbl(n_sec, function(n) {
  x = sample(c(0,1), size = n, replace = T, prob = c(1-pi, pi))
  log_likelihood_second_derivative(pi, x)
})

data.frame(n_sec, second_derivatives)
```


Standard error: $SE = \sqrt(\frac{1}{information})$

If you can get the likelihood for your model, you done.

### Comparing models

* Optimize likelihood for parameters
* Get likelihood for this
* $L_i$ is the probability of the data using model $i$

Likelihood ratio: $\frac{L_i}{L_j}$

#### Penalize likelihood for number of predictors

##### Akaike's information criterion (AIC)

$$AIC = -2*l + 2*n_{parameters}$$

Goal: minimize AIC

If the difference between AIC's < 2, then they are the same.

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
