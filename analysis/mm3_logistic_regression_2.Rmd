---
title: "Logistic regression 2"
author: "Wouter van Amsterdam"
date: 2018-01-11
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Tutor
Jan van den Broek

## Likelihood ratio test

$$2*(l_1 - l_0)$$

Follows a $\chi^2$ distribution with $n_{parameters_1} - n_{parameters_0}$ degrees of freedom, when the sample size is not too small

### Deviance

Calculate (log) likelihood for actual observered data

Deviance

$$D = 2 (l_{observations} - l_model)$$

The model with the lowest deviance, is the closests to the actual data, 
so that is the best model.

Comparing deviances of 2 models is exactly the same as likelihood ratio test statistic.

### Wald test

Take $\frac{\beta}{se(\beta)}$ and use t-test.

However, likelihood ratio test is better

### Profile of log-likelihood

Log-likelihood as function of model parameter. 
For each value of this parameter, the likelihood is maximized over all other parameters.

The profile gives confidence-intervals.

Interpretation:
Those value for parameters $b_0$ that lead to the conclusion of not rejecting $H_0$.

### Profile log-likelihood confidence intervals

Take $L_1$, and $L_0$ for a model with a specific value for $\beta$, say 0.

$$H_0: \beta = \beta_0$$

$$2(l_1 - l_0) = 2(l(a,b) - l(a, b = b_0))$$

This follows a $\chi^2$ distribution with 1 degree of freedom, 
since $l_0$ has 1 parameter ($a$), and $l_1$ has 2 parameters ($a,b$)

Get all the values of $\beta_0$ for which you do not reject $H_0$

Based on the $\chi^2$ distribution, get the boundary for rejecting $H_0$.
Since $l_1 = l(a, b)$ is fixed, this depends only on $l(a, b = b_0)$,
and results in a boundary for $l(a, b = b_0)$.

This confidence bound inherits the properties of the likelihood profile, 
e.g. a-symmetry.

### Residuals for logistic regression

Residual for observation $i$ with prediction $p_i$ is difference in 
likelihood based on ${p_i}$,
versus likelihood based on $y_i$. Deviance

$$D(y_i) = 2 * (ln(y_i^{y_i}*(1-y_i)^{1-y_i}) - ln(p_i^{y_i}(1-p_i)^{1-y_i}))$$

Then:

$$res_{dev}=sign(y_i-p_i)*\sqrt{D(y_i)}$$

### Inspect residuals

Always two groups of residuals, due to 0-1 data and definition of residuals.

Look at extreme residuals.


### Stepwise procedures

Using p-values is wrong, we do not now how to calculate them.

Anly use AIC.


#### Use mtcars for example
```{r}
data(mtcars)

fit <- glm(am ~ mpg+cyl+disp+drat, data= mtcars, family = binomial)

summary(fit)

step(fit)
```

## P-value vs likelihood

Likelihood: given these data, what is the likelihood of my model(s)
P-value: given a null-hypothesis, what is the probability of my data under 
the null-model

P-value only mean something for a truely i.i.d. sample

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
