---
title: "Assignments for applied Bayesian statistics"
author: "Wouter van Amsterdam"
date: 2018-04-30
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

# Setup R

We will use R version 3.3.2 for compatibility

```{r}
library(dplyr)
library(data.table)
library(magrittr)
library(purrr)
library(here) # for tracking working directory
library(ggplot2)
library(epistats)
library(broom)
library(rjags)
library(Bain)
```

# Day 1

## Exercise 1: A Bayesian test for proportions

> Several studies suggest that cognitive behavioral therapy is an effective treatment for Posttrau- matic Stress Disorder (PTSD) in male veterans. Imagine that you performed a study to compare Prolonged Exposure (PE), a type of cognitive behavioral therapy, with Present-Centered therapy (PC), a supportive intervention. In this randomized controlled trial 284 veterans were assigned to receive either PE or PC. The (dichotomous) outcome measure of interest was “loss of diagnosis”(LD); that is, each veteran that was no longer diagnosed with PTSD after treatment got the score 1, and the ones that still had the diagnosis got the score 0.

> In this exercise you will use rjags to examine whether PE and PC are effective treatments for PTSD in veterans. The data you need for this exercise are displayed in Table 1 below.

> Table 1
> PE	PC
Loss of diagnosis	58	40
Total men treated	141	143

### 1.a Specify your Bayesian model for JAGS

> Open the file Model1.txt (you can open it in R, or in a text editor like notepad). We will specify the model in this file in a couple of steps. In the the Model1.txt file you can add your own comments by putting # in front of those comments. In that way, JAGS will know that it should not run that code, but consider it as a comment


#### Step 1. Likelihood

> We are interested in the proportion of veterans that had a LD in each treatment condition, that is, we are interested in the probability of observing a succes (a 1) in the pe condition (θ_pe) and the pc condition (θ_pc).

> We can model the number of successes for each condition with binomial distributions:

> y_pe ~ Binomial(θ_pe, n_pe)

> y_pc ~ Binomial(θ_pc, n_pc)

> That is, for each condition, we assume that the data y (the number of successes in the data) have a binomial distribution with as parameters θ, the probability of observing a success, and n, the total number of observations in the condition.

> The total number of observations in the conditions we know, as well as of course the data y (see Table 1). The probabilities θ¸ are what we want to estimate from the information we have, and we will do this in a Bayesian way.

> In the model code for JAGS in Model1.txt we have to specify these distributions within model{ } so that JAGS can find the likelihood based on this information. Therefore, within model{ } we add the following code in Model1.txt:

> # likelihood of the data
ype ~ dbin(ppe,npe)
ypc ~ dbin(ppc,npc)
Step 2. Priors
Next, we have to specify prior distributions for all the parameters we need to estimate. That is, we need priors for θ_pe (ppe in our code) and θ_pc (ppc in our code). A commonly used conjugate prior distribution for probabilities from a binomial distribution is the beta-distribution. That is,

> θ_pe ~ Beta(a_pe, b_pe)

> θ_pc ~ Beta(a_pc, b_pc)

> with shape parameters a (a_pe, a_pc) and b (b_pe, b_pc).

> For each condition, we will specify that a = b = 1 in our model code to obtain low-informative prior distributions. That is, we add the following code in Model1.txt within model{ }:

> # prior distributions
ppe ~ dbeta(1,1)
ppc ~ dbeta(1,1)
Step 3. Interesting additional quantities
Finally we will specify a contrast you may be interested in, namely the ratio of the probability of succes in condition PE and the probability of succes in condition PC (θ_pe /θ_pc). By adding the following code within model{ } we will tell JAGS to calculate this ratio in each iteration of the estimator:

> # contrast
ratio <- ppe/ppc
You could also add any other quantity based on the probabilities in your code, for example, the difference in the probabilities.

> Our model code is done for now - save your code as Model1.txt in your working directory.

### 1.b Preparing the data for JAGS in R

> Before we can use our model, we first have to get our data (from Table 1) into R. Make four variables in R that contain respectively the number of veterans that lost their diagnosis for condition PE, the number of veterans that lost their diagnosis for condition PC, the total number of veterans that were in condition PE, and the total number of veterans that were in condition PC.

> # data for exercise 1


```{r}
y_pe <- 58
y_pc <- 40
n_pe <- 141
n_pc <- 143 

```

> Now, if we want to use this data for our analysis in rjags we need to put these data in what is called a list. In the list we specify for all the names we used for the data in the jags model code (that is, ype, ypc, npe, npc) what the actual data is that we have stored in R (y_pe, y_pc, n_pe, n_pc).

> About lists: You can store various types of R objects (such as vectors, matrices or other lists) in a list. The list type of object is used for specifying data and initial values in rjags amongst other things, but it is, for instance, also frequently used for storing and presenting output from analyses in R. If you want to know more about lists you can read more here: http://rforpublichealth.blogspot.nl/2015/03/basics-of-lists.html.

> Make a list out of the data, and call the list you made to see what is inside as follows:

> # data for exercise 1 for rjags

(actually, naming arguments in lists works in R without parentheses)

```{r}
dat_ex1 <- list('ype'=y_pe, 'ypc'=y_pc, 'npe'=n_pe, 'npc'=n_pc) 
dat_ex1
dat_ex1 <- list(ype=y_pe, ypc=y_pc, npe=n_pe, npc=n_pc) 
dat_ex1

```

### 1.c Specifying and running the Bayesian analysis with rjags

> In order to run the analyses we first have to tell rjags what the model file is, what the data is, and specify the number of chains for the analyses (we use 2 for this exercise - you will learn more about this on Tuesday). It is also possible to specify initial values, but we will skip this for now (JAGS will generate the initial values itself based on the priors you specified in Model1.txt). We specify the model by means of the function jags.model(). To learn a bit more about this function already, run ?jags.model in R.

> We use the jags.model() function and we name it rjags_ex1 as follows :

```{r}
nsamp = 10000
rjags_ex1 <- jags.model(file=here("analysis", "bayes_1_Model1.txt"),
                   data = dat_ex1,
                   n.chains = 2) ###compile and initialize jags model

```

> After you have run this line of code, jags will compile the model and specify initial values for (initialize) the model. Now, we want to start the estimation procedure to fit our Bayesian model. We start by letting the estimation procedure run for a while, to start up and converge (these first few iterations in the estimation procedure are called burn-in - more on this on Tuesday). We do this with the function update(). Using update(), jags starts the estimation procedure, but it doesn’t save this as the results.

> We tell update which model to update in the first argument, and how many times in the second argument as follows:

```{r}
update(rjags_ex1, nsamp) ####burnin
```

> When jags is done with the first updates, we can now tell it to run some more iterations in our estimation procedure, and to calculate and store some results for us. For this we use the function coda.samples(). We need to tell coda.samples() which model to update, for which parameters we want jags to save the results, and how many iterations to use (we use 1000). To learn more about this function, use ?coda.samples().

> First we will make a vector with function c() with the names of all the parameters we want to see the results for. Next, we specify the function coda.samples, and we name it samples_rjags_ex1.


```{r}
parstosave=c('ppe','ppc', 'ratio')
samples_rjags_ex1=coda.samples(model=rjags_ex1, variable.names=parstosave, n.iter=nsamp) 
```

> When it has reached 100% it is done. For this model, it will go really fast, but for more complicated models it might take a little while, depending on how many iterations you chose.

### 1.d Inspect and interpret the results

> After updating you normally have to check whether the algorithm has reached the target distribution. On Tuesday you will learn why and how this is done. For now, you may assume the sampler has reached convergence, and you can go on with analyzing the posterior results.

> If you call samples_rjags_ex1 you will get all the iterations for all the parameters rjags has saved for you. It can be very convenient to have all of these samples, however, for now we just want to see some summary results. To get summary results use function summary() on samples_rjags_ex1.

```{r}
summary(samples_rjags_ex1) 
```

> You will see the mean, standard deviation, and various quantiles of the posteriors for each estimated parameter (you will also see two kinds of standard errors, but you can disregard these for now). These summary statistics are calculated by rjags over all of the saved samples per parameter in samples_rjags_ex1.

> You can also makes plots of the posterior distributions like this:

```{r}
plot(samples_rjags_ex1,density=TRUE, trace=FALSE) ###densities
```

> Write down and interpret these posterior results (mean, median, 95% credibility interval). Do you think PE is an effective treatment for PTSD in male veterans? What about PC? Would you consider one treatment superior to the other? What do you base this on?

Based on the ratio of the two, the 95% credible interval does not include 
the null-value of no effect (ratio = 1), so it seems that ppe is more effective

### 1.e Obtaining the results analytically

> Rjags has provided you with the posterior results for θ_pe, θ_pc, and their ratio. In this part of the exercise you will obtain the posterior means for these parameters analytically.

> It can be shown that the posterior distribution for each θ i is given by:
It can further be shown that the posterior mean ( θ̂ i ) of the distribution is given by:

> Calculate the posterior mean for both proportions θ_pe and θ_pc using this equation. Then calculate the ratio of these means. Compare these results to those you have obtained using rjags. Are the results similar?

```{r}
a = 1; b = 1
theta_pe <- (a + y_pe) / (a + y_pe + b + n_pe - y_pe)
theta_pc <- (a + y_pc) / (a + y_pc + b + n_pc - y_pc)
theta_pe
theta_pc
theta_pe / theta_pc
```

## Exercise 2: Informative Prior Distributions

> On an international conference on PTSD you meet two fellow researchers, Thomas B. and Ronald F., who also evaluated the use of PE versus PC. Thomas recently executed a randomized clinical trial with 520 female veterans to evaluate the use of PE versus PC. Ronald F. did a comparable trial with 235 WOII male veterans in 1946.

### 2.a Including data from previous studies

> As discussed during the today’s lecture on informative prior specification, it can we worthwhile to include data obtained in previous studies in the analysis of new data.

> Would you be interested to include the results obtained in either of the trials in the prior distribution for the analysis of your own data? Which data do you think would be most relevant and why?

thomas has data on only women, which is a small subset of all veterans, and women 
differ from men with regards to psychiologic disorders

ronald used data from 1946, which is a very long time ago

I would give more weight to the ronald's data

so use these beta distributions:
ppe ~ dbeta(41,66)
ppc ~ dbeta(45,86)

So now:

```{r}
40 / 105
45 / 130
(40 / 105) / (45 / 130)
```


### 2.b Including data from previous studies

> In Exercise 2.c you will rerun the analysis from Exercise 1 with informative prior distributions based on the data obtained by either Thomas or Ronald. In order to do so you will need to alter the uninformative Beta(1,1) distributions into informative distributions. This is what their data looks like:

> Table 2. PTSD data from Thomas B.
PE	PC
Loss of diagnosis	120	80
Total men treated	245	275
Table 3. PTSD data from Ronald F.
PE	PC
Loss of diagnosis	40	45
Total men treated	105	130
Given the data that you consider to be the most relevant, what would your new informative prior distributions look like?

> Hint: You can think of parameters a_i and b_i of the Beta-distribution as the prior number of successes and failures + 1 respectively.

### 2.c Rerun the analysis with your informative prior of choice

> Run the analysis again with your informative prior distributions. Ask for the posterior results for both proportions θ_pe and θ_pc and their ratio. Compare the results (mean, median, 95% Central Credibility Interval) with the results obtained in Exercise 1. To what degree do the informative priors influence the posterior results? Do you think this is an desirable effect?

```{r}
rjags_ex2 <- jags.model(file=here("analysis", "bayes_1_Model2.txt"),
                   data = dat_ex1,
                   n.chains = 2) ###compile and initialize jags model
update(rjags_ex2, nsamp)
samples_rjags_ex2 = coda.samples(model=rjags_ex2, variable.names=parstosave, n.iter=nsamp) 
summary(samples_rjags_ex2)
```

The ratio is lower, but the 95% credible interval still does not include 1

### 2.d (bonus) Rerun the analysis with the other informative prior

> Rerun the analysis with informative prior distributions based on the data obtained with the other randomized controlled trial. Compare the results from all three analyses. Which informative priors affect the results the most? Why?

```{r}
120 / 245
80 / 275
(120 / 245) / (80 / 275)
```


```{r}
rjags_ex2.2 <- jags.model(file=here("analysis", "bayes_1_Model2.2.txt"),
                   data = dat_ex1,
                   n.chains = 2) ###compile and initialize jags model
update(rjags_ex2.2, nsamp)
samples_rjags_ex2.2 = coda.samples(model=rjags_ex2.2, variable.names=parstosave, n.iter=nsamp) 
summary(samples_rjags_ex2.2)

```

Using these priors, the ratio comes out higher

```{r}
summaries <- map(list(model_1 = samples_rjags_ex1,
                      model_2 = samples_rjags_ex2,
                      model_2.2 = samples_rjags_ex2.2),
                 summary)
map(summaries, "quantiles")
```

The data from Thomas were based on more people, but whe estimated coefficient
was closer to what we observed in our data, so the influence on the estimate 
was smaller

> What happens when you include conflicting data? What happens when your prior is based on more data than available in the current study?

It overrides the data

### 2.d (bonus) Try to obtain the posterior results in 2.c and 2.d analytically.

Remember we used
ppe ~ dbeta(41,66)
ppc ~ dbeta(45,86)

And 

ppe ~ dbeta(121,126)
ppc ~ dbeta(80,196)

```{r}
a_pe = 41; b_pe = 66
a_pc = 46; b_pc = 86
theta_pe <- (a_pe + y_pe) / (a_pe + y_pe + b_pe + n_pe - y_pe)
theta_pc <- (a_pc + y_pc) / (a_pc + y_pc + b_pc + n_pc - y_pc)
theta_pe / theta_pc
theta_pe
theta_pc


a_pe = 121; b_pe = 126
a_pc = 80;  b_pc = 196
theta_pe <- (a_pe + y_pe) / (a_pe + y_pe + b_pe + n_pe - y_pe)
theta_pc <- (a_pc + y_pc) / (a_pc + y_pc + b_pc + n_pc - y_pc)
theta_pe / theta_pc

```

> How did you include the prior information in the calculation of the posterior mean? Did you get similar results?

Yes the results are very similar.

In fact, they were closer to the 50% percentile than to the means

```{r}
map(summaries, "statistics")
```

## Exercise 3: Evaluating Assumptions with a Posterior Predictive Check

> In this exercise you will focus on the data from the PE-group. In the previous exercise you assumed these data were binomially distributed. In this exercise you will assess whether the binomial model actually fits the data. If all the observed statuses of diagnosis are truly a sequence of independent Bernoulli trials (such that the number of successes is binomially distributed), the proportion of success in the first half of the data should be equal to the proportion of success in the second half. You will evaluate this using rjags, by performing a posterior predictive check.

### 3.a Specify your Bayesian model for JAGS

> Specify the basic model for the bernouilli distributed data of the PE condition, with uninformative prior distributions.

#### Step 1. Likelihood

> Open the file Model2.txt. Within the model{} statement, we need to specify the distributions and equations that describe the data for the PE-condition, so that JAGS can determine the likelihood of the data. The data you will use for this exercise is a series of n=141 Bernoulli trials. Please refer to the data in your model with x (like how in the previous exercises the data was called  y_pe or y_pc), and refer to the total sample size with n.


> You can do this with the following code:
for (i in 1:n){
x[i] ~ dbern(ppe)
}
You see this code makes use of a “for loop”. Inside the for loop, it says x[i] ~ dbern(ppe). The for loop works like this: In its first loop, it note that i is equal to 1. Then it goes to the equation x[i] ~ dbern(ppe), and reads this as x[1] ~ dbern(ppe), that is, the first observation in the data x comes from a bernoulli distribution with parameter ppe. Then, it finishes its first loop, and goes to the second loop. Now it notes that i=2, and that x[2] ~ dbern(ppe), that is, the second observation in the data x also comes from a bernoulli distribution with parameter ppe. Then it goes to the third loop, and so on, until i is equal to n (with n being equal to the sample size of x). That is, this code says that each observation in x comes from a bernoulli distribution with parameter ppe.

#### Step 2. Priors

> Now, in file Model2.txt, within the model{} statement, specify a Beta(1,1) prior distribution for θ_pe.

### 3.b 

> Specify all additional quantities in your model file that you need to do the posterior predictive check.

#### Step 1. 

> Obtain the observed proportion of successes in each half of your data, and the difference between these proportions
Within the model{} statement specify you wish to calculate the proportion of success in the first and second half of the data. Then specify you wish to calculate the difference between the two proportions. The latter is your discrepancy measure of interest.

> You can do this with the following code:

> # discrepancy measure in the data
ppe1 <- sum(x[1:70])/70
ppe2 <- sum(x[71:141])/71
dif <- ppe1-ppe2
More information on the sum-function and other logical functions can be found in the JAGS manual under “Functions”.

#### Step 2. 

> Obtain simulated data sets that are in line with your model assumptions.
By sampling from a binomial distribution with the same probability as in the real dataset, we can obtain simulated data sets of bernoulli trials (each sample from the binomial distribution is equal to one data set of bernoulli trials). Specify two binomial distributions - the first one with a sample size that is equal to the first half of that of the real data, the second one with the same size as the second half of the real data. If the real data is also truly a set of bernouilli trials, the proportions of succes in the simulated datasets should be similar to what we found for the real dataset.

> You can thus add the following lines within the model{} statement to obtain samples from the posterior predictive distribution in each iteration of the sampler (replicates of the first and second halves of the data):

> # posterior predictive distribution
postpred.ype1 ~ dbin(ppe,70)
postpred.ype2 ~ dbin(ppe,71)
Step 3. Obtain the observed proportion of successes in each half of each simulated data set, and the difference between these proportions.
Obtain the proportion of success in the first and second half of the replicated data (simular to step 1), and calculate the difference between the two proportions (you could call it postpred.dif). This piece of code is not given, so you have to program this yourself.

> Step 4. Use the step function to obtain the posterior predictive p-value (ppp-value)
You can use the step function in JAGS in the following way to get a posterior predictive p-value for the difference between the real and simulated difference between the proportions of successes in each half of the data:

> # posterior predictive p-value
p <- step(postpred.dif - dif)
What does the step function do? See the JAGS manual (in chapter ‘Functions’) or (Ntzoufras 2009, p.96) for details on the  step() function.

Step function is False  when x < 0, and True when x >= 1

> Now you know what the step function does, what does the mean of the posterior of p mean based on this piece of code? What is H0 in our (posterior predictive) hypothesis? What would you expect the posterior distribution of p to look like if H0 were true?

In what proportion of the simulated samples was the difference higher than 
the observed difference.

If H0 is true, the observed difference should fall within the bulk of the distribution
of the simulated differences

### 3.c Use rjags to run your Bayesian analysis with the posterior predictive check.

> Run this line of code in R to obtain a list with the data for the rjags model:

```{r}
dat_ex3= list(x=c(0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,0,1,0,1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,0,1,1,1,0,1,0,1,1,0,0,0,1,0,0,1,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,0,0,0,0,0,1,0,0,1,1,0,1,0,1,1,0,1,0,0,0,0,1,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,1,0,1,0,0,0,0,1,1,0,0)
, n=141)
```

> Now run the rjags analysis on your data with the model you specified in Model2.txt, in the same general way as in Exercise 1 and 2.

```{r}
nsamp = 1000
rjag_3 <- jags.model(here("analysis", "bayes_1_Model3.txt"),
                    data = dat_ex3,
                    n.chains = 2)
update(rjag_3, nsamp)
vars_to_save <- c("postpred.ppe1", "postpred.ppe2", "postpred.dif", "p")
rjag_3_samples <- coda.samples(rjag_3, variable.names = vars_to_save, n.iter = nsamp)
```

### 3.d Report and interpret the ppp-value.

```{r}
summary(rjag_3_samples)
```

> What are your conclusions? Finally, give some thought to how this relates to the classical alternative for the ppp-value. Describe the difference(s).


The proportion of times the posterior predictive check difference was higher than the observed difference

This was in 22% of the cases, which means that the 
observed difference is not very extreme under 
the null-hypothesis

```{r}
x1 <- sum(dat_ex3$x[1:70])
n1 <- 70
x2 <- sum(dat_ex3$x[71:141])
n2 <- 71
prop.test(c(x1, x2), c(n1, n2))
```

The p-values are not very close


> Considerations for your presentation on Friday:
Perform a Bayesian analysis on some (your own?) data set with uninformative and informative prior distributions
Perform a posterior predictive pheck to check model assumptions for a particular (your own?) data set
Present about the difference between classical hypothesis testing and using posterior predictive p-values

# Day 2

> Applied Bayesian Statistics: Lab Meeting 2. Convergence, model selection with the DIC, and prediction.
Today’s objectives
Today you will use rjags to sample from the posterior distribution of a logistic regression model, and see if the procedure seems to have converged. You will also practice using the information criterium DIC in the context of the selection of predictors for a logistic regression model. Furthermore, for a person with known predictors but unknown outcome, you will obtain a prediction and credibility interval for the unknown outcome.

## Exercise 1: Evaluating Convergence for a Logistic Regression Model

> In this exercise you will develop a clinical prediction rule for detecting major depressive disorder in primary care. The dataset you will use for this exercise consists of various measures of 1046 participants, aged 18-65 years. These participants were recruited from general practice waiting rooms. Major depressive disorder (depr: 0=no depression, 1=depression) was assessed with the Composite International Diagnostic Interview according to DSM-IV-TR criteria. For this exercise we will predict depression diagnosis with gender (0=male, 1=female).


### 1.a Specify your Bayesian logistic regression model for JAGS

#### Step 1. Likelihood

> Open file ModelDay2.txt, and specify a univariate logistic regression model with gender as a predictor variable, such that JAGS can determine the likelihood of the data.

> A logistic regression model could be specified for OpenBUGS as follows:

> # likelihood of the data
for (i in 1:n){
logit(p[i]) <- alpha + b.gender*gender[i]
depr[i] ~ dbern(p[i])}

```{r}
con1 <- file(here("analysis", "bayes_2_Modelday2.txt"), open = "r")
lines1 <- readLines(con1)

```

#### Step 2. Priors

> Next, specify prior distributions for all parameters in the model that need to be estimated. What type of prior distributions would you specify for the regression coefficient and for the intercept? How would you specify these distributions to be uninformative?

Take 

- alpha: normal with mean 0, variance 10000 (precision 1/10000)
- beta: normal with mean 0, variance 10000 (precision 1/10000)

> You can refer to the jags manual for an overview of all available probability distributions (https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/jags_user_manual.pdf/download). Pay special attention to what the parameters of each distribution represent - for example, the normal distribution is not specified with a mean and standard deviation as you might expect, but with a mean and a precision (the precision is equal to 1/variance).

#### Step 3. Interesting additional quantities

> You might want to specify the exponent of your regression coefficient in order to interpret directly the posterior distribution of the odds and odds ratios. To do so, include the following commands in the model:

> # odds ratios
odds0 <- exp(alpha)
or.gender <- exp(b.gender)

### 1.b Fit the Logistic Regression Model with rjags.

#### Step 1. data

> With these lines of code you can load and specify the data for rjags:

```{r}
load(here("data", "data_practical2.Rdata"))
data_prac2_ex1 <- dat_prac2[,1:2]
n=length(data_prac2_ex1[,1])
data_ex1 <- list("n"=n, "depr"= data_prac2_ex1[,1],"gender"= data_prac2_ex1[,2])
```

#### Step 2. initial values

> This time you will have also specify the initial values yourself (instead of generating initial values randomly). You should specify initial values for all the parameters in your model, and put them in a list. Make three of these lists, each with different initial values - one for each chain. Here is an example for the first of your chains:

> Finally, put all three of your lists into a list like so:


```{r}
ins1 <- list(alpha=0, b.gender=1)
ins2 <- list(alpha = -5, b.gender = 0)
ins3 <- list(alpha = 5, b.gender = -5)
ivals <- list(ins1, ins2, ins3)
```

#### Step 3 rjags model

> Now specify your jags.model() function similar to the exercises from yesterday. Use three chains, and add  inits= ivals as an argument to specify initial values for these chains.

```{r}
jm_logit <- jags.model(here("analysis", "bayes_2_Modelday2.txt"),
                       data = data_ex1,
                       n.chains = length(ivals),
                       inits = ivals)
```

####Step 4 Burnin and samples

> Specify that you want to take 2000 samples burnin. Then specify which parameters you want to save, and use coda.samples() to take 2000 samples (per chain) to base your results on. Store the latter samples in object samps_ex1.

```{r}
nburnin = 2000
update(jm_logit, nburnin)

nsamps = 2000
to_save <- c("alpha", "b.gender", "odds0", "or.gender")
samps_ex1 <- coda.samples(jm_logit, variable.names = to_save, n.iter = nsamps)
```

###1.c Inspect the convergence of the model

> Inspect the trace plots
You can obtain trace plots for each parameter with the following line of code:

```{r}
plot(samps_ex1, density = F)
```

> Do the three chains seem to mix properly?

Yes pretty good mix

> Inspect the density plots
You can obtain trace plots for each parameter with the following line of code:

```{r}
plot(samps_ex1, trace = F)
```

> Do the densities look nice and smooth? If not, you might want to take more samples.

Yes they are pretty smooth

> Inspect the Gelman Rubin convergence diagnostics
You can get both plots for the gelman rubin statistics for all the samples, and obtain summary statistics for them in the following way:

```{r}
gelman.plot(samps_ex1) ##plot gelman rubin statistics
gelman.diag(samps_ex1) ## table gelman rubin statistics

```

> Does the black line in the plot converge to 1? Are the point estimates and majority of plausible values very close to 1 on equal to 1 for each parameter?

Yes all very close

> Inspect the monte carlo standard errors (naive se) for each parameter
Use the function summary() on samples_ex1 and inspect the standard errors for each parameter. The monte carlo standard errors (both the naive, and the time series error which is corrected for the autocorrelation of the chain) represent an estimate of the uncertainty contributed by only having a finite number of samples from the posterior. These errors should be small compared to the standard deviation of your estimates - a rule of thumb is that they should be smaller than about 5% of the posterior standard deviation. Is this the case for each parameter?

```{r}
summary(samps_ex1)
```

Yes they are lower than 5% from the SD of the parameter

### Conclusion

> Do you think the sampler has converged? If not, consider centering the predictor to avoid a strong correlation between the intercept and slope (as discussed in today’s lecture), and/or taking more samples.

To me it seems pretty converged

> Tip: You can make a scatter plot to visually inspect the correlation between the intercept and slope for the first chain like this:

Make them into a data.frame, bind together by chain

```{r}
samps_df <- samps_ex1 %>%
  map(as.data.frame) %>%
  rbindlist(idcol = "chain")
```

Plot by chain

```{r}
samps_df %>%
  ggplot(aes(x = alpha, y = b.gender)) + 
  geom_point(alpha = .5) + 
  facet_wrap(~chain)
```

Does not seem too narrow in either of the 3 chains

### 1.d Inspect and interpret the results

> Once you have ensured yourself that the sampler has converged (for all parameters) you can continue to report and interpret the posterior results. What is the posterior estimate of the OR for gender? Looking at the 95% Credible Interval would you want to keep this predictor in the model?

> Use the function summary() and your density plots to draw your conclusions.

```{r}
summary(samps_ex1)
```

From the 95% credible interval, I would keep the regression coefficient in.

### 1.e Credible and Confidence Intervals

> Describe the difference between the classical Confidence Interval and the Bayesian Credibility Interval.

Check with glm


```{r}
fit_glm <- glm(depr ~ gender, family = binomial, data = data_prac2_ex1)
summary(fit_glm)
confint(fit_glm)
```

The confidence intervals are not the same.

## Exercise 2: Model Selection with the DIC for a Logistic Regression Model

### 2.a Choose five candidate models

> Based on what you know about depressive disorders, select three candidate predictor variables (in addition to gender) from the variables listed below. Choose and write down five interesting candi- date models that you could build with these predictors.


> Candidate predictors for the model are:
gender: 0=male, 1=female
age: age in years
educ: educational level, 0=more than primary, 1=only primary
consult: number of consults in the last 12 months
household: 0=with others, 1=alone
partner: 0=with partner, 1=single
diagnosis: wether GP made diagnosis, 0=diagnosis , 1=only complaint (no diagnosis)
problems: presentation of mental or social problems at inclusion, 0=no, 1=yes
somatic: presentation of somatic complaints at inclusion, 0=no, 1=yes
antidep: antidepressant prescribed in the last 12 months, 0=no, 1=yes
depGP: depression code at GP, 0=no, 1=yes
cidi1: any depressed feelings in lifetime, 0=no, 1=yes
cidi2: any loss of interest in lifetime, 0=no, 1=yes

All could be important, I choose:

- problems
- age
- household

models:

- fit0: depr ~ 1
- fit1: depr ~ gender
- fit2: depr ~ gender + age
- fit3: depr ~ gender + age + problems
- fit4: depr ~ gender + age + problems + household

### 2.b Specify and fit each of the five models, and monitor the DIC for each model.

First inspect the data

```{r}
str(dat_prac2)
```

Setup a container for the models

```{r}
library(formula.tools)
models <- data.frame(
  model = 0:4,
  outcome = "depr",
  rhs_char = c("1", 
               "gender", 
               "gender + age", 
               "gender + age + problems", 
               "gender + age + problems + household")
)
models %<>%
  mutate(
    model_file = paste0("bayes_2_Model2.", model, ".txt"),
    model_file_full = here("analysis", model_file),
    formula_char = paste0(outcome, "~", rhs_char),
    covariates = map(formula_char, ~rhs.vars(formula(.x))),
    parameters = map(covariates, ~{
      params = c("alpha")
      if (length(.x) > 0) params = c(params, paste0("b.", .x))
      return(params)})
    )
models$parameters
```



Declare starting values for each model

These should be lists with values named per variable

We will set starting values between -5 and 5 for each of them, as they 
are all regression coefficients. To make sure not all variables 
start with the same coefficient, we will cycle the starting values

```{r}
nmodels <- nrow(models)
start_values <- c(-5, 0, 5)
nchains = length(start_values)
# models$formula_char %>%
#   map(formula) %>% 
#   map(rhs.vars) %>%
#   map(print)

start_values_list <- vector(length = nmodels, mode = "list")
for (i in 1:nmodels) {
  params = models$parameters[i][[1]]
  nparams = length(params)

  start_values_model <- vector(length = nchains, mode = "list")
  
  for (j in 1:nchains) {
    start_values_model[[j]] <- vector(length = nparams, mode = "list")
    names(start_values_model[[j]]) <- params

    ## assign value of alpha
    start_values_model[[j]][["alpha"]] <- start_values[j]

    ## assign values of other variables
    if (nparams > 1) {
      for (k in 2:(nparams)) {
        shifter = 1 + ((1 + j + k) %% 3)
        start_values_model[[j]][[k]] <- start_values[shifter]
      }
    }
  }
  start_values_list[[i]] <- start_values_model
}
print(start_values_list[[2]])
```


Compile the models, store them in a list


```{r}
start_values_list[[2]][[1]]
```


```{r}
dat_prac2_cage <- mutate(dat_prac2, age = (age - min(age)) / diff(range(age)))
j_models <- map2(models$model_file_full, start_values_list,
                 ~jags.model(.x, 
                             data = dat_prac2_cage,
                             inits = .y,
                            n.chains = 3)) 

```

```{r}
models %<>%
  mutate(jags_model = j_models,
         init_values = start_values_list,
         out_vars = c("alpha", setdiff(rhs_char, "1")))
```


> Do not forget to specify prior distributions and initial values for all parameters in each logistic regression model you run. For each model, inspect the convergence, examine and write down the posterior results: posterior means and 95% CIs of the parameters in each model, and the DIC.

Now burnin each model and extract the relevant parameters

```{r}
nburnin = 2000
niter = 2000

burnin_and_run <- function(model, params, nburnin, niter) {
  update(model, nburnin)
  # out_vars <- c("alpha")
  # if (length(vars) > 0) {
  #   out_vars <- c(out_vars, paste0("b.", vars), paste0("or.", vars))
  # }
  samples = coda.samples(model, n.iter = niter, variable.names = params)
  return(samples)
}

models %<>%
  mutate(samples = map2(jags_model, parameters,
                        ~burnin_and_run(.x, .y, nburnin = nburnin, niter = niter)))

```

Check convergence for the models

(implement in ggplot later)

```{r}
# models %>%
  # mutate(sample_df = samples %>%
  #          map(~as.data.frame(.x[[1]])) %>%
  #          rbindlist(idcol = "chain", fill = T))

```

```{r}
plot(models$samples[1][[1]])
plot(models$samples[2][[1]])
plot(models$samples[3][[1]])
plot(models$samples[4][[1]])
plot(models$samples[5][[1]])
```



Extract the DIC

```{r}
models %<>% 
  mutate(dic = map(jags_model, ~dic.samples(.x, n.iter = niter, type = "pD")))

```

Look at DICs

```{r}
get_dic <- function(x) {
  deviance <- sum(x$deviance)
  psum <- sum(x[[2]])
  penalized_deviance = deviance + psum
  return(penalized_deviance)
}

models %<>%
  mutate(dic_value = map_dbl(dic, get_dic))
```

```{r}
models %>%
  select(outcome, rhs_char, dic_value)
```


> Obtaining the right variables for your analysis and putting these data in a list
For each model you need to specify the right data. For this you need to see in which columns of the dataframe dat_prac2 your variables of interest are stored. You need to select these columns, store them in an object, and then store those in a list for rjags. For example, if you want to select the variables depr, gender and partner, you can go about it in the following way:

> head(dat_prac2)  ###Use this code to see an abbreviated version of the dataframe, and count in which columns your variables are
### we see that depr is in column 1, gender in 2, and partner in 7.
##Store each column you need in an object with an appropriate name:
depr <- dat_prac2[,1] 
gend <- dat_prac2[,2]
part <- dat_prac2[,7]

> n=length(data_prac2_ex1[,1])  ### don't forget to specify n too - the number of observations which is needed for your for loop.

> ###Now state which object belongs with which variable in your jags model code, and put this in a list you can supply to rjags:

> data_m1 <- list("n"=n, "depr"=depr, "gender" = gend, "partner" = part)
Monitoring the DIC
To keep track of the dic, add the following (adjusted) line of code after using coda.samples()

> ##in the first argument you specify which rjags model to sample the dic for, the second how many samples to take, and the third the type of penalty function (pD for the DIC).
dic.mod1 <- dic.samples(model = rjags_ex1, n.iter=2000, type = "pD")
To obtain the dic, call dic.mod1 and write down the penalized deviance.

### 2.c Which variables are included in your final model?

All

> Report and interpret the posterior results of your final model.

Exercise 3: Prediction with a Bayesian Logistic Regression Model
In this exercise you will predict the probability of depression for a new patient using Bayesian prediction.

## 3.a Add the data for the new patient to the data set

> Add the data for the predictor variables for the new patient at the bottom (last row) of the data frame dat_prac2. Specify the depression score as NA, indicating that this is a missing observation.

> You can do this with the following code:

> load("data_practical2.Rdata")

```{r}
data_new_patient=c(NA,2,3.50000E+01,2.00000E+00,0.00000E+00,0.00000E+00, 0.00000E+00,1.00000E+00,1.00000E+00,1.00000E+00,
                   0.00000E+00,0.00000E+00,1.00000E+00,1.00000E+00)
dat_prac2[1047,] <- data_new_patient
dat_prac2_cage_pred <- mutate(dat_prac2, age = (age - min(age)) / diff(range(age)))
```

> Is the new patient male or female? Has he/she been prescribed antidepressants?

### 3.b Run the model and report and interpret the results

> Report and interpret the 95% CI of the predicted probability of depression for this patient. Don’t forget to request depr[1047] and p[1047] as parameters to save samples for. What are your conclusions?

```{r}
final_init <- start_values_list[[5]]
final_params <- models$parameters[5][[1]]

jm_predict <- jags.model(here("analysis", "bayes_2_Model2.4_predict.txt"),
                         data = dat_prac2_cage_pred, 
                         inits = final_init, n.chains = length(final_init))

update(jm_predict, nburnin)
samp_predict <- coda.samples(jm_predict, variable.names = c(final_params, "pred_depr", "pred_p"),
                             n.iter = niter)
```

```{r}
summary(samp_predict)
```


### 3.c Dealing with missing data

> Compare this Bayesian approach of handling missing data with classical missing data methods you are familiar with. What are differences and similarities?

Does not work with missing values


> Today’s considerations for your presentation on Friday:
Fit a Bayesian logistic regression model on your own data

> Discuss differences and similarities between the DIC and other information criteria.

> Use a Bayesian model for prediction with your own data set

> Discuss missing data analysis from a Bayesian perspective


## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
